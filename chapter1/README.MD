# **딥러닝과 언어 모델링**  
LLM은 기술적으로는 딥러닝에 기반을 두고 있다. 딥러닝이란 인간의 두뇌에 영감을 받아 만들어진 신경망(neural network)으로서 데이터의 패턴을
학습하는 머신러닝의 한 분야다. 딥러닝은 표 형태의 정형 데이터뿐만 아니라 텍스트와 이미지 같은 비정형 데이터에서도 뛰어난 인식 성능을 보여
2010년대 중반 이후 AI 분야의 주류 모델로 자리 잡았다. LLM은 사람의 언어를 컴퓨터가 이해하고 생성할 수 있도록 연구하는 자연어 처리 분야에 속하며
특히 그중에서도 사람과 비슷하게 텍스트를 생성하는 방법을 연구하는 자연어 생성에 속한다. LLM은 다음에 올 단어가 무엇일지 예측하면서 문장을 하나씩
만들어 가는 방식으로 텍스트를 생성하는데 이렇게 다음에 올 단어를 예측하는 모델을 언어 모델이라고 한다.  
  
딥러닝 기반의 언어 모델이 지금처럼 자리 잡기까지 역사적으로 중요했던 세 가지 사건이 있었다. 2013년 구글에서 단어를 의미를 담아 숫자로 표현하는  
워드투벡(word2vec)을 발표했으며 2017년에는 기계 번역 성능을 높이기 위해 개발한 트랜스포머 아키텍처(transformer architecture)를 공개했다.  
2018년 OpenAI가 트랜스포머 아키텍처를 활용한 GPT-1 모델을 공개했다.  
  
# **데이터의 특징을 스스로 추출하는 딥러닝**  
딥러닝에서 문제를 해경하는 방법
- 문제의 유형(예: 자연어 처리, 이미지 처리)에 따라 일반적으로 사용되는 모델을 준비한다.  
- 풀고자 하는 문제에 대한 학습 데이터를 준비한다.  
- 학습 데이터를 반복적으로 모델에 입력한다.  
  
이렇게 3단계만 거치면 문제는 간단하게 풀린다. 딥러닝은 이처럼 단순한 접근 방식을 통해 기존에는 쉽게 풀 수 없었던 텍스트나 이미지 같은  
비정형 데이터 문제도 쉽게 풀어냈다. 딥러닝의 놀라운 단순성은 기존의 머신러닝 접근법과 비교했을 때 더 명확히 드러난다.  
  
딥러닝이 머신러닝과 갖아 큰 차이를 보이는 '지점은 데이터의 특징을 누가 뽑는가?' 이다. 기존 머신러닝에서는 데이터의 특징을 연구자 또는 개발자  
가 찾고 모델에 입력으로 넣어 결과를 출력했다. 반면 딥러닝에서는 모델이 스스로 데이터의 특징을 찾고 분류하는 모든 과정을 학습한다.  
  
# **임베딩: 딥러닝 모델이 데이터를 표현하는 방식**  
딥러닝 모델은 학습 과정에서 데이터의 특징을 추출하는 방법도 함께 배운다. 컴퓨터는 숫자만 처리할 수 있기 때문에 딥러닝 모델은 데이터의 의미를  
숫자의 집합으로 표현한다. 데이터의 의미와 특징을 포착해 숫자로 표현한 것을 임베딩(embedding)이라고 부른다. 임베딩은 딥러닝을 이해할 때 가장  
중요한 개념 중 하나다. 임베딩이란 데이터를 그 의미를 담아 여러 개의 숫자의 집합으로 표현하는 것을 말한다.  
  
데이터를 임베딩으로 표현하면 데이터 사이의 거리를 계산하고 거리를 바탕으로 관련 있는 데이터와 관련이 없는 데이터를 구분할 수 있다.  
임베딩은 거리를 계산할 수 있기 떄문에 다음과 같은 작업에 활용할 수 있다.  
- 검색 및 추천: 검색어와 관련이 있는 상품을 추천한다.  
- 클러스터링 및 분류: 유사하고 관련이 있는 데이터를 하나로 묶는다.  
- 이상치(outlier) 탐지: 나머지 데이터와 거리가 먼 데이터는 이상치로 볼 수 있다.  
  
단어를 임베딩으로 변환한 것을 일컬어 단어 임베딩(word embedding)이라고 한다.  
  
단어의 경우에는 보통 수십에서 수만 개의 숫자로 표현된다. 단어 임베딩에서는 0.1, 0.7과 같은 숫자가 어떤 의미인지 알기 어렵다. 딥러닝 모델이  
데이터에서 특징을 추출하는 방법을 알아서 학습하기 때문에 사람이 그 의미를 하나하나 파악할 수 없는 것이다. 숫자 하나하나의 의미는 알기  
어렵지만 [0.1, 0.7, ..., 0.3]이라는 숫자 집합 전체로 입력한 단어의 의미를 담고 있다.  
  
딥러닝 모델은 데이터를 통해 학습하는 과정에서 그 데이터를 가장 잘 이해할 수 있는 방식을 함께 배운다. 그렇게 데이터의 의미를 숫자로 표현한  
것이 임베딩이다.  
  
# **언어 모델링: 딥러닝 모델의 언어 학습법**  
언어 모델링이란 모델이 입력받은 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식을 말한다. 다음 단어를 예측하는 방식으로 훈련한 모델을  
언어 모델(language model)이라고 한다. 언어 모델링은 텍스트를 생성하는 모델을 학습시키는 방법으로도 사용되지만 대량의 데이터에서 언어의  
특성을 학습하는 사전 학습(pre-training)과제로도 많이 사용된다.  
  
딥러닝 분야에서는 하나의 문제를 해결하는 과정에서 얻은 지식과 정보를 다른 문제를 풀 때 사용하는 방식을 많이 활용하는데 이를 전이 학습  
(transfer learning)이라고 부른다. 전이 학습에서는 대량의 데이터로 모델을 학습시키는 사전 학습과 특정한 문제를 해결하기 위한 데이터로 추가  
학습하는 미세 조정(fine-tuning)의 두 단계로 나눠 학습을 진행한다. 언어 모델링은 자연어 처리 분야에서 사전 학습을 위한 과제로 사용된다.  
  
일찍이 이미지 인식 분야에서는 이미 자연어 처리 분야보다 먼저 대량의 데이터로 이미 학습된 모델을 필요한 과제 맞춰 추가로 학습하는 전이 학습  
개념이 널리 활용되고 있었다.  
  
유방암이 양성인지 악성인지 분류하는 새로운 문제를 풀고자 할 때 유방암 데이터만으로 학습한 모델보다 사전 학습 모델의 일부를 가져와 활용했을  
때 일반적으로 성능이 더 높다. 사전 학습에 사용한 이미지와 현재 풀고자 하는 과제의 이미지가 다르더라도 선이나 점 같은 특징을 파악하는 능력은  
공통적으로 필요하기 떄문이다. 이때 사전 학습 모델을 미세 조정해 풀고자 하는 과제를 흔히 다운스트림(downstream)과제라고 부른다. 전이 학습은  
학습 데이터가 적은 경우에 특히 유용한데 사전 학습한 모델을 활용하면 더 적은 유방암 이미지만으로도 높은 성능의 모델을 학습시킬 수 있다.  
  
기존의 머신러닝 모델 학습은 처음부터 끝까지 해결하려는 문제의 데이터로 학습하는 방식을 사용했다. 하지만 딥러닝의 전이 학습은 대량의 데이터로  
학습하는 단계(사전 학습)와 현재 해결하려는 문제의 데이터로 추가 학습(미세 조정)하는 두 단계로 모델을 학습시킨다.  
  
지도 학습(supervised learning)방식은 각각의 데이터셋으로 별도의 모델을 학습시켰다.  
전이 학습 방식은 모델의 본체 부분은 대규모 데이터셋인 이미지넷으로 학습한 모델에서 가져오고 분류를 수행하는 헤드 부분은 해결하려는 작업의  
데이터셋으로 추가 학습한다. 헤드 부분은 일반적으로 본체 부분에 비해 작기 떄문에 비교적 적은 데이터로도 학습이 가능하다. 이때 헤드를 추가  
학습하는 과정이 사전 학습에 비해 적은 양의 학습 데이터를 사용한다는 의미에서 미세 조정이라고 부른다.  
  
자연어 처리 분야에서도 이미지 인식에서와 같이 전이 학습 개념을 활용하고 싶었지만 마땅한 사전 학습 방식을 찾지 못하고 있었다. 그러던 중  
2018년 fast.ai의 제레미 하워드와 세바스찬 루더가 다은 단어를 예측하는 언어 모델링 방식으로 사전 학습을 수행했을 때 훨씬 적은 레이블  
데이터로도 기존 지도 학습 모델의 성능을 뛰어넘는다는 사실을 발견했다.  
  
다음 단어를 예측하는 방식으로 언어 모델을 사전 학습하고 나서 다운스트림 과제의 데이터셋으로 언어 모델 미세 조정을 수행하고 마지막으로 텍스트  
분류 미세 조정을 했을 때 바로 텍스트 분류 모델로 학습시켰을 때보다 성능이 높았다. 텍스트 데이터에 따로 레이블이 없더라도 단지 다음 단어를  
예측하는 방식으로 사전 학습에 활용할 수 있는 길이 드디어 열린 것이다.  
  
제레미 하워드와 세바스찬 루더는 당시 많이 활용되던 순환신경망(Recurrent Neural Network, RNN)에서 언어 모델링이 사전 학습 과제로  
적합하다는 사실을 확인했다. 2018년은 트랜스포머 아키텍처를 활용한 모델이 처음 등장하던 시기였다. 대표적인 트랜스포머 모델로는 구글의 BERT  
와 OpenAI의 GPT가 있다. OpenAI는 GPT-1의 논문에서 트랜스포머 모델에서도 언어 모델링으로 사전 학습을 수행했을 때가 그렇지 않았을 때에  
비해 다운스트림 과제에서 모델의 성능이 높았다고 발표했다. 이를 통해 언어 모델링은 자연어 처리 분야에서 가장 대표적인 사전 학습 방법으로 자리  
잡았다.  
  
# **언어 모델이 챗GPT가 되기까지**  
  
# **RNN에서 트랜스포머 아키텍처로**  
딥러닝이나 머신러닝 분야에서 텍스트는 단어가 연결된 문장 형태의 데이터를 일컫는다. 이처럼 작은 단위(단어)의 데이터가 연결되고 그 길이가  
다양한 뎅터의 형태를 시퀀스(sequence)라 한다. 예를 들어 텍스트, 오디오, 시계열과 같은 데이터가 시퀀스 데이터라고 할 수 있다. 역사적으로  
이러한 시퀀스 데이터를 처리하기 위해 크게 순환신경망이나 트랜스포머의 두 가지 아키텍처로 대표되는 다양한 모델을 사용해 왔다. 여기서 모델  
아키텍처란 딥러닝 모델이 갖는 구조를 의미한다.  
  
트랜스포머가 개발되기 전에는 RNN을 활용해서 텍스트를 생성했다. RNN은 입력하는 텍스트를 순차적으로 처리해서 다음 단어를 예측한다. RNN의  
특징은 모델이 하나의 자재 상태(hidden state)에 지금까지의 입력 텍스트의 맥락을 압축한다는 점이다. 첫 번째 입력인 '검은'이 모델을 통과했을  
때는 잠재 상태에 '검은'의 정보가 쌓이고 두 번째 입력인 '고양이가'가 모델을 통과하면 잠재 상태에 '검은'과 '고양이가'의 정보가 누적된다.  
이렇게 입력이 늘어나면서 하나의 잠재 상태에 입력 텍스트의 맥락이 압축된다.  
  
RNN은 '물을' 다음의 단어를 예측하기 위해 앞에서 나온 맥락인 '검은 고양이가 밥을 먹고 물을'을 순차적으로 한 단어씩 처리해서 하나의 잠재  
상태로 만든다. 그 잠재 상태를 통해 RNN은 다음 단어로 '마신다'가 나온다고 예측한다. 이 방식은 여러 단어로 구성된 맥락을 하나의 잠재 상태에  
압축하기 때문에 메로리를 적게 사용한다는 장점이 있다. 또한 다음 단어를 예측할 때 지금까지 계산을 통해 만들어진 잠재 상태와 입력 단어만  
있으면 되기 때문에 다음 단어를 빠르게 생성할 수 있다. 하지만 순차적으로 입력되는 단어를 하나의 잠재 상태에 압축하다 보니 먼저 입력한 단어의  
의미가 점차 희석되며 입력이 길어지는 경우 의미를 충분히 담지 못하고 성능이 떨어진다는 문제가 있다.  
  
2017년 등장한 트랜스포머 아키텍처는 RNN의 순차적인 처리 방식을 버리고 맥락을 모두 참조하는 어텐션(attention)연산을 사용해 RNN의 문제를  
대부분 해결했다. RNN과 달리 맥락 데이터를 그대로 모두 활용해 다음 단어를 예측한다. 트랜스포머 아키텍처는 '마신다'라는 다음 단어를 예측할 때  
이전의 맥락인 '검은', '고양이가', '밥을', '먹고', '물을' 과의 관계를 모두 계산한다.  
  
트랜스포머 아키텍처는 맥락을 압축하지 않고 그대로 활용하기 떄문에 성능을 높일 수 있지만 입력 텍스트가 길어지면 맥락 데이터를 모두 저장하고  
있어야 하기 떄문에 메모리 사용량이 증가한다. 또한 매번 다음 단어를 예측할 때마다 맥락 데이터를 모두 확인해야 하기 때문에 입력이 길어지면  
예측이 걸리는 시간도 증가한다. 성능이 높아지는 대신 무겁고 비효율적인 연산을 사용하게 된 것이다.  
  
트랜스포머 아키텍처는 많은 연산량이 필요하다는 단점이 있지만 성능이 좋고 순차적으로 처리하는 RNN과 달리 병렬 처리를 통해 학습 속도를 높일  
수 있어 현재는 대부분의 LLM이 트랜스포머 아키텍처를 기반으로 하고 있다. 트랜스포머는 성능이 높지만 비효율적이고 RNN은 효율적이지만 성능이  
낮다. 성능이 높으면서도 효율적인 새로운 아키텍처를 찾기 위한 연구가 꾸준히 있었지만 대부분 트랜스포머의 성능에 미치지 못해 큰 주목을 받지  
못했다. 그러던 중 최근 뛰어난 성능과 효율성을 갖춘 새로운 아키텍처(맘바, Mamba)가 공개되며 많은 기대를 받고 있다.  
  
# **GTP 시리즈로 보는 모델 크기와 성능의 관계**  
2017년 트랜스포머 아키텍처가 등장한 뒤 2018년 트랜스포머 아키텍처를 활용한 구글의 BERT와 OpenAI의 GPT가 등장하면서 자연어 처리 분야에서  
주류 모델 아키텍처였던 RNN은 빠르게 트랜스포머로 대체됐다. 또한 다음 단어를 예측하는 언어 모델링이 사전 학습 방식으로 자리 잡으며 더 큰  
모델을 학습시킬 수 있는 기반이 마련됐다.  
  
이런 흐름 속에서 더 큰 모델을 학습시켜 자연어 이해 및 생성 능력을 높이려는 시도가 이어졌다. 대표적으로 OpenAIsms 2018년 1억 1700만 개의  
파라미터를 사용하는 GPT-1을 공개하고 2019년에는 모델을 12.8배 키워 15억 개의 파라미터를 사용하는 GPT-2를 공개했다. 또 1년 후 2020년에는  
모델을 116.7배 키워 1750억 개의 파라미터를 사용하는 GPT-3을 공개했다. OpenAI의 GPT 시리즈는 모델 구조에는 큰 변경 없이 오직 모델과 학습  
데이터셋의 크기만 키웠는데 이렇게 간단한 접근 방식으로도 언어 모델의 성능이 크게 높아져 GPT-3에 와서는 사람의 언어 생성 능력과 유사하다는  
평가를 받았다.  
  
모델의 크기가 커지고 학습 데이터가 많을수록 모델의 성능이 높아진다. 이는 언어 모델이 학습 데이터를 압축한다는 관점에서 본다면 이 사실을 더  
직관적으로 이해할 수 있다. 언어 모델의 경우 학습 데이터와 언어 모델의 결과가 모두 생성된 언어다. 따라서 언어 모델이 학습하는 과정을 학습  
데이터를 압축하는 과정으로 해석할 수 있다. 여기서 말하는 압축은 우리가 일상생활에서 사용하는 zip 파일을 만들 때처럼 무손실 압축을 의미하는  
것이 아니고 공통되고 중요한 패턴을 남기는 손실 압축이다. 대표적인 오픈소스 LLM인 메타(Meta)와 라마2(Llama2) 모델을 예시로 보면 약 10TB의  
텍스트로 학습해 최종적으로 140GB 크기의 모델이 된다. 학습 데이터 대비 약 1.4% 정도의 작은 모델에 학습 데이터가 갖고 있던 텍스트 생성의  
패턴을 압축했다고 볼 수 있다.  
  
압축의 관점에서 봤을 때 모델이 커지면 학습 데이터가 갖고 있는 언어 생성 패턴을 더 많이 학습할 수 있기 떄문에 모델 성능이 높아진다고 이해할  
수 있다. 하지만 모델이 계속해서 커진다고 성능이 높아지지는 않고 학습 데이터의 크기가 최대 모델 크기의 상한이라고 볼 수 있다. 예를 들어 학습  
데이터의 크기가 100GB일 경우 모델의 크기를 100GB보다 키운다고 하더라도 모델 성능이 높아지기를 기대하기는 어렵다.  
  
# **챗GPT의 등장**  
2020년 1750억 개의 파라미터를 가진 GPT-3는 충분히 뛰어난 텍스트 생성 능력을 보였지만 2022년 챗GPT와 비교했을 때 큰 반향을 일으키지  
못했다. 다양한 이유가 있겠지만 가장 큰 이유는 GPT-3는 그저 사용자의 말을 이어서 작성하는 능력밖에 없었기 떄문이다. 챗GPT는 사용자의  
요청사항을 이해하고 그에 맞춰 답변을 생성하지만 GPT-3는 그렇지 않았다.  
  
어마어마한 비용이 들지만 만들 수 있는 가치는 비교적 적었던 GPT-3를 챗GPT로 바꾼 것은 OpenAI가 논문의 연구 결과 발표와 함께 공개한  
지도 미세 조정(supervised fine-tuning)과 RLHF(Reinforcement Learning from Human Feedback, 사람의 피드백을 활용한 강화 학습)  
라는 기술이었다. 이 기술을 통해 챗GPT는 그저 사용자가 한 말 다음에 이어질 말을 생성하는 것이 아니라 사용자의 요청을 해결할 수 있는 텍스트를  
생성하게 됐다.  
  
LLM이 생성하는 답변을 사용자의 요청 의도에 맞추는 것을 정렬(alignment)이라고 한다. 사용자가 LLM의 답변에서 얻고자 하는 가치를 반영해  
LLM을 학습해서 LLM이 사용자에게 도움이 되고 가치를 전달할 수 있도록 하는 것이다. 지도 미세 조정은 정렬을 위한 가장 핵심적인 학습 과정으로서  
언어 모델링으로 사전 학습한 언어 모델을 지시 데이터셋(instruction dataset)으로 추가 학습하는 것을 뜻한다. 이때 지시 데이터셋은 사용자가  
요청 또는 지시한 사항과 그에 대한 적절한 응답을 정리한 데이터셋을 의미한다. OpenAI는 수많은 데이터 작업자를 고용해 LLM이 받을 법한 질문과  
그에 대한 답변을 작성하게 했고 이 데이터를 활용해 지도 미세 조정을 수행했다. 그 결과 챗GPT와 같이 사용자의 요청에 맞춰 응답하는 모델을 만들  
수 있었다.  
  
하지만 사용자의 요청에 맞춰 으답하는 것이 항상 옳은 것은 아닌데 예를 들어 폭탄이나 약물을 제조하는 방법을 묻는 경우 AI가 정보를 제공해  
준다면 사용자가 결국 위엄해질 수 있다. 또한 같은 내용의 답변이라도 사용자가 더 이해하기 쉽게 생성한다거나 인종, 성별 등에 차별적인 표현을  
사용하지 않는 등 다양한 관점에서 사용자에게 도움이 되도록 노력해야 한다.  
  
OpenAI에서는 두 가지 답변 중 사용자가 더 선호하는 답변을 선택한 데이터셋을 구축했는데 이를 선호 데이터셋(preference dataset)이라고 한다.  
선호 데이터셋으로 LLM의 답변을 평가하는 리워드 모델(reward model)을 만들고 LLM이 점점 더 높은 점수를 받을 수 있도록 추가 학습하는데 이때  
강화 학습(reinforcement learning)을 사용하기 떄문에 이 기술을 일컬어 RLHF라고 불렀다.  
  
# **LLM 애플리케이션의 시대가 열리다**  
  
# **지식 사용법을 획기적으로 바꾼 LLM**  
LLM이 사회에 큰 영향을 미치고 있는 이유는 다재다능함 때문이다. LLM의 다재다능함이란 하나의 언어 모델이 다양한 작업에서 뛰어난 능력을 보이는  
것을 의미한다. 기존에는 언어에 대해 다루는 AI 분야인 자연어 처리 분야를 크게 언어를 이해하는 자연어 이해(텍스트 분류, 감성 분석, 기계 독해)  
와 언어를 생성하는 자연어 생성(언어 모델링, 소설 작성)의 두 분야로 나눠 접근했다. 또 각각의 영역에서도 일부 좁은 영역의 작업을 해결하기  
위해(번역, 요약) 별도의 모델을 개발하는 방식으로 문제에 접근하는 경우가 많았다.  
  
하지만 LLM의 경우 언어 이해와 언어 생성 능력이 모두 뛰어나다. 처음부터 자연어 생성을 위한 모델이기 때문에 언어 생성 능력이 뛰어나고 모델의  
크기가 커지면서 언어 추론 능력을 포함해 언어 이해 능력이 크게 높아졌다. 지시 데이터셋으로 사용자의 요청에 응답하는 방식을 학습하면서 다양한  
작업에 적절히 응답하는 능력도 갖췄다. 이처럼 LLM은 언어 이해와 생성 두 측면 모두에서 뛰어나고 사용자의 요청에 맞춰 다양한 작업을 수행하는  
다재다능함을 가졌다는 점에서 이전까지의 자연어 처리 모델과 뚜렷이 구분된다.  
  
다재다능함이 중요한 이유는 사람이 하는 일 대부분은 언어 이해와 생성을 함께 필요로 하며 여러 작업이 복합적으로 섞여 있기 때문이다. 예를 들어  
사용자가 개발자라면 새로운 요구사항을 바탕으로 기존 코드를 이해해서 새로운 코드를 생성해야 한다. 또 기술과 관련된 문서를 읽고 요약하거나  
보고서를 작성하는 일도 한다. 마지막으로 현재의 상황과 여러 제한 조건을 바탕으로 어떤 판단이나 의사결정을 내리는 일도 수행해야 한다. 세 가지  
작업 모두 언어를 이해하는 것뿐만 아니라 결과를 언어로 생성해야 한다.  
  
기존의 자연어 처리 접근 방식에서는 이렇게 복합적인 작업을 수행하기 위해 언어 이해 모델과 언어 생성 모델을 각각 개발해 연결했다. 난이도나  
복잡도에 따라서는 언어 이해에도 여러 개의 모델이 필요하고 언어 생성에도 여러 모델이 필요한 경우도 생긴다. 많은 모델을 개발하고 연결할수록  
시스템의 복잡도가 높아지고 관리는 어려워진다.  
  
하지만 다재다능한 LLM을 활용한다면 하나의 LLM으로 작업을 수행하도록 만들 수 있다. 복잡도가 훨씬 낮기 떄문에 더 빠르게 다양한 작업에 AI를  
활용할 수 있다는 장점이 있다.  
  
LLM의 이런 다재다능함을 활용하면 사용자가 LLM을 통해 검색한 정보가 요약되고 쉽게 해설된 결과를 통해 지식을 비교적 쉽게 습득할 수 있고  
습득한 지식을 더 빠른 시간 안에 새로운 결과물로 조합해 낼 수 있다.  
  
# **sLLM: 더 작고 효율적인 모델 만들기**  
LLM 애플리케이션을 개발할 때 LLM을 활용하는 방법은 크게 두 가지로 나눌 수 있다. 첫 번쨰는 OpenAI의 GPT-4나 구글의 제미나이처럼 상업용  
API를 사용하는 방법이다. 두 번째는 오픈소스 LLM을 활용해 직접 LLM API를 생성해 사용하는 방법이다. 일반적으로 상업용 모델은 오픈소스  
LLM에 비해 모델이 크고 범용 텍스트 생성 능력이 뛰어나다. 하지만 오픈소스 LLM은 원하는 도메인의 데이터, 작업을 위한 데이터로 자유롭게 추가  
학습할 수 있다는 장점이 있다. 이렇게 추가 학습을 하는 경우 모델 크기가 작으면서도 특정 도메인 데이터나 작업에서 높은 성능을 보이는 모델을  
만들 수 있는데 이를 sLLM이라고 한다.  
  
챗GPT의 성공 이후 상업용 모델을 사용한 애플리케이션 개발도 많았지만 많은 기업이 자신의 조직에 특화된 sLLM을 개발하기 위해 시도했다.  
대표적으로 2024년 4월 메타는 라마-3 모델을 오픈소스로 공개하면서 sLLM 연구를 리드하고 있고 구글도 2024년 6월 제미나이 개발에 사용한  
기술로 만든 젬마-2 모델을 공개했다. 2024년 4월 마이크로소프트는 언어 추론 능력에 집중한 Phi-3를 공개했다. 특히 Phi-3 미니 모델은 38억  
개의 적은 파라미터로 강력한 언어 추론 능력을 보여 큰 충격을 줬다. 마지막으로 텍스트 요청을 SQL로 변환하는 작업에서 GPT-4를 뛰어넘은 디포그  
에이아이(defog.ai)의 SQLCoder도 있다.  
  
# **더 효율적인 학습과 추론을 위한 기술**  
LLM의 기반이 되는 트랜스포머 아키텍처 연산은 무겁고 또 모델 성능을 높이기 위해 모델의 크기를 키우면서 LLM의 학습과 추론에 필요한 연산량이  
크게 증가했다. LLM은 많은 연산량을 빠르게 처리하기 위해 다른 딥러닝 모델과 마찬가지로 GPU를 사용한다. GPU는 많은 연산을 병렬로 처리하는 데  
특화된 처리 장치인데 고가의 장비이기 떄문에 LLM을 사용하기 위한 비용 중 상당 부분이 GPU 비용에서 발생한다. 비용도 비용이지만 특히 챗GPT의  
성공 이후 GPU 수요가 급증하면서 돈을 주고도 GPU를 구하지 못하는 품귀 현상도 나타나고 있다.  
  
이런 배경에서 LLM을 학습하고 추론할 때 GPU를 더 효율적으로 사용해 적은 GPU 자원으로도 LLM을 활용할수 있도록 돕는 연구가 활발히 진행되고  
있다. 대표적으로는 모델 파라미터를 더 적은 비트로 표현하는 양자화(quantization)와 모델 전체를 학습하는 것이 아니라 모델의 일부만 학습하는 
LoRA(Low Rank Adaptation)방식이 있다. 또한 무거운 어텐션 연산을 개선해 효율적인 학습과 추론을 가능하게 하는 연구도 있다.  
  
# **LLM의 환각 현상을 대처하는 검색 증강 생성(RAG) 기술**  
LLM은 매우 강력한 도구이지만 한 가지 큰 문제가 있는데 바로 환각 현상이라고 불리는 것이다. 이는 LLM이 잘못된 정보나 실제로 존재하지 않는  
정보를 만들어 내는 현상을 말한다. 왜 이런 현상이 일어나는지는 정확히 알기 어렵지만 기본적으로 LLM은 학습 데이터를 압축해 그럴듯한 문장을  
만들 뿐 어떤 정보가 사실인지 거짓인지 학습한 적은 없어 특정 정보가 사실인지 판단할 능력은 없다. 또한 학습 데이터를 압축하는 과정에서 비교적  
드물게 등장하는 정보는 소실될 텐데 그런 정보의 소실이 부정확한 정보를 생성하는 원인이 될 수도 있다. OpenAI의 존 슐먼은 데이터셋으로 LLM을  
지도 미세 조정하는 과정에서 LLM이 기존에 알지 못하는 정보가 포함된 경우 환각 현상을 유발할 수 있다고 제안했다.  
  
OpenAI의 창립 멤버이기도 한 안드레이 카르파타는 LLM이 텍스트를 생성하는 것은 마치 꿈꾸는 것과 비슷하다고 말하기도 했다. 즉 환각 현상은  
LLM만으로는 해결이 쉽지 않은 치명적인 단점이라고 할 수 있다. 이런 문제를 줄이기 위해 검색 증강 생성(Retrieval Augmented Generation,  
RAG)이라는 기술을 사용한다. RAG 기술은 프롬프트에 LLM이 답변할 때 필요한 정보를 미리 추가함으로써 잘못된 정보를 생성하는 문제를 줄인다.  
  
# **LLM의 미래: 인식과 행동의 확장**  
대표적으로는 세 가지 큰 흐름이 있다.  
  
LLM이 더 다양한 형식의 데이터(예: 이미지, 비디오, 오디오 등)를 입력으로 받을 수 있고 출력으로도 여러 형태의 데이터를 생성할 수 있도록  
발전시킨 멀티 모달(multi modal) LLM이 있다.  
  
LLM이 텍스트 생성 능력을 사용해 계획을 세우거나 의사결정을 내리고 필요한 행동까지 수행하는 에이전트(agent)연구도 활발히 진행되고 있다.  
  
LLM이 사용하는 트랜스포머 아키텍처를 새로운 아키텍처로 변경해 더 긴 입력을 효율적으로 처리하려는 연구도 주목받고 있다. 오디오와 비디오 같은  
데이터는 텍스트에 비해 입력이 훨씬 긴데 LLM이 더 긴 입력을 처리할 수 있게 되면 오디오와 비디오 데이터를 처리하는 능력도 향상될 것이다.  
  
멀티 모달 모델이란 다양한 형태의 입력을 받을 수 있는 LLM을 말한다. LLM은 기본적으로 텍스트를 입력으로 받고 텍스트를 출력으로 내보내기  
때문에 다양한 사용 사례를 해결하는 데는 한계가 있다. 대표적으로 이미지에 대한 정보는 LLM이 처리하기가 꽤 까다로운데 이런 문제를 해결하기  
위해 멀티 모달 모델에 대한 연구가 활발히 진행되고 있다. 2024년 5월 공개된 GPT-4o는 뛰어난 이미지 및 음성 처리 능력을 보여줬고 구글의  
제미나이, 앤트로픽의 클로드 같은 상업용 모델도 이미지를 함께 처리할 수 있다. 여러 오픈소스 멀티 모달 모델이 등장하고 있으며 RAG에서도  
이미지와 텍스트를 함께 검색하는 멀티 모달 RAG에 대한 관심이 높아지고 있다.  
  
LLM을 단순히 텍스트를 생성하는 기능 외에 스스로 판단하고 행동하는 에이전트의 두뇌로 사용하려는 시도도 늘고 있다. LLM은 뛰어난 언어 이해 및  
추론 능력을 갖추고 있기 때문에 LLM을 활용하면 주어진 상황을 인식하고 필요한 행동을 계획해 직접 수행하게 만들 수 있다. 2023년 LLM을 핵심  
엔진으로 사용하는 자동화 시스템인 AutoGPT가 공개되면서 LLM 기반 에이전트의 가능성에 대한 기대가 커졌고 최근에는 마이크로소프트의 AutoGen,  
CrewAI등 여러 개의 에이전트를 활용해 문제를 해결하는 멀티 에이전트 프레임워크도 많은 인기를 얻고 있다.  
  

