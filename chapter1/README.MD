# **딥러닝과 언어 모델링**  
LLM은 기술적으로는 딥러닝에 기반을 두고 있다. 딥러닝이란 인간의 두뇌에 영감을 받아 만들어진 신경망(neural network)으로서 데이터의 패턴을  
학습하는 머신러닝의 한 분야다. 딥러닝은 표 형태의 정형 데이터뿐만 아니라 텍스트와 이미지 같은 비정형 데이터에서도 뛰어난 인식 성능을 보여  
2010년대 중반 이후 AI 분야의 주류 모델로 자리 잡았다. LLM은 사람의 언어를 컴퓨터가 이해하고 생성할 수 있도록 연구하는 자연어 처리 분야에 속하며  
특히 그중에서도 사람과 비슷하게 텍스트를 생성하는 방법을 연구하는 자연어 생성에 속한다. LLM은 다음에 올 단어가 무엇일지 예측하면서 문장을 하나씩  
만들어 가는 방식으로 텍스트를 생성하는데 이렇게 다음에 올 단어를 예측하는 모델을 언어 모델이라고 한다.  
  
딥러닝 기반의 언어 모델이 지금처럼 자리 잡기까지 역사적으로 중요했던 세 가지 사건이 있었다. 2013년 구글에서 단어를 의미를 담아 숫자로 표현하는  
워드투벡(word2vec)을 발표했으며 2017년에는 기계 번역 성능을 높이기 위해 개발한 트랜스포머 아키텍처(transformer architecture)를 공개했다.  
2018년 OpenAI가 트랜스포머 아키텍처를 활용한 GPT-1 모델을 공개했다.  
  
# **데이터의 특징을 스스로 추출하는 딥러닝**  
딥러닝에서 문제를 해경하는 방법
- 문제의 유형(예: 자연어 처리, 이미지 처리)에 따라 일반적으로 사용되는 모델을 준비한다.  
- 풀고자 하는 문제에 대한 학습 데이터를 준비한다.  
- 학습 데이터를 반복적으로 모델에 입력한다.  
  
이렇게 3단계만 거치면 문제는 간단하게 풀린다. 딥러닝은 이처럼 단순한 접근 방식을 통해 기존에는 쉽게 풀 수 없었던 텍스트나 이미지 같은  
비정형 데이터 문제도 쉽게 풀어냈다. 딥러닝의 놀라운 단순성은 기존의 머신러닝 접근법과 비교했을 때 더 명확히 드러난다.  
  
딥러닝이 머신러닝과 갖아 큰 차이를 보이는 '지점은 데이터의 특징을 누가 뽑는가?' 이다. 기존 머신러닝에서는 데이터의 특징을 연구자 또는 개발자  
가 찾고 모델에 입력으로 넣어 결과를 출력했다. 반면 딥러닝에서는 모델이 스스로 데이터의 특징을 찾고 분류하는 모든 과정을 학습한다.  
  
# **임베딩: 딥러닝 모델이 데이터를 표현하는 방식**  
딥러닝 모델은 학습 과정에서 데이터의 특징을 추출하는 방법도 함께 배운다. 컴퓨터는 숫자만 처리할 수 있기 때문에 딥러닝 모델은 데이터의 의미를  
숫자의 집합으로 표현한다. 데이터의 의미와 특징을 포착해 숫자로 표현한 것을 임베딩(embedding)이라고 부른다. 임베딩은 딥러닝을 이해할 때 가장  
중요한 개념 중 하나다. 임베딩이란 데이터를 그 의미를 담아 여러 개의 숫자의 집합으로 표현하는 것을 말한다.  
  
데이터를 임베딩으로 표현하면 데이터 사이의 거리를 계산하고 거리를 바탕으로 관련 있는 데이터와 관련이 없는 데이터를 구분할 수 있다.  
임베딩은 거리를 계산할 수 있기 떄문에 다음과 같은 작업에 활용할 수 있다.  
- 검색 및 추천: 검색어와 관련이 있는 상품을 추천한다.  
- 클러스터링 및 분류: 유사하고 관련이 있는 데이터를 하나로 묶는다.  
- 이상치(outlier) 탐지: 나머지 데이터와 거리가 먼 데이터는 이상치로 볼 수 있다.  
  
단어를 임베딩으로 변환한 것을 일컬어 단어 임베딩(word embedding)이라고 한다.  
  
단어의 경우에는 보통 수십에서 수만 개의 숫자로 표현된다. 단어 임베딩에서는 0.1, 0.7과 같은 숫자가 어떤 의미인지 알기 어렵다. 딥러닝 모델이  
데이터에서 특징을 추출하는 방법을 알아서 학습하기 때문에 사람이 그 의미를 하나하나 파악할 수 없는 것이다. 숫자 하나하나의 의미는 알기  
어렵지만 [0.1, 0.7, ..., 0.3]이라는 숫자 집합 전체로 입력한 단어의 의미를 담고 있다.  
  
딥러닝 모델은 데이터를 통해 학습하는 과정에서 그 데이터를 가장 잘 이해할 수 있는 방식을 함께 배운다. 그렇게 데이터의 의미를 숫자로 표현한  
것이 임베딩이다.  
  
# **언어 모델링: 딥러닝 모델의 언어 학습법**  
언어 모델링이란 모델이 입력받은 텍스트의 다음 단어를 예측해 텍스트를 생성하는 방식을 말한다. 다음 단어를 예측하는 방식으로 훈련한 모델을  
언어 모델(language model)이라고 한다. 언어 모델링은 텍스트를 생성하는 모델을 학습시키는 방법으로도 사용되지만 대량의 데이터에서 언어의  
특성을 학습하는 사전 학습(pre-training)과제로도 많이 사용된다.  
  
딥러닝 분야에서는 하나의 문제를 해결하는 과정에서 얻은 지식과 정보를 다른 문제를 풀 때 사용하는 방식을 많이 활용하는데 이를 전이 학습  
(transfer learning)이라고 부른다. 전이 학습에서는 대량의 데이터로 모델을 학습시키는 사전 학습과 특정한 문제를 해결하기 위한 데이터로 추가  
학습하는 미세 조정(fine-tuning)의 두 단계로 나눠 학습을 진행한다. 언어 모델링은 자연어 처리 분야에서 사전 학습을 위한 과제로 사용된다.  
  
일찍이 이미지 인식 분야에서는 이미 자연어 처리 분야보다 먼저 대량의 데이터로 이미 학습된 모델을 필요한 과제 맞춰 추가로 학습하는 전이 학습  
개념이 널리 활용되고 있었다.  
  
유방암이 양성인지 악성인지 분류하는 새로운 문제를 풀고자 할 때 유방암 데이터만으로 학습한 모델보다 사전 학습 모델의 일부를 가져와 활용했을  
때 일반적으로 성능이 더 높다. 사전 학습에 사용한 이미지와 현재 풀고자 하는 과제의 이미지가 다르더라도 선이나 점 같은 특징을 파악하는 능력은  
공통적으로 필요하기 떄문이다. 이때 사전 학습 모델을 미세 조정해 풀고자 하는 과제를 흔히 다운스트림(downstream)과제라고 부른다. 전이 학습은  
학습 데이터가 적은 경우에 특히 유용한데 사전 학습한 모델을 활용하면 더 적은 유방암 이미지만으로도 높은 성능의 모델을 학습시킬 수 있다.  
  
기존의 머신러닝 모델 학습은 처음부터 끝까지 해결하려는 문제의 데이터로 학습하는 방식을 사용했다. 하지만 딥러닝의 전이 학습은 대량의 데이터로  
학습하는 단계(사전 학습)와 현재 해결하려는 문제의 데이터로 추가 학습(미세 조정)하는 두 단계로 모델을 학습시킨다.  
  
지도 학습(supervised learning)방식은 각각의 데이터셋으로 별도의 모델을 학습시켰다.  
전이 학습 방식은 모델의 본체 부분은 대규모 데이터셋인 이미지넷으로 학습한 모델에서 가져오고 분류를 수행하는 헤드 부분은 해결하려는 작업의  
데이터셋으로 추가 학습한다. 헤드 부분은 일반적으로 본체 부분에 비해 작기 떄문에 비교적 적은 데이터로도 학습이 가능하다. 이때 헤드를 추가  
학습하는 과정이 사전 학습에 비해 적은 양의 학습 데이터를 사용한다는 의미에서 미세 조정이라고 부른다.  
  
자연어 처리 분야에서도 이미지 인식에서와 같이 전이 학습 개념을 활용하고 싶었지만 마땅한 사전 학습 방식을 찾지 못하고 있었다. 그러던 중  
2018년 fast.ai의 제레미 하워드와 세바스찬 루더가 다은 단어를 예측하는 언어 모델링 방식으로 사전 학습을 수행했을 때 훨씬 적은 레이블  
데이터로도 기존 지도 학습 모델의 성능을 뛰어넘는다는 사실을 발견했다.  
  
다음 단어를 예측하는 방식으로 언어 모델을 사전 학습하고 나서 다운스트림 과제의 데이터셋으로 언어 모델 미세 조정을 수행하고 마지막으로 텍스트  
분류 미세 조정을 했을 때 바로 텍스트 분류 모델로 학습시켰을 때보다 성능이 높았다. 텍스트 데이터에 따로 레이블이 없더라도 단지 다음 단어를  
예측하는 방식으로 사전 학습에 활용할 수 있는 길이 드디어 열린 것이다.  
  
제레미 하워드와 세바스찬 루더는 당시 많이 활용되던 순환신경망(Recurrent Neural Network, RNN)에서 언어 모델링이 사전 학습 과제로  
적합하다는 사실을 확인했다. 2018년은 트랜스포머 아키텍처를 활용한 모델이 처음 등장하던 시기였다. 대표적인 트랜스포머 모델로는 구글의 BERT  
와 OpenAI의 GPT가 있다. OpenAI는 GPT-1의 논문에서 트랜스포머 모델에서도 언어 모델링으로 사전 학습을 수행했을 때가 그렇지 않았을 때에  
비해 다운스트림 과제에서 모델의 성능이 높았다고 발표했다. 이를 통해 언어 모델링은 자연어 처리 분야에서 가장 대표적인 사전 학습 방법으로 자리  
잡았다.  
  
# **언어 모델이 챗GPT가 되기까지**  
  
# **RNN에서 트랜스포머 아키텍처로**  
딥러닝이나 머신러닝 분야에서 텍스트는 단어가 연결된 문장 형태의 데이터를 일컫는다. 이처럼 작은 단위(단어)의 데이터가 연결되고 그 길이가  
다양한 뎅터의 형태를 시퀀스(sequence)라 한다. 예를 들어 텍스트, 오디오, 시계열과 같은 데이터가 시퀀스 데이터라고 할 수 있다. 역사적으로  
이러한 시퀀스 데이터를 처리하기 위해 크게 순환신경망이나 트랜스포머의 두 가지 아키텍처로 대표되는 다양한 모델을 사용해 왔다. 여기서 모델  
아키텍처란 딥러닝 모델이 갖는 구조를 의미한다.  
  
트랜스포머가 개발되기 전에는 RNN을 활용해서 텍스트를 생성했다. RNN은 입력하는 텍스트를 순차적으로 처리해서 다음 단어를 예측한다. RNN의  
특징은 모델이 하나의 자재 상태(hidden state)에 지금까지의 입력 텍스트의 맥락을 압축한다는 점이다. 첫 번째 입력인 '검은'이 모델을 통과했을  
때는 잠재 상태에 '검은'의 정보가 쌓이고 두 번째 입력인 '고양이가'가 모델을 통과하면 잠재 상태에 '검은'과 '고양이가'의 정보가 누적된다.  
이렇게 입력이 늘어나면서 하나의 잠재 상태에 입력 텍스트의 맥락이 압축된다.  
  
RNN은 '물을' 다음의 단어를 예측하기 위해 앞에서 나온 맥락인 '검은 고양이가 밥을 먹고 물을'을 순차적으로 한 단어씩 처리해서 하나의 잠재  
상태로 만든다. 그 잠재 상태를 통해 RNN은 다음 단어로 '마신다'가 나온다고 예측한다. 이 방식은 여러 단어로 구성된 맥락을 하나의 잠재 상태에  
압축하기 때문에 메로리를 적게 사용한다는 장점이 있다. 또한 다음 단어를 예측할 때 지금까지 계산을 통해 만들어진 잠재 상태와 입력 단어만  
있으면 되기 때문에 다음 단어를 빠르게 생성할 수 있다. 하지만 순차적으로 입력되는 단어를 하나의 잠재 상태에 압축하다 보니 먼저 입력한 단어의  
의미가 점차 희석되며 입력이 길어지는 경우 의미를 충분히 담지 못하고 성능이 떨어진다는 문제가 있다.  
  
2017년 등장한 트랜스포머 아키텍처는 RNN의 순차적인 처리 방식을 버리고 맥락을 모두 참조하는 어텐션(attention)연산을 사용해 RNN의 문제를  
대부분 해결했다. RNN과 달리 맥락 데이터를 그대로 모두 활용해 다음 단어를 예측한다. 트랜스포머 아키텍처는 '마신다'라는 다음 단어를 예측할 때  
이전의 맥락인 '검은', '고양이가', '밥을', '먹고', '물을' 과의 관계를 모두 계산한다.  
  
트랜스포머 아키텍처는 맥락을 압축하지 않고 그대로 활용하기 떄문에 성능을 높일 수 있지만 입력 텍스트가 길어지면 맥락 데이터를 모두 저장하고  
있어야 하기 떄문에 메모리 사용량이 증가한다. 또한 매번 다음 단어를 예측할 때마다 맥락 데이터를 모두 확인해야 하기 때문에 입력이 길어지면  
예측이 걸리는 시간도 증가한다. 성능이 높아지는 대신 무겁고 비효율적인 연산을 사용하게 된 것이다.  
  
트랜스포머 아키텍처는 많은 연산량이 필요하다는 단점이 있지만 성능이 좋고 순차적으로 처리하는 RNN과 달리 병렬 처리를 통해 학습 속도를 높일  
수 있어 현재는 대부분의 LLM이 트랜스포머 아키텍처를 기반으로 하고 있다. 트랜스포머는 성능이 높지만 비효율적이고 RNN은 효율적이지만 성능이  
낮다. 성능이 높으면서도 효율적인 새로운 아키텍처를 찾기 위한 연구가 꾸준히 있었지만 대부분 트랜스포머의 성능에 미치지 못해 큰 주목을 받지  
못했다. 그러던 중 최근 뛰어난 성능과 효율성을 갖춘 새로운 아키텍처(맘바, Mamba)가 공개되며 많은 기대를 받고 있다.  
  

