# **새로운 아키텍처**  
트랜스포머 아키텍처는 2017년 발표 이후 자연어 처리는 물론 딥러닝 전 분야에서 핵심 아키텍처로 자리 잡았다. 발전 속도가 빠른 AI 분야에서 이렇게 
널리 퍼지고 오랜 기간 핵심적인 자리를 지키고있다는 점에서 트랜스포머가 얼마나 강력한 아키텍처인지 엿볼 수 있다. 사람들은 항상 '언젠가 새로운 아키텍처가 
나오겠지'라고 생각했지만 오랫동안 트랜스포머 아키텍처의 성능을 뛰어넘는 모델이 나오지 않았다. 그러던 중 2023년 12월 맘바(Mamba) 아키텍처가 
"트랜스포머와 성능이 비슷하거나 뛰어나면서 추론 속도가 5배"라고 주장하며 발표되면서 AI 분야를 뜨겁게 달구고 있다.  
  
![img.png](image/img.png)  
  
맘바를 이해하기 위해서는 위 그림과 같이 SSM(State Space Model)과 선택 메커니즘(selective mechanism)을 알아야 한다. 맘바는 RNN(Recurrent Neural Network)
을 개선한 모델이라고 할 수 있는데 SSM은 그중에서 속도를 높이기 위한 전략이고 선택 메커니즘은 문장의 맥락을 효율적으로 압축해 성능을 높이려는 
전략이다.  
  
# **기존 아키텍처의 장단점**  
지금까지 자연어 처리에서 사용하던 모델은 크게 두 그룹으로 묶을 수 있다. 먼저 2017년 이전에는 RNN을 주로 활용했고 2017년 이후에는 트랜스포머를 
주로 활용했다.  
  
![img.png](image/img2.png)  
  
각각의 장단점을 비교하면 위 표와 같다. RNN은 추론은 효율적이지만 학습할 때 입력을 병렬로 처리하지 못하고 순차적으로 입력하기 때문에 학습 속도가 
느리다는 단점이 있다. 또한 모델을 업데이트하기 위한 역전파 과정에서 그레이디언트가 너무 작아지거나 커지는 그레이디언트 소실(gradient vanishing) 
또는 그레이디언트 폭발(gradient exploding) 현상이 나타나 학습이 불안정했다. 마지막으로 한정된 메모리에 맥락을 압축하는 RNN의 특성상 문장이 
길어지면 성능이 떨어졌다. 이 문제는 이후 트랜스포머의 기반이 된 어텐션이 개발된 배경이기도 하다.  
  
앞서 언급한 대로 트랜스포머는 어텐션 연산을 사용하면서 시퀀스 길이가 길어져도 성능이 잘 유지된다. 어텐션 연산은 이전까지의 모든 텍스트와 관련도를 
계산하기 때문에 매 순간 모든 텍스트를 참조하기 떄문에 상당히 무겁다. 하지만 병렬화가 가능하다는 장점이 있는데 병렬화로 인해 학습 속도가 빠르고 
어텐션 연산으로 RNN보다 성능이 높아 다양한 사용 사례에서 채택됐다. 트랜스포머는 시퀀스 길이가 길어지면 빠르게 연산량이 커진다는 단점이 있는데 학습 
시에는 시퀀스 길이의 제곱에 비례하여 추론 시에는 시퀀스 길이에 비례하여 연산량이 증가한다.  
  
트랜스포머보다 연산이 가벼우면서 성능이 높은 모델을 개발하기 위해 RNN을 변형하는 연구가 꾸준히 있었다. 대표적으로 SSM 계열의 모델은 RNN이 갖고 있는 
추론의 효율성을 유지하면서 트랜스포머가 가진 학습 시 병렬 연산을 가능하게 하겠다는 목표로 개발됐다.  
  
# **SSM**  
맘바는 SSM 계열의 모델이다. SSM이란 내부 상태를 가지고 시간에 따라 달라지는 시스템을 해석하기 위해 사용하는 모델링 방법을 말한다. 일반적으로 SSM은 
다음과 같은 식으로 내부 상태와 출력이 시간에 따라 달라지는 과정을 수식화한다. 식에서 h는 모델 내부의 상태이고 x는 모델에 들어오는 입력을 말한다. 
A, B, C, D는 입력, 상태와 출력 사이의 관계를 연결하는 행렬이다. 마지막으로 y는 해당 시점의 출력이다.  
  
![img.png](image/img3.png)  
  
![img.png](image/img4.png)  
  
위 그림과 예시를 통해 직관적으로 이해해 보자. 시간에 따라 달라지는 시스템은 위 그림과 같이 표현할 수 있다. t 시점에 입력 x(t)가 들어오면 시스템의 
내부 상태는 h(t - 1)에서 h(t)로 바뀐다. 그리고 현재 시점의 출력 y(t)는 입력과 상태에 관련된다. 예를 들어 흑연을 넣으면 다이아몬드가 만들어지는 
기계가 있다고 하자. 이때 입력 x는 기계에 넣어주는 흑연의 양이고 상태 h는 기계 내부의 온도, 압력 또는 남아 있는 흑연의 양 등이 될 수 있다. 내부 
상태는 현재의 내부 상태와 입력으로 들어온 흑연의 양에 따라 달라진다. 또한 생산되는 다이아몬드의 양 y는 현재의 내부 상태와 입력된 흑연의 양에 따라 
달라진다.  
  
위의 식에서 추가로 중요하게 볼 부분은 입력, 상태, 출력 사이의 관계를 표현하는 A, B, C, D 모두 행렬이기 때문에 선형(linear) 관계를 가정한다는 점이다. 
딥러닝에서는 비선형(non-linear) 관계를 모델이 학습할 수 있도록 ReLU와 같은 활성 함수(activation function)를 계산에 추가해 왔다. 하지만 SSM은 
비선형 연산을 제거해서 계산의 효율성을 높인다.  
  
