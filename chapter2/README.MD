# **LLM의 중추, 트랜스포머 아키텍처 살펴보기**  
LLM(Large Language Model, 대규모 언어 모델)은 모델 크기가 큰 딥러닝 기반의 언어 모델로 2024년 현재 대부분의 LLM이 트랜스포머 아키텍처를
기반으로 한다. 따라서 트랜스포머 아키텍처를 이해하지 않고는 LLM과 관련된 기술을 정확히 이해하기 어렵다.  
  
트랜스포머 아키텍처는 언어를 이해하는 인코더(encoder)와 언어를 생성하는 디코더(decoder) 부분으로 나뉘는데 자연어 처리 모델은 이중 어떤
부분을 사용하느냐에 따라 세 가지 그룹으로 나눌 수 있다.  
  
# **트랜스포머 아키텍처란**  
트랜스포머 아키텍처는 2017년 구글의 아쉬쉬 바스와니 외 7인이 발표한 논문에서 처음 등장했다. 이 논문에서는 머신러닝을 통한 언어를 번역하는
기계 번역 성능을 높이기 위한 방법을 연구했는데 이 방법은 당시 널리 사용되던 RNN에 비해 성능 면에서 큰 폭으로 앞섰다. 또한 트랜스포머는
RNN에 비해 성능만 높은 것이 아니라 모델 학습 속도도 빨랐다. 완전히 새로운 형태의 모델이 성능과 속도 면에서 뛰어난 모습을 보이자 많은
인공지능 연구자들이 각자의 연구에 프랜스포머를 적용하기 시작했다. 현재 트랜스포머는 자연어 처리는 물론 컴퓨터 비전, 추천 시스템 등 모든 AI
분야에서 핵심 아키텍처로 사용되고 있다.  
  
기존에 자연어 처리 문제에서 사용하던 RNN은 텍스트를 순차적으로 하나씩 입력하는 형태다. 사람이 글을 읽을 때 왼쪽에서 오른쪽으로 차례대로
읽는 것처럼 딥러닝 모델에 텍스트를 순차적으로 넣어준 것이다.  
  ![img.png](image/img.png)  
x는 텍스트 토큰(token)을 의미한다. 토큰은 거의 모든 자연어 처리 연산의 기본 단위이고 보통 단어보다 짧은 텍스트 단위다. h는 입력 토큰을
RNN 모델에 입력했을 떄의 출력인데 그림에서 확인할 수 있듯이 이전 토큰의 출력을 다시 모델에 입력으로 사용하기 떄문에 입력을 병렬적으로
처리하지 못하는 구조다. 이렇게 순차적으로 처리해야 하기 떄문에 학습 속도가 느리고 입력이 길어지면 먼저 입력한 토큰의 정보가 희석되면서
성능이 떨어진다는 문제가 있다. 또한 성능을 높이기 위해 층을 깊이 쌓으면 그레이디언트 소실(gradient vanishing)이나 그레이디언트 증폭
(gradient exploding)이 발생하며 학습이 불안정했다.  
  
트랜스포머는 이런 RNN의 문제를 해결하기 위해 입력을 하나씩 순차적으로 처리하는 방식을 버리고 셀프 어텐션(self-attention)이라는 개념을
도입했다. 셀프 어텐션은 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현(representation)을 조정하는 역할을
한다.  
  
트랜스포머 아키텍처는 RNN에 비해 다음과 같은 장점을 보였다.  
- 확장성: 더 깊은 모델을 만들어도 학습이 잘된다. 동일한 블록을 반복해 사용하기 떄문에 확장이 용이하다.  
- 효율성: 학습할 때 병렬 연산이 가능하기 떄문에 학습 시간이 단축된다.  
- 더 긴 입력 처리: 입력이 길어져도 성능이 거의 떨어지지 않는다.  
  
현재 사용되는 대부분의 LLM은 트랜스포머 아키텍처를 활용하고 있고 더 정확히 말하자면 트랜스포머 아키텍처가 있었기 떄문에 대규모 언어 모델이
가능했다고 할 수 있다.  
  
![img.png](image/img2.png)  
  
트랜스포머 아키텍처는 위의 그림과 같은 요소로 구성돼 있다. 트랜스포머 아키텍처는 위의 그림만 이해하면 모두 알았다고 봐도 될 정도로 이 그림은
중요하다.  
  
영어를 한국어로 번역한다고 가정하고 그림의 화살표를 따라 트랜스포머 아키텍처를 살펴보자. 트랜스포머 아키텍처는 크게 인코더와 디코더로 나뉜다.
그림의 왼쪽 상자는 언어를 이해하는 역할을 하는 인코더이고 오른쪽 상자는 언어를 생성하는 역할을 하는 디코더다. 공통적으로 입력을 임베딩층을
통해 숫자 집합인 임베딩으로 변환하고 위치 인코딩(positional encoding)층에서 문장의 위치 정보를 더한다. 인코더에서는 층 정규화
(layer normalization), 멀티 헤드 어텐션(multi-head attention), 피드 포워드(feed forward)층을 거치며 영어 문장을 이해하고 그 결과를
그림 중간의 선에 나타나듯이 디코더로 전달한다. 디코더에서는 인코더에서와 유사하게 층 정규화, 멀티 헤드 어텐션 연산을 수행하면서 크로스
어텐션 연산을 통해 인코더가 전달한 데이터를 출력과 함께 종합해서 피드 포워드 층을 거쳐 한국어 번역 결과를 생성한다.  
  
# **텍스트를 임베딩으로 변환하기**  
컴퓨터는 텍스트를 그대로 계산에 사용할 수 없다. 따라서 텍스트를 수자 형식의 데이터로 변경해야 한다. 텍스트를 모델에 입력할 수 있는 숫자형 데이터인
임베딩으로 변환하기 위해서는 크게 세 가지 과정을 거쳐야 한다. 먼저 텍스트를 적절한 단위로 잘라 숫자형 아이디(ID)를 부여하는 토큰화(tokenization)를 
수행한다. 다음으로 토큰 아이디를 토큰 임베딩 층을 통해 여러 숫자의 집합인 토큰 임베딩으로 변환한다. 마지막으로 위치 인코딩 층을 통해 토큰의 위치 정보를 담고 
있는 위치 임베딩을 추가해 최종적으로 모델에 입력할 임베딩을 만든다.  
  
![img.png](image/img3.png)  
  
# **토큰화**  
토큰화란 텍스트를 적절한 단위로 나누고 숫자 아이디를 부여하는 것을 말한다.  

![img.png](image/img4.png)  
  
토큰화란 위 그림과 같이 작게는 자모(자음과 모음) 단위부터 크게는 단어 단위로 나눌 수 있다. 음절은 중간 정도 단위로 볼 수 있다. 토큰화를 할 때는
어떤 토큰이 어떤 숫자 아이디로 연결됐는지 기록해 둔 사전(vocabulary)을 만들어야 한다. 큰 단위를 기준으로 토큰화할수록 텍스트의 의미가 잘 유지된다는 
강점이 있지만 사전의 크기가 커진다는 단점이 있다. 단어로 토큰화를 하는 경우 텍스트에 등장하는 단어의 수만큼 토큰 아이디가 필요하기 떄문에 사전이 커진다. 
또한 이전에 본 적이 없는 새로운 단어는 사전에 없기 때문에 처리하지 못하는 OOV(Out Of Vocabulary, 사전에 없는 단어) 문제가 자주 발생한다.  
  
반대로 작은 단위로 토큰화하는 경우 사전의 크기가 작고 OOV 문제를 줄일 수 있지만 텍스트의 의미가 유지되지 않는다는 단점이 있다.  
  
![img.png](image/img5.png)  
  
작은 단위와 큰 단위 모두 각각의 장단점이 뚜렷하기 떄문에 위의 그림과 같이 데이터에 등장하는 빈도에 따라 토큰화 단위를 결정하는 서브워드(subword) 
토큰화 방식을 사용한다. 서브워드 토큰화 방식에서는 자주 나오는 단어는 단어 단위 그대로 유지하고 가끔 나오는 단어는 더 작은 단위로 나눠 텍스트의 의미를 
최대한 유지하면서 사전의 크기는 작고 효율적으로 유지할 수 있다. 한글의 경우 보통 음절과 단어 사이에서 토큰화된다.  
  
텍스트를 숫자 아이돌 바꾸는 과정은 아래의 예제와 같은 코드로 수행할 수 있다. 최근의 토큰화는 서브워드 토큰화가 기본이지만 편의를 위해 단어 단위(띄어쓰기 단위)
로 토큰화를 수행했다. 앞으로의 예제에서도 표현의 편의상 단어 단위로 토큰화한 경우가 있을 수 있는데 실제로는 서브워드 토큰화를 활용한다는 점은 변함이 없다.  

transformer_with_code.ipynb 파일에서 토큰화 코드 참조  
  
# **토큰 임베딩으로 변환하기**  
딥러닝 모델이 텍스트 데이터를 처리하기 위해서는 입력으로 들어오는 토큰과 토큰 사이의 관계를 계산할 수 있어야 한다. 토큰과 토큰 사이의 관계를 계산하기 
위해서는 토큰의 의미를 숫자로 나타낼 수 있어야 하는데 앞서 토큰화에서 부여한 토큰 아이디는 하나의 숫자일 뿐이므로 토큰의 의미를 담을 수 없다. 의미를 담기 
위해서는 최소 2개 이상의 숫자 집합인 벡터(vector)여야 한다. 데이터를 의미를 담아 숫자 집합으로 변환하는 것을 임베딩이라고 한다. 여기서는 토큰을 
임베딩으로 변환하기 떄문에 토큰 임베딩이라고 부른다.  
  
아래 예제와 같이 파이토치(PyTorch)가 제공하는 nn.Embedding 클래스를 사용하면 토큰 아이디를 토큰 임베딩으로 변환할 수 있다. 이 코드에서는 
nn.Embedding 클래스에 사전 크기가 len(str2idx)(= 5)이고 embedding_dim차원의 임베딩을 생성하는 임베딩 층인 embed_laryer를 만들고 입력 
토큰을 임베딩 층을 통해 임베딩으로 변환했다. embedding_dim을 16으로 설정해 토큰 하나를 16차원의 벡터로 변환한다. 출력 결과를 보면 1개의 문장이고 
5개의 토큰이 있고 16차원의 임베딩이 생성됐음을 확인할 수 있다.  

transformer_with_code.ipynb 파일에서 토큰 아이디에서 벡터로 변환 참조  
  
위의 코드에서 임베딩 층(embed_layer)은 토큰의 의미를 담아 벡터로 변환한 것은 아니다. 지금의 임베딩 층은 그저 입력 토큰 아이디(input_ids)를 16차원의 
임의의 숫자 집합으로 바꿔줄 뿐이다. 임베딩 층이 단어의 의미를 담기 위해서는 딥러닝 모델이 학습 데이터로 훈련되어야 한다.  
  
바로 이 지점에서 딥러닝이 기존 머신러닝과 차별화되는데 딥러닝에서는 모델이 특정 작업을 잘 수행하도록 학습하는 과정에서 데이터의 의미를 잘 담은 임베딩을 
만드는 방법도 함께 학습한다.  
  
![img.png](image/img6.png)  
  
예를 들어 위 그림과 같이 입력 텍스트가 속한 카테고리를 맞추는 텍스트 분류 모델이 있다고 하자. 딥러닝 모델은 학습 데이터를 통해 이 그림과 같이 "나는 
최근 파리 여행을 다녀왔다"라는 문장에 맞는 카테고리로 잘 분류하도록 학습된다. 그 과정에서 딥러닝 모델의 첫 번째 단계인 임베딩 층도 학습되면서 점차 토큰의 
의미를 잘 담은 임베딩을 생성하게 된다.  
  
# **위치 인코딩**  
RNN과 트랜스포머의 가장 큰 차이점은 입력을 순차적으로 처리하는지 여부다. RNN은 입력을 순차적으로 처리하는데 그렇기 때문에 자연스럽게 입력 데이터의 순서 
정보가 고려된다. 트랜스포머는 순차적인 처리 방식을 버리고 모든 입력을 동시에 처리하는데 그 과정에서 순서 정보가 사라지게 된다. 하지만 텍스트에서 순서는 매우 
중요한 정보이기 떄문에 추가해 줘야 하는데 그 역할을 위치 인코딩이 담당한다.  
  
[Attention is All you need] 논문에서는 사인과 코사인을 활용한 수식을 통해 위치에 대한 정보를 입력했다. 하지만 그 이후에는 위치 인코딩도 위치에 따른 
임베딩 층을 추가해 학습 데이터를 통해 학습하는 방식을 많이 활용하고 있다. 수식을 통해 위치 정보를 추가하는 방식이나 임베딩으로 위치 정보를 학습하는 방식 
모두 결국 모델로 추론을 수행하는 시점에서는 입력 토큰의 위치에 따라 고정된 임베딩을 더해주기 때문에 이를 절대적 위치 인코딩(absolute position encoding)
이라고 부른다.  
  
절대적 위치 인코딩 방식은 간단하게 구현할 수 있다는 장점이 있지만 토큰과 토큰 사이의 상대적인 위치 정보는 활용하지 못하고 학습 데이터에서 보기 어려웠던 긴 
텍스트를 추론하는 경우에는 성능이 떨어진다는 문제가 있어 최근에는 상대적 위치 인코딩(relative position encoding)방식도 많이 활용된다. 지금은 트랜스포머가 모든 
입력 토큰을 동등하게 처리하기 때문에 입력으로 위치 정보를 함께 더해준다는 사실만 기억하면 충분하다.  
  
절대적 위치 인코등 중 위치 정보를 학습하는 방식은 아래 예제와 같이 새로운 임베딩 층을 하나 추가하고 위치 인덱스(position_ids)에 따라 임베딩을 더하도록 
구현할 수 있다. 이 코드에서는 최대 토큰 수(max_position)를 12로 설정하여 위치 인코딩을 생성하는 위치 임베딩 층(position_embed_layer)을 정의했다. 
위치 아이디(position_ids)에는 0부터 입력 토큰의 수(input_ids.size(1))까지 1씩 증가하도록 데이터를 생성한다. position_ids를 위치 임베딩 층에 입력해 
위치 인코딩(position_encodings)을 생성하고 토큰 임베딩(token_embeddings)에 위치 인코딩을 더해 모델에 입력할 최종 입력 임베딩(input_embeddings)을 준비한다.  

transformer_with_code.ipynb 파일에서 절대적 위치 인코딩 참조  
  


  

  

  
