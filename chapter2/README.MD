# **LLM의 중추, 트랜스포머 아키텍처 살펴보기**  
LLM(Large Language Model, 대규모 언어 모델)은 모델 크기가 큰 딥러닝 기반의 언어 모델로 2024년 현재 대부분의 LLM이 트랜스포머 아키텍처를
기반으로 한다. 따라서 트랜스포머 아키텍처를 이해하지 않고는 LLM과 관련된 기술을 정확히 이해하기 어렵다.  
  
트랜스포머 아키텍처는 언어를 이해하는 인코더(encoder)와 언어를 생성하는 디코더(decoder) 부분으로 나뉘는데 자연어 처리 모델은 이중 어떤
부분을 사용하느냐에 따라 세 가지 그룹으로 나눌 수 있다.  
  
# **트랜스포머 아키텍처란**  
트랜스포머 아키텍처는 2017년 구글의 아쉬쉬 바스와니 외 7인이 발표한 논문에서 처음 등장했다. 이 논문에서는 머신러닝을 통한 언어를 번역하는
기계 번역 성능을 높이기 위한 방법을 연구했는데 이 방법은 당시 널리 사용되던 RNN에 비해 성능 면에서 큰 폭으로 앞섰다. 또한 트랜스포머는
RNN에 비해 성능만 높은 것이 아니라 모델 학습 속도도 빨랐다. 완전히 새로운 형태의 모델이 성능과 속도 면에서 뛰어난 모습을 보이자 많은
인공지능 연구자들이 각자의 연구에 프랜스포머를 적용하기 시작했다. 현재 트랜스포머는 자연어 처리는 물론 컴퓨터 비전, 추천 시스템 등 모든 AI
분야에서 핵심 아키텍처로 사용되고 있다.  
  
기존에 자연어 처리 문제에서 사용하던 RNN은 텍스트를 순차적으로 하나씩 입력하는 형태다. 사람이 글을 읽을 때 왼쪽에서 오른쪽으로 차례대로
읽는 것처럼 딥러닝 모델에 텍스트를 순차적으로 넣어준 것이다.  
  ![img.png](image/img.png)  
x는 텍스트 토큰(token)을 의미한다. 토큰은 거의 모든 자연어 처리 연산의 기본 단위이고 보통 단어보다 짧은 텍스트 단위다. h는 입력 토큰을
RNN 모델에 입력했을 떄의 출력인데 그림에서 확인할 수 있듯이 이전 토큰의 출력을 다시 모델에 입력으로 사용하기 떄문에 입력을 병렬적으로
처리하지 못하는 구조다. 이렇게 순차적으로 처리해야 하기 떄문에 학습 속도가 느리고 입력이 길어지면 먼저 입력한 토큰의 정보가 희석되면서
성능이 떨어진다는 문제가 있다. 또한 성능을 높이기 위해 층을 깊이 쌓으면 그레이디언트 소실(gradient vanishing)이나 그레이디언트 증폭
(gradient exploding)이 발생하며 학습이 불안정했다.  
  
트랜스포머는 이런 RNN의 문제를 해결하기 위해 입력을 하나씩 순차적으로 처리하는 방식을 버리고 셀프 어텐션(self-attention)이라는 개념을
도입했다. 셀프 어텐션은 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현(representation)을 조정하는 역할을
한다.  
  
트랜스포머 아키텍처는 RNN에 비해 다음과 같은 장점을 보였다.  
- 확장성: 더 깊은 모델을 만들어도 학습이 잘된다. 동일한 블록을 반복해 사용하기 떄문에 확장이 용이하다.  
- 효율성: 학습할 때 병렬 연산이 가능하기 떄문에 학습 시간이 단축된다.  
- 더 긴 입력 처리: 입력이 길어져도 성능이 거의 떨어지지 않는다.  
  
현재 사용되는 대부분의 LLM은 트랜스포머 아키텍처를 활용하고 있고 더 정확히 말하자면 트랜스포머 아키텍처가 있었기 떄문에 대규모 언어 모델이
가능했다고 할 수 있다.  
  
![img.png](image/img2.png)  
  
트랜스포머 아키텍처는 위의 그림과 같은 요소로 구성돼 있다. 트랜스포머 아키텍처는 위의 그림만 이해하면 모두 알았다고 봐도 될 정도로 이 그림은
중요하다.  
  
영어를 한국어로 번역한다고 가정하고 그림의 화살표를 따라 트랜스포머 아키텍처를 살펴보자. 트랜스포머 아키텍처는 크게 인코더와 디코더로 나뉜다.
그림의 왼쪽 상자는 언어를 이해하는 역할을 하는 인코더이고 오른쪽 상자는 언어를 생성하는 역할을 하는 디코더다. 공통적으로 입력을 임베딩층을
통해 숫자 집합인 임베딩으로 변환하고 위치 인코딩(positional encoding)층에서 문장의 위치 정보를 더한다. 인코더에서는 층 정규화
(layer normalization), 멀티 헤드 어텐션(multi-head attention), 피드 포워드(feed forward)층을 거치며 영어 문장을 이해하고 그 결과를
그림 중간의 선에 나타나듯이 디코더로 전달한다. 디코더에서는 인코더에서와 유사하게 층 정규화, 멀티 헤드 어텐션 연산을 수행하면서 크로스
어텐션 연산을 통해 인코더가 전달한 데이터를 출력과 함께 종합해서 피드 포워드 층을 거쳐 한국어 번역 결과를 생성한다.  
  
# **텍스트를 임베딩으로 변환하기**  
컴퓨터는 텍스트를 그대로 계산에 사용할 수 없다. 따라서 텍스트를 수자 형식의 데이터로 변경해야 한다. 텍스트를 모델에 입력할 수 있는 숫자형 데이터인
임베딩으로 변환하기 위해서는 크게 세 가지 과정을 거쳐야 한다. 먼저 텍스트를 적절한 단위로 잘라 숫자형 아이디(ID)를 부여하는 토큰화(tokenization)를 
수행한다. 다음으로 토큰 아이디를 토큰 임베딩 층을 통해 여러 숫자의 집합인 토큰 임베딩으로 변환한다. 마지막으로 위치 인코딩 층을 통해 토큰의 위치 정보를 담고 
있는 위치 임베딩을 추가해 최종적으로 모델에 입력할 임베딩을 만든다.  
  
![img.png](image/img3.png)  
  
# **토큰화**  
토큰화란 텍스트를 적절한 단위로 나누고 숫자 아이디를 부여하는 것을 말한다.  

![img.png](image/img4.png)  
  
토큰화란 위 그림과 같이 작게는 자모(자음과 모음) 단위부터 크게는 단어 단위로 나눌 수 있다. 음절은 중간 정도 단위로 볼 수 있다. 토큰화를 할 때는
어떤 토큰이 어떤 숫자 아이디로 연결됐는지 기록해 둔 사전(vocabulary)을 만들어야 한다. 큰 단위를 기준으로 토큰화할수록 텍스트의 의미가 잘 유지된다는 
강점이 있지만 사전의 크기가 커진다는 단점이 있다. 단어로 토큰화를 하는 경우 텍스트에 등장하는 단어의 수만큼 토큰 아이디가 필요하기 떄문에 사전이 커진다. 
또한 이전에 본 적이 없는 새로운 단어는 사전에 없기 때문에 처리하지 못하는 OOV(Out Of Vocabulary, 사전에 없는 단어) 문제가 자주 발생한다.  
  
반대로 작은 단위로 토큰화하는 경우 사전의 크기가 작고 OOV 문제를 줄일 수 있지만 텍스트의 의미가 유지되지 않는다는 단점이 있다.  
  
![img.png](image/img5.png)  
  
작은 단위와 큰 단위 모두 각각의 장단점이 뚜렷하기 떄문에 위의 그림과 같이 데이터에 등장하는 빈도에 따라 토큰화 단위를 결정하는 서브워드(subword) 
토큰화 방식을 사용한다. 서브워드 토큰화 방식에서는 자주 나오는 단어는 단어 단위 그대로 유지하고 가끔 나오는 단어는 더 작은 단위로 나눠 텍스트의 의미를 
최대한 유지하면서 사전의 크기는 작고 효율적으로 유지할 수 있다. 한글의 경우 보통 음절과 단어 사이에서 토큰화된다.  
  
텍스트를 숫자 아이돌 바꾸는 과정은 아래의 예제와 같은 코드로 수행할 수 있다. 최근의 토큰화는 서브워드 토큰화가 기본이지만 편의를 위해 단어 단위(띄어쓰기 단위)
로 토큰화를 수행했다. 앞으로의 예제에서도 표현의 편의상 단어 단위로 토큰화한 경우가 있을 수 있는데 실제로는 서브워드 토큰화를 활용한다는 점은 변함이 없다.

  

  

  
