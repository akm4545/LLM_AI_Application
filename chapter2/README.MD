# **LLM의 중추, 트랜스포머 아키텍처 살펴보기**  
LLM(Large Language Model, 대규모 언어 모델)은 모델 크기가 큰 딥러닝 기반의 언어 모델로 2024년 현재 대부분의 LLM이 트랜스포머 아키텍처를  
기반으로 한다. 따라서 트랜스포머 아키텍처를 이해하지 않고는 LLM과 관련된 기술을 정확히 이해하기 어렵다.  
  
트랜스포머 아키텍처는 언어를 이해하는 인코더(encoder)와 언어를 생성하는 디코더(decoder) 부분으로 나뉘는데 자연어 처리 모델은 이중 어떤  
부분을 사용하느냐에 따라 세 가지 그룹으로 나눌 수 있다.  
  
# **트랜스포머 아키텍처란**  
트랜스포머 아키텍처는 2017년 구글의 아쉬쉬 바스와니 외 7인이 발표한 논문에서 처음 등장했다. 이 논문에서는 머신러닝을 통한 언어를 번역하는  
기계 번역 성능을 높이기 위한 방법을 연구했는데 이 방법은 당시 널리 사용되던 RNN에 비해 성능 면에서 큰 폭으로 앞섰다. 또한 트랜스포머는  
RNN에 비해 성능만 높은 것이 아니라 모델 학습 속도도 빨랐다. 완전히 새로운 형태의 모델이 성능과 속도 면에서 뛰어난 모습을 보이자 많은  
인공지능 연구자들이 각자의 연구에 프랜스포머를 적용하기 시작했다. 현재 트랜스포머는 자연어 처리는 물론 컴퓨터 비전, 추천 시스템 등 모든 AI  
분야에서 핵심 아키텍처로 사용되고 있다.  
  
기존에 자연어 처리 문제에서 사용하던 RNN은 텍스트를 순차적으로 하나씩 입력하는 형태다. 사람이 글을 읽을 때 왼쪽에서 오른쪽으로 차례대로  
읽는 것처럼 딥러닝 모델에 텍스트를 순차적으로 넣어준 것이다.  
  ![img.png](image/img.png)  
x는 텍스트 토큰(token)을 의미한다. 토큰은 거의 모든 자연어 처리 연산의 기본 단위이고 보통 단어보다 짧은 텍스트 단위다. h는 입력 토큰을  
RNN 모델에 입력했을 떄의 출력인데 그림에서 확인할 수 있듯이 이전 토큰의 출력을 다시 모델에 입력으로 사용하기 떄문에 입력을 병렬적으로  
처리하지 못하는 구조다. 이렇게 순차적으로 처리해야 하기 떄문에 학습 속도가 느리고 입력이 길어지면 먼저 입력한 토큰의 정보가 희석되면서  
성능이 떨어진다는 문제가 있다. 또한 성능을 높이기 위해 층을 깊이 쌓으면 그레이디언트 소실(gradient vanishing)이나 그레이디언트 증폭  
(gradient exploding)이 발생하며 학습이 불안정했다.  
  
트랜스포머는 이런 RNN의 문제를 해결하기 위해 입력을 하나씩 순차적으로 처리하는 방식을 버리고 셀프 어텐션(self-attention)이라는 개념을  
도입했다. 셀프 어텐션은 입력된 문장 내의 각 단어가 서로 어떤 관련이 있는지 계산해서 각 단어의 표현(representation)을 조정하는 역할을  
한다.  
  
트랜스포머 아키텍처는 RNN에 비해 다음과 같은 장점을 보였다.  
- 확장성: 더 깊은 모델을 만들어도 학습이 잘된다. 동일한 블록을 반복해 사용하기 떄문에 확장이 용이하다.  
- 효율성: 학습할 때 병렬 연산이 가능하기 떄문에 학습 시간이 단축된다.  
- 더 긴 입력 처리: 입력이 길어져도 성능이 거의 떨어지지 않는다.  
  
현재 사용되는 대부분의 LLM은 트랜스포머 아키텍처를 활용하고 있고 더 정확히 말하자면 트랜스포머 아키텍처가 있었기 떄문에 대규모 언어 모델이  
가능했다고 할 수 있다.  
  
![img.png](img2.png)  
  
트랜스포머 아키텍처는 위의 그림과 같은 요소로 구성돼 있다. 트랜스포머 아키텍처는 위의 그림만 이해하면 모두 알았다고 봐도 될 정도로 이 그림은  
중요하다.  
  
영어를 한국어로 번역한다고 가정하고 그림의 화살표를 따라 트랜스포머 아키텍처를 살펴보자. 트랜스포머 아키텍처는 크게 인코더와 디코더로 나뉜다.  
그림의 왼쪽 상자는 언어를 이해하는 역할을 하는 인코더이고 오른쪽 상자는 언어를 생성하는 역할을 하는 디코더다. 공통적으로 입력을 임베딩층을  
통해 숫자 집합인 임베딩으로 변환하고 위치 인코딩(positional encoding)층에서 문장의 위치 정보를 더한다. 인코더에서는 층 정규화  
(layer normalization), 멀티 헤드 어텐션(multi-head attention), 피드 포워드(feed forward)층을 거치며 영어 문장을 이해하고 그 결과를  
그림 중간의 선에 나타나듯이 디코더로 전달한다. 디코더에서는 인코더에서와 유사하게 층 정규화, 멀티 헤드 어텐션 연산을 수행하면서 크로스  
어텐션 연산을 통해 인코더가 전달한 데이터를 출력과 함께 종합해서 피드 포워드 층을 거쳐 한국어 번역 결과를 생성한다.  
  

  
