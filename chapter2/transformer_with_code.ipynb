{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **토큰화 코드**"
   ],
   "metadata": {
    "id": "c61R8-_wIDt_",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pCjTUSeMA6Sq",
    "outputId": "30c74608-b058-4b2a-f473-74a31daab5e9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "input_text_list: ['나는', '최근', '파리', '여행을', '다녀왔다']\n",
      "str2idx: {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n",
      "idx2str: {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n",
      "input_ids: [0, 1, 2, 3, 4]\n"
     ]
    }
   ],
   "source": [
    "# 띄어쓰기 단위로 분리\n",
    "input_text = \"나는 최근 파리 여행을 다녀왔다\"\n",
    "input_text_list = input_text.split()\n",
    "\n",
    "print(\"input_text_list:\", input_text_list)\n",
    "\n",
    "# 토큰 -> 아이디 딕셔너리와 아이디 -> 토큰 딕셔너리 만들기\n",
    "str2idx = {word:idx for idx , word in enumerate(input_text_list)}\n",
    "idx2str = {idx:word for idx , word in enumerate(input_text_list)}\n",
    "\n",
    "print(\"str2idx:\", str2idx)\n",
    "print(\"idx2str:\", idx2str)\n",
    "\n",
    "# 토큰을 토큰 아이디로 변환\n",
    "input_ids = [str2idx[word] for word in input_text_list]\n",
    "print(\"input_ids:\", input_ids)\n",
    "\n",
    "# 출력 결과\n",
    "# input_text_list: ['나는', '최근', '파리', '여행을', '다녀왔다']\n",
    "# str2idx: {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n",
    "# idx2str: {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n",
    "# input_ids: [0, 1, 2, 3, 4]"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **토큰 아이디에서 벡터로 변환**\n"
   ],
   "metadata": {
    "id": "sZj7s_y_7fOw",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "embedding_dim = 16\n",
    "# len(str2idx) = 전체 단어의 길이 추출\n",
    "# str2idx: {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4} = 길이 5\n",
    "\n",
    "# embedding_dim = 차원 = 16\n",
    "\n",
    "# embed_layer = 위의 설정으로 임베딩 층을 만든다\n",
    "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
    "\n",
    "# torch.tensor = 텐서로 변환 / 리스트나 배열은 GPU 사용이 불가능해서 모델이 이해할 수 있게 바꿈\n",
    "# torch.tensor(input_ids) = 토큰 아이디를 텐서로 변환\n",
    "# embed_layer(torch.tensor(input_ids)) = 토큰 아이디를 16차원의 임의의 숫자 집합의 벡터로 변환\n",
    "input_embeddings = embed_layer(torch.tensor(input_ids)) #(5, 16)\n",
    "\n",
    "# unsqueeze = 텐서에 차원을 추가\n",
    "# 딥러닝 모델은 데이터를 배치로 처리하기 때문에 한개가 있더라도 배치 형태로 만들어야 모델이 처리할 수 있다\n",
    "input_embeddings = input_embeddings.unsqueeze(0) # (1, 5, 16)\n",
    "input_embeddings.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0s6Qvsaz7kBN",
    "outputId": "7c39ba28-0bb2-4fb4-b35d-1d5eb74d02b2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 9,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "metadata": {},
     "execution_count": 9
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **절대적 위치 인코딩**"
   ],
   "metadata": {
    "id": "DT_2YLQiEorf",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "embedding_dim = 16\n",
    "max_position = 12\n",
    "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
    "# 기존의 임베딩 층에 추가할 위치 임베딩 층\n",
    "# 위치 인코딩을 생성하는 위치 임베딩 층\n",
    "# max_position = 12 = 최대 토큰 수 -> 12개\n",
    "position_embed_layer = nn.Embedding(max_position, embedding_dim)\n",
    "\n",
    "# torch.arange = 0 부터 시작하는 1차원 텐서 생성\n",
    "# dtype=torch.long = long 타입으로 생성 -> 위치 인덱스를 나타내기에 알맞은 타입\n",
    "# unsqueeze = 배치 처리하기 위해 차원 추가\n",
    "position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
    "# 위치 아이디를 임베딩 층에 입력해 위치 인코딩 설정 (벡터)\n",
    "position_encodings = position_embed_layer(position_ids)\n",
    "token_embeddings = embed_layer(torch.tensor(input_ids))\n",
    "token_embeddings = token_embeddings.unsqueeze(0)\n",
    "\n",
    "# 토큰 임베딩에 위치 인코딩을 더해 모델에 입력할 최종 입력 임베딩 준비\n",
    "input_embeddings = token_embeddings + position_encodings\n",
    "input_embeddings.shape"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BRQ2WbXjEsJo",
    "outputId": "07b7bbec-b099-455f-c199-93850411db85",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 10,
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "torch.Size([1, 5, 16])"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **쿼리, 키, 값 벡터를 만드는 nn.Linear층**"
   ],
   "metadata": {
    "id": "wxnuoVXum1-s",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "head_dim = 16\n",
    "\n",
    "# 쿼리, 키, 값을 계산하기 위한 변환\n",
    "# 학습 가능한 가중치를 만들기 위해 선형 층을 사용\n",
    "# 모델이 학습을 통해 어떤 특징이 중요한지 알아서 학습하도록 도와줌\n",
    "# 선형층은 단순히 가중치를 곱하는 연산이지만,\n",
    "# 이 가중치가 학습됨에 따라 입력 벡터에서 중요한 특징을 강조하고, 덜 중요한 특징을 약화할 수 있다.\n",
    "\n",
    "# embedding_dim = 입력차원, head_dim = 출력차원?\n",
    "weight_q = nn.Linear(embedding_dim, head_dim)\n",
    "weight_k = nn.Linear(embedding_dim, head_dim)\n",
    "weight_v = nn.Linear(embedding_dim, head_dim)\n",
    "\n",
    "# 변환 수행\n",
    "# 만들어낸 가중치에 임베딩을 통과시킴\n",
    "querys = weight_q(input_embeddings) # (1, 5, 16)\n",
    "keys = weight_k(input_embeddings) # (1, 5, 16)\n",
    "values = weight_v(input_embeddings) # (1, 5, 16)"
   ],
   "metadata": {
    "id": "uF2Yj6zynI1s",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **스케일 점곱 방식의 어텐션**"
   ],
   "metadata": {
    "id": "SLaEVWPqThyS",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from math import sqrt\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def compute_attention(querys, keys, values, is_causal=False):\n",
    "  # querys.size(-1) querys의 마지막 차원의 크기를 가져옴\n",
    "  # dim_k = 16과 같음(head_dim)\n",
    "  dim_k = querys.size(-1) #16\n",
    "\n",
    "  # @ 연산자는 행렬 곱셈(MatMul)을 수행하는 연산자\n",
    "  # Python 3.5 이상에서는 NumPy와 PyTorch에서 @연산자가 matmul()과 동일하게 동작한다\n",
    "  # transpose 두개의 차원을 서로 바꿔주는 PyTorch 함수 (안의 인자는 substring 작동방식과 유사)\n",
    "\n",
    "  # 분산이 커지는 것을 방지하기 위해 임베딩 차원 수(dim_k)의 제곱근으로 나눈다 sqrt(dim_k)\n",
    "  # 정규화 과정 진행\n",
    "  scores = querys @ keys.transpose(-2, -1) / sqrt(dim_k)\n",
    "  # 소프트 맥스 함수를 사용하여 다중 분류의 여러 선형 방정식의 출력 값을 정규화 (확률 분포로 변환 -> 각 쿼리에 대해 모든 키의 중요도를 확률 값으로 변환)\n",
    "  # dim=1 -> 시퀀스 길이 차원에 대해 정규화\n",
    "\n",
    "  # 결과적으로 weights는 가중치 행렬이 된다\n",
    "  weights = F.softmax(scores, dim=1)\n",
    "\n",
    "  # 가중치와 값을 곱해 입력과 동일한 형태의 출력을 반환한다\n",
    "  return weights @ values"
   ],
   "metadata": {
    "id": "8gjqtgvvTvba",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **어텐션 연산의 입력과 출력**"
   ],
   "metadata": {
    "id": "euj2_nK6cw10",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "print(\"원본 입력 형태: \", input_embeddings.shape)\n",
    "\n",
    "after_attention_embeddings = compute_attention(querys, keys, values)\n",
    "\n",
    "print(\"어텐션 적용 후 형태: \", after_attention_embeddings.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0-V8mqcfc0CO",
    "outputId": "9fae8e69-8f85-471e-fa9d-fd05520796a2",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 13,
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "원본 입력 형태:  torch.Size([1, 5, 16])\n",
      "어텐션 적용 후 형태:  torch.Size([1, 5, 16])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# **어텐션 연산을 수행하는 AttentionHead 클래스**"
   ],
   "metadata": {
    "id": "ZRlVusAAYubJ",
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 파이썬 상속\n",
    "# PyTorch의 nn.Module 상속\n",
    "class AttentionHead(nn.Module):\n",
    "  # 생성자\n",
    "  def __init__(self, token_embed_dim, head_dim, is_causal=False):\n",
    "    super().__init__()\n",
    "    self.is_causal = is_causal\n",
    "    self.weight_q = nn.Linear(token_embed_dim, head_dim) #쿼리 벡터 생성을 위한 선형 층\n",
    "    self.weight_k = nn.Linear(token_embed_dim, head_dim) #키 벡터 생성을 위한 선형 층\n",
    "    self.weight_v = nn.Linear(token_embed_dim, head_dim) #값 벡터 생성을 위한 선형 층\n",
    "\n",
    "  def forward(self, querys, keys, values):\n",
    "    # 위에서 관계도 계산을 위해 만들어둔 함수 재활용\n",
    "    outputs = compute_attention(\n",
    "        self.weight_q(querys), #쿼리 벡터\n",
    "        self.weight_k(keys), #키 벡터\n",
    "        self.weight_v(values), #값 벡터\n",
    "        is_causal = self.is_causal\n",
    "    )\n",
    "\n",
    "    return outputs\n",
    "\n",
    "# 클래스 생성\n",
    "attention_head = AttentionHead(embedding_dim, embedding_dim)\n",
    "after_attention_embeddings = attention_head(input_embeddings, input_embeddings, input_embeddings)\n"
   ],
   "metadata": {
    "id": "c7XK-ZGaYyWg",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 14,
   "outputs": []
  }
 ]
}