{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **토큰화 코드**"
      ],
      "metadata": {
        "id": "c61R8-_wIDt_"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pCjTUSeMA6Sq",
        "outputId": "6b1c40fc-4370-44ef-e2ad-31d03729c20f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_text_list: ['나는', '최근', '파리', '여행을', '다녀왔다']\n",
            "str2idx: {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n",
            "idx2str: {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n",
            "input_ids: [0, 1, 2, 3, 4]\n"
          ]
        }
      ],
      "source": [
        "# 띄어쓰기 단위로 분리\n",
        "input_text = \"나는 최근 파리 여행을 다녀왔다\"\n",
        "input_text_list = input_text.split()\n",
        "\n",
        "print(\"input_text_list:\", input_text_list)\n",
        "\n",
        "# 토큰 -> 아이디 딕셔너리와 아이디 -> 토큰 딕셔너리 만들기\n",
        "str2idx = {word:idx for idx , word in enumerate(input_text_list)}\n",
        "idx2str = {idx:word for idx , word in enumerate(input_text_list)}\n",
        "\n",
        "print(\"str2idx:\", str2idx)\n",
        "print(\"idx2str:\", idx2str)\n",
        "\n",
        "# 토큰을 토큰 아이디로 변환\n",
        "input_ids = [str2idx[word] for word in input_text_list]\n",
        "print(\"input_ids:\", input_ids)\n",
        "\n",
        "# 출력 결과\n",
        "# input_text_list: ['나는', '최근', '파리', '여행을', '다녀왔다']\n",
        "# str2idx: {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4}\n",
        "# idx2str: {0: '나는', 1: '최근', 2: '파리', 3: '여행을', 4: '다녀왔다'}\n",
        "# input_ids: [0, 1, 2, 3, 4]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **토큰 아이디에서 벡터로 변환**\n"
      ],
      "metadata": {
        "id": "sZj7s_y_7fOw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "embedding_dim = 16\n",
        "# len(str2idx) = 전체 단어의 길이 추출\n",
        "# str2idx: {'나는': 0, '최근': 1, '파리': 2, '여행을': 3, '다녀왔다': 4} = 길이 5\n",
        "\n",
        "# embedding_dim = 차원 = 16\n",
        "\n",
        "# embed_layer = 위의 설정으로 임베딩 층을 만든다\n",
        "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
        "\n",
        "# torch.tensor = 텐서로 변환 / 리스트나 배열은 GPU 사용이 불가능해서 모델이 이해할 수 있게 바꿈\n",
        "# torch.tensor(input_ids) = 토큰 아이디를 텐서로 변환\n",
        "# embed_layer(torch.tensor(input_ids)) = 토큰 아이디를 16차원의 임의의 숫자 집합의 벡터로 변환\n",
        "input_embeddings = embed_layer(torch.tensor(input_ids)) #(5, 16)\n",
        "\n",
        "# unsqueeze = 텐서에 차원을 추가\n",
        "# 딥러닝 모델은 데이터를 배치로 처리하기 때문에 한개가 있더라도 배치 형태로 만들어야 모델이 처리할 수 있다\n",
        "input_embeddings = input_embeddings.unsqueeze(0) # (1, 5, 16)\n",
        "input_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0s6Qvsaz7kBN",
        "outputId": "8aaa8ccd-c10e-48f7-d065-614634cac749"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **절대적 위치 인코딩**"
      ],
      "metadata": {
        "id": "DT_2YLQiEorf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 16\n",
        "max_position = 12\n",
        "embed_layer = nn.Embedding(len(str2idx), embedding_dim)\n",
        "# 기존의 임베딩 층에 추가할 위치 임베딩 층\n",
        "# 위치 인코딩을 생성하는 위치 임베딩 층\n",
        "# max_position = 12 = 최대 토큰 수 -> 12개\n",
        "position_embed_layer = nn.Embedding(max_position, embedding_dim)\n",
        "\n",
        "# torch.arange = 0 부터 시작하는 1차원 텐서 생성\n",
        "# dtype=torch.long = long 타입으로 생성 -> 위치 인덱스를 나타내기에 알맞은 타입\n",
        "# unsqueeze = 배치 처리하기 위해 차원 추가\n",
        "position_ids = torch.arange(len(input_ids), dtype=torch.long).unsqueeze(0)\n",
        "# 위치 아이디를 임베딩 층에 입력해 위치 인코딩 설정 (벡터)\n",
        "position_encodings = position_embed_layer(position_ids)\n",
        "token_embeddings = embed_layer(torch.tensor(input_ids))\n",
        "token_embeddings = token_embeddings.unsqueeze(0)\n",
        "\n",
        "# 토큰 임베딩에 위치 인코딩을 더해 모델에 입력할 최종 입력 임베딩 준비\n",
        "input_embeddings = token_embeddings + position_encodings\n",
        "input_embeddings.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BRQ2WbXjEsJo",
        "outputId": "4380958f-78a3-4347-84fa-d631f910bd86"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "torch.Size([1, 5, 16])"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    }
  ]
}