# **임베딩 모델로 데이터 의미 압축하기**  
검색 증강 생성(RAG)는 프롬프트의 요청의 맥락 정보를 추가해 LLM의 생성 품질을 향상하는 기술이다. 맥락 정보를 저장하고 검색하기 위해서는 텍스트를 
임베딩 벡터로 변환해야 하는데 이때 임베딩 모델을 사용한다. 지금까지 임베딩 모델의 활용에만 집중할 수 있도록 상업용 임베딩 모델인 OpenAI의 text-embedding-ada-002를 
사용했다.  
  
텍스트를 숫자로 표현하지만 의미를 담지는 못하는 원핫 인코딩(one-hot encoding)부터 문장을 벡터로 표현하는 문장 임베딩(sentence embedding)까지 
AI 분야에서는 텍스트를 의미를 담아 더 압축적인 임베딩 벡터를 만드는 방향으로 발전해 왔다.  
  
텍스트 임베딩 모델은 문장의 의미를 담아 임베딩 벡터로 변환하기 때문에 문자열이 동일하지 않더라도 검색할 수 있다. 임베딩 벡터의 유사도를 기반으로 
검색하는 방법을 의미 검색(semantic search)이라고 부른다.  
  
의미 검색은 검색할 때 벡터 유사도를 활용하기 때문에 의미상 유사한 문서를 찾을 수 있다는 장점이 있지만 문자열을 비교하는 키워드 검색 방식보다는 
관련도가 떨어지는 문서가 검색될 수 있다는 단점이 있다. 이런 단점을 보완하기 위해 키워드 검색과 의미 검색을 조합해 사용하는 하이브리드 검색을 사용할 
수 있다. 대표적인 키워드 검색 방식인 BM25, 그리고 키워드 검색과 의미 검색의 순위를 조합해 최종 순위를 산정할 때 사용하는 RRF(Reciprocal Rank Fusion)를 
알아본다.  
  
하단 명령어를 실행해 라이브러리를 다운로드 한다.  
  
!pip install transformers==4.40.1 datasets==2.19.0 sentence-transformers==2.7.0  
faiss-cpu==1.8.0 llama-index==0.10.34 llama-index-embeddings-huggingface==0.2.0 -qqq  
  
# **텍스트 임베딩 이해하기**  
9장에서는 "북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?"이라는 질문에 관련된 기사 본문을 찾아 프롬프트에 추가했다. 여기서 질문과 
기사가 문자열 그대로 일치하지 않는데 검색할 수 있었다는 점이 중요하다. 여러 문장의 텍스트를 임베딩 벡터로 변환하는 방식을 텍스트 임베딩 또는 문장 임베딩
이라고 부른다. 문장 임베딩 방식은 이제 널리 사용되지만 최근까지도 텍스트의 의미를 담아 임베딩 벡터로 표현하는 과제는 어려운 문제였다.  
  
임베딩(embedding)이란 '데이터의 의미를 압축한 숫자 배열(벡터)'을 말한다. 기본적으로 컴퓨터는 숫자 형식의 데이터만 연산할 수 있기 때문에 컴퓨터로 
처리하기 위해서는 텍스트, 이미지, 음성 등 모든 데이터를 숫자 형식으로 바꿔야 한다. 이때 가능하면 데이터의 의미를 담아 숫자로 변환할 수 있다면 
좋을 텐데 비교적 최근까지도 어려운 과제였다.  
  
# **문장 임베딩 방식의 장점**  
만약 데이터의 의미를 숫자로 표현할 수 있다면 데이터가 서로 유사한지, 관련이 있는지와 같이 중요한 정보를 활용할 수 있다. 예를 들어 '학교', '공부', 
'운동'이라는 세 단어가 있을 때 아래 예제를 사용해 임베딩으로 변환하면 단어 간의 코사인 유사도를 통해 '학교'가 '공부'와 0.595만큼 유사하고 '학교'와 
'운동'은 0.325만큼 유사하다는 정보를 활용할 수 있다.  
  
chapter10.ipynb 파일에서 문장 임베딩을 활용한 단어 간 유사도 계산 참조  
  
문장 임베딩 방식을 사용하면 서로 다른 텍스트를 마치 사람이 이해하는 것처럼 서로 유사한지, 서로 관련이 있는지 판단할 수 있다는 장점이 있다.  
  
# **원핫 인코딩**  
앞서 활용한 예시를 그대로 활용해 '학교', '공부', '운동'이라는 세 가지 단어가 있다고 하자. 세 데이터를 숫자로 변환한다고 하면 다음과 같이 간단히 
1, 2, 3을 아이디로 붙일 수 있다.  
  
학교 - 1, 공부 - 2, 운동 - 3  
  
숫자로 변환은 했지만 이 방법을 사용할 경우 공부가 학교의 두 배(2 = 1 * 2)라거나 운동이 학교의 세 배(3 = 1 * 3)라는 오해가 생길 수 있다. 이런 
문제를 피하기 위해 다음과 같이 표현하는 것을 원핫 인코딩(one-hot encoding)이라고 한다.  
  
학교 - [1, 0, 0], 공부 - [0, 1, 0], 운동 - [0, 0, 1]  
  
이 방식을 사용하면 '식사'라는 새로운 데이터를 추가해도 아래처럼 독립적으로 추가할 수 있고 단어와 단어 사이에 아무런 관계도 나타내지 않는다.  
  
학교 - [1, 0, 0, 0], 공부 - [0, 1, 0, 0], 운동 - [0, 0, 1, 0], 식사 - [0, 0, 0, 1]  
  
원핫 인코딩은 범주형 데이터 사이에 의도하지 않은 관계가 담기는 걸 방지한다는 장점이 있지만 충분히 관련이 있는 단어 사이의 관계도 표현할 수 없다는 
치명적인 단점이 있다. '학교'와 '공부'사이에는 '무언가를 배운다'는 공통점이 있지만 원핫 인코딩에서는 그 의미를 전혀 살릴 수 없다. 아래 예제의 코드를 
실행하면 원핫 인코딩으로 표현했을 때 학교와 공부의 유사도는 0이고 학교와 운동의 유사도도 0이다.  
  
chapter10.ipynb 파일에서 원핫 인코딩의 한계 참조  
  
# **백오브워즈**  
백오브워즈(Bag of Words)는 '비슷한 단어가 많이 나오면 비슷한 문장 또는 문서'라는 가정을 활용해 문서를 숫자로 변환한다. 백오브워즈는 단어의 순서에 
관계없이 해당 문서에 등장한 단어와 그 등장 횟수를 집계한다. 예를 들어 경제 기사 2개와 IT 기사 2개에 등장하는 단어 빈도가 아래 표와 같다고 하자.  
  
![img.png](image/img.png)  
  
경제 기사라면 다루는 내용에 따라 차이는 있겠지만 기본적으로 '대출', '증시', '부동산'과 같은 단어를 사용할 가능성이 높다. 물론 IT 기사에도 '부동산' 
같은 단어가 등장할 수 있지만 일반적으로 경제 기사보다는 낮은 빈도를 보일 것이다. 백오브워즈는 아이디어가 직관적이고 구현이 간단함에도 훌륭히 작동하기 
떄문에 문장과 문서의 의미를 표현하는 방법으로 오랫동안 사용돼 왔다.  
  
하지만 백오브워즈는 한 가지 문제가 있는데 어떤 단어가 많이 나왔다고 해서 문서의 의미를 파악하는 데 크게 도움이 되지 않는 경우가 있다는 점이다. 
예를 들어 '은/는/이/가', '을/를' 같은 조사는 거의 모든 한국어 문서에 등장한다. 또한 위의 예시에서 'AI'라는 단어는 분야를 가리지 않고 여러 기사에서 
언급하기 떄문에 'AI'라는 단어가 등장했다는 사실만으로는 문서의 의미를 예측하기 어렵다.  
  
# **TF-IDF**  
TF-IDF(Term Frequency-Inverse Document Frequency)는 앞서 설명한 '어느 문서에나 나오는 단어'문제를 보완하기 위해 다음 수식을 활용해 많은 
문서에 등장하는 단어의 중요도를 작게 만든다. 수식에서 TF(w)는 특정 문서에서 특정 단어 w가 등장한 횟수(Term Frequency)를 나타낸다. 백오브워즈의 빈도 
집계와 같은 부분이다. DF(w)는 특정 단어 w가 등장한 문서의 수(Document Frequency)를 의미한다.  
  
TF-IDF(w) = TF(w) * log(N/DF(w))  
  
![img.png](image/img2.png)  
  
위 표에서 전체 문서의 수(N)가 4일 때 'AI'라는 단어는 3개의 문서에 등장했으므로 DF("AI") = 3이다. log(N/DF(w))는 특정 단어가 여러 문서에 
등장할수록 값이 작아지는데 만약 '이'라는 조사가 4개의 문서에 모두 등장했다면 log(N/DF("이")) = log(4/4) = 0이 된다.  
  
위 표를 살펴보면 조사 '이'는 'LLM'에 비해 IT 기사에서 더 많이 등장하지만 모든 문서에 등장했기 때문에 중요도가 없다(log(4/4) = 0)고 판단해서 
최종적으로 TF-IDF("이") 값은 0이 된다. TF-IDF는 백오브워즈의 문제를 성공적으로 보완하면서 오랫동안 활발히 사용됐고 검색에서도 BM25(Best Matching 25) 
와 같은 TF-IDF의 변형 방식이 현재까지도 가장 보편적인 연관도 점수 계산 방식으로 사용되고 있다.  
  
원핫 인코딩, 백오브워즈, TF-IDF는 문서에 등장하는 단어의 수만큼 차원이 커지는데 경제 기사 1, 2와 IT 기사 1, 2에 포함된 모든 단어의 수만큼 표가 
가로로 길어져야 한다. 만약 총 10000개의 단어가 전체 문서에 사용됐다면 하나의 문장과 문서를 표현하기 위해 10000차원의 벡터를 사용하게 되는데 그러면 
필연적으로 대부분의 수가 0인 벡터가 된다. 이렇게 대부분이 0인 벡터를 희소(sparse)하다고 하는데 희소한 벡터는 의미를 '압축'해서 담고 있지 못하기 
떄문에 벡터와 벡터 사이의 관계를 활용하기 어렵다. 앞으로 설명할 워드투벡과 문장 임베딩은 보통 100 ~ 1000차원 정도로 훨씬 압축된 형태인데 회소한 
벡터와 대비해서 밀집 임베딩(dense embedding)이라고 부른다.  
  
# **워드투벡**  
워드투벡(word2vec)은 단어가 '함께 등장하는 빈도' 정보를 활용해 단어의 의미를 압축하는 단어 임베딩 방법이다. 글을 읽다 보면 자주 함께 사용되는 
단어들이 있다. 예를 들어 'AI'는 'ML'또는 '머신러닝'과, '한강'은 '라면'이나 '자전거'와 함께 자주 등장한다. 그렇다면 특정 단어 주변에 어떤 
단어가 있는지 예측하는 모델을 만든다면 단어의 의미를 표현한 임베딩을 모델이 생성할 수 있지 않을까?  
  
![img.png](image/img3.png)  
  
워드투벡은 이런 가정에 기반해 위 그림과 같이 주변 단어로 가운데 단어를 예측하는 방식(CBOW)과 중간 단어로 주변 단어를 예측하는 방식(스킵그램)으로 
모델을 학습시킨다.  
  
그림을 좀 더 자세히 살펴보면 CBOW(Continuous Bag of Words)는 주변의 단어 정보로 중간에 있을 단어를 예측하는 방식이다. 그림 a에서는 t번째 단어를 
예측하기 위해 위아래로 2개의 단어 정보(t - 2, t - 1, t + 1, t + 2)를 활용한다. 스킵그램(skip-gram)은 반대로 가운데 단어 정보로 주변의 단어를 
예측하는데 그림 b에서 t번째 단어 정보로 주변의 4개의 단어(t - 2, t - 1, t + 1, t + 2번째)가 무엇인지 맞추는 방식으로 학습한다.  
  
![img.png](image/img4.png)  
  
이렇게 주변 단어를 예측하는 방식을 사용해 학습한 모델로 단어를 임베딩 벡터로 변환했을 때 위 그림과 같이 '여자'라는 단어의 임베딩 벡터와 '남자'라는 
단어의 임베딩 벡터 사이의 거리와 방향이 '여왕'과 '왕' 사이의 거리 및 방향과 비슷하게 나왔다. 단어의 의미를 압축해 숫자로 표현하니 단어와 단어 
사이의 관계를 계산할 수 있고 그 관계에도 의미가 담겨 있음을 확인한 것이다.  
  
원핫 인코딩, 백오브워즈, TF-IDF에서는 희소 벡터를 통해 텍스트를 표현했고 워드투벡은 인공신경망을 사용해 단어의 의미를 압축해 밀집 임베딩으로 표현하도록 
발전했다. 워드투벡의 성공 이훌 단어를 넘어 문장을 밀집 임베딩으로 표현하는 방법이 개발됐다.  
  
# **문장 임베딩 방식**  
워드투벡은 단어를 임베딩 벡터로 변환함으로써 단어와 단어 사이의 관계를 판단하는 데 밀집 임베딩을 활용할 수 있는 길을 열었다. 하지만 우리가 텍스트를 
활용할 때는 단어 단위보다는 문장이나 문단 같은 더 큰 단위를 사용한다. 따라서 여러 단위가 합쳐진 문장을 임베딩 벡터로 변환하는 방법이 필요했다.  
  
# **문장 사이의 관계를 계산하는 두 가지 방법**  
워드투벡의 성공 이후 인공신경망을 활용해 텍스트를 밀집 임베딩으로 표현하는 기술이 개발됐다. 문장 임베딩을 활용하면 문장 사이의 유사도나 관련성을 
벡터 연산을 통해 쉽게 계산할 수 있다는 장점이 있다. 트랜스포머 인코더 구조를 활용한 BERT(Bidirectional Encoder Representations from Transformers)모델은 
입력 문장을 문장 임베딩으로 변환하는 데 있어 뛰어난 성능을 보였다.  
  
![img.png](image/img5.png)  
  
BERT 모델을 사용해 문장과 문장 사이의 관계를 계산하는 방법은 크게 두 가지로 나눌 수 있다. 첫 번째는 그림 a에 나와 있는 바이 인코더(bi-encoder) 
방식이다. 이 방식에서는 각각의 문장(문장 A와 B)를 독립적으로 BERT 모델에 입력으로 넣고 모델의 출력 결과인 문장 임베딩 벡터(그림의 u와 v) 사이의 
유사도를 코사인 유사도(Cosine similarity)와 같은 별도의 계산을 통해 구한다. 그림에서 풀링 층은 문장의 길이가 달라져도 문장 임베딩의 차원이 같도록 
맞춰주는 층이다. 두 번째 방식은 그림 b에 나와 있는 교차 인코더(cross-encoder) 방식이다. 이 방식에서는 두 문장을 함께 BERT 모델에 입력으로 넣고 
모델이 직접 두 문장 사이의 관계를 0에서 1 사이의 값으로 출력한다. 교차 인코더 방식은 바이 인코더 방식에 비해 계산량이 많지만 두 문장의 상호작용을 
고려할 수 있어 좀 더 정확한 관계 예측이 가능하다.  
  
![img.png](image/img6.png)  
  
바이 인코더와 교차 인코더의 차이를 더 정확히 이해하기 위해 위 그림에서 교차 인코더를 활용해 문서를 검색하는 경우를 예시로 살펴보자. 교차 인코더 방식은 
하나의 BERT 모델에 검색 쿼리 문장과 검색 대상 문장을 함께 입력으로 넣고 텍스트 사이의 유사도 점수(그림의 s)를 계산한다. 이렇게 계산했을 때 그림에서 
확인할 수 있듯이 직접적으로 두 텍스트 사이의 관계를 모두 계산하기 때문에 두 텍스트의 유사도를 정확히 계산할 수 있다는 장점이 있다. 하지만 입력으로 넣은 
두 문장의 유사도만 계산하기 때문에 다른 문장과 검색 쿼리의 유사도를 알고 싶으면 다시 동일한 연산을 반복해야 한다는 단점이 있다.  
  
![img.png](image/img7.png)  
  
예를 들어 위 그림과 같이 1000개의 문장을 저장하고 있을 때 문장 A와 가장 유사한 문장을 찾고 싶다고 하자. 그러면 교차 인코더를 사용하면 문장 A와 
문장 1의 유사도를 계산하고 순차적으로 문장 2, 문장 3, ..., 문장 1000까지 모두 계산해 1000번의 계산을 거쳐야 한다. 하지만 BERT 모델이 사용하는 
어텐션 연산은 계산량이 많은 무거운 연산이기 때문에 가급적 적게 수행해야 한다.  
  
![img.png](image/img8.png)  
  
하지만 교차 인코더를 사용하는 경우 새로운 문장과 유사한 문장을 찾고 싶을 때마다 동일한 연산을 수행해야 한다. 위 그림에서 새로운 문장 B와 가장 
유사한 문장을 검색하고 싶다면 다시 문장 1부터 문장 1000까지 1000번의 BERT 연산을 수행해야 한다. 즉 새로운 검색 문장마다 매번 저장된 문장 수
(그림에서는 1000)만큼의 BERT 연산을 수행해야 가장 유사한 문장을 찾을 수 있다.  
  
![img.png](image/img9.png)  
  
교차 인코더는 모든 문장 조합에 대해 유사도를 계산해야 가장 유사한 문장을 검색할 수 있어 확장성이 떨어진다. 이런 문제를 극복하기 위해 위 그림과 
같이 검색 쿼리 문장과 검색 대상 문장을 각각의 모델에 입력하는 바이 인코더 방식이 개발됐다. 그림에서 각 문장은 동일한 모델을 통과해 각 문장에 대한 
임베딩으로 변환된다. 그리고 두 문장 사이의 유사도는 각 문장 임베딩을 코사인 유사도와 같은 유사도 계산 방식을 통해 최종적인 유사도 점수(그림의 s)를 
산출한다.  
  
두 문장 사이의 유사도 점수를 결과로 반환하는 교차 인코더와 달리 바이 인코더는 각 문장의 독립적인 임베딩을 결과로 반환하기 때문에 유사도를 계산하고 싶은 
문장이 바뀌더라도 추가적인 BERT 연산이 필요 없다.  
  
![img.png](image/img10.png)  
  
이해를 돕기 위해 위 그림을 살펴보자. 저장된 1000개의 문장 중 문장 A와 가장 유사한 문장을 찾고 있다. 그럴 때 바이 인코더를 통해 문장 A에 대한 
문장 임베딩을 생성하고 문장 1부터 문장 1000까지 1000개의 문장에 대한 문장 임베딩을 생성한다. 그리고 문장 임베딩끼리 코사인 유사도를 사용해 유사도 
점수를 계산한다. 여기서 코사인 유사도는 BERT 연산에 비해 훨씬 계산량이 적기 때문에 빠르게 수행할 수 있다.  
  
![img.png](image/img11.png)  
  
바이 인코더의 강점은 새로운 문장 B와 가까운 문장을 검색할 때 드러난다. 위 그림에서 새로운 문장 B와 가장 가까운 문장을찾는 경우, 문장 1에서 문장 1000의 
문장 임베딩은 이미 문장 A를 검색할 때 만들었기 때문에 문장 B를 문장 임베딩으로 만드는 한 번의 BERT 연산만 수행하면 문장 B와 유사한 문장을 검색할 
수 있다. 앞서 교차 인코더의 경우 검색하는 문장이 바뀔 때마다 1000번의 연산을 수행해야 했는데 바이 인코더를 사용하면 검색하는 문장에 대해서만 
BERT 연산을 수행하기 때문에 계산량이 줄고 확장성이 높아진다.  
  
문장 임베딩을 계산하는 바이 인코더를 활용하면 각 문장에 대해 한 번씩만 BERT 연산을 수행하면 유사도는 코사인 유사도 같은 가벼운 연산으로 구할 
수 있기 때문에 문장이 ㅁ낳아져도 계산에 오랜 시간이 걸리지 않는다. 만약 1000개의 문서에 대한 관계를 모두 계산하고 싶다고 할 때 교차 인코더에서는 
1000개의 조합 수(499500)만큼 BERT 연산이 필요하지만 바이 인코더에서는 1000번의 BERT 연산만 수행하면 나머지는 코사인 유사도 계산으로 구할 수 있다.  
  
바이 인코더를 사용하면 문장을 문장 임베딩으로 변환하고 빠르게 검색을 수행할 수 있다. Sentence-Transformers 라이브러리는 허깅페이스 트랜스포머 
라이브러리를 기반으로 쉽게 바이 인코더를 사용할 수 있는 다양한 기능을 지원한다.  
  
