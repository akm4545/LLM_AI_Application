# **임베딩 모델로 데이터 의미 압축하기**  
검색 증강 생성(RAG)는 프롬프트의 요청의 맥락 정보를 추가해 LLM의 생성 품질을 향상하는 기술이다. 맥락 정보를 저장하고 검색하기 위해서는 텍스트를 
임베딩 벡터로 변환해야 하는데 이때 임베딩 모델을 사용한다. 지금까지 임베딩 모델의 활용에만 집중할 수 있도록 상업용 임베딩 모델인 OpenAI의 text-embedding-ada-002를 
사용했다.  
  
텍스트를 숫자로 표현하지만 의미를 담지는 못하는 원핫 인코딩(one-hot encoding)부터 문장을 벡터로 표현하는 문장 임베딩(sentence embedding)까지 
AI 분야에서는 텍스트를 의미를 담아 더 압축적인 임베딩 벡터를 만드는 방향으로 발전해 왔다.  
  
텍스트 임베딩 모델은 문장의 의미를 담아 임베딩 벡터로 변환하기 때문에 문자열이 동일하지 않더라도 검색할 수 있다. 임베딩 벡터의 유사도를 기반으로 
검색하는 방법을 의미 검색(semantic search)이라고 부른다.  
  
의미 검색은 검색할 때 벡터 유사도를 활용하기 때문에 의미상 유사한 문서를 찾을 수 있다는 장점이 있지만 문자열을 비교하는 키워드 검색 방식보다는 
관련도가 떨어지는 문서가 검색될 수 있다는 단점이 있다. 이런 단점을 보완하기 위해 키워드 검색과 의미 검색을 조합해 사용하는 하이브리드 검색을 사용할 
수 있다. 대표적인 키워드 검색 방식인 BM25, 그리고 키워드 검색과 의미 검색의 순위를 조합해 최종 순위를 산정할 때 사용하는 RRF(Reciprocal Rank Fusion)를 
알아본다.  
  
하단 명령어를 실행해 라이브러리를 다운로드 한다.  
  
!pip install transformers==4.40.1 datasets==2.19.0 sentence-transformers==2.7.0  
faiss-cpu==1.8.0 llama-index==0.10.34 llama-index-embeddings-huggingface==0.2.0 -qqq  
  
# **텍스트 임베딩 이해하기**  
9장에서는 "북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?"이라는 질문에 관련된 기사 본문을 찾아 프롬프트에 추가했다. 여기서 질문과 
기사가 문자열 그대로 일치하지 않는데 검색할 수 있었다는 점이 중요하다. 여러 문장의 텍스트를 임베딩 벡터로 변환하는 방식을 텍스트 임베딩 또는 문장 임베딩
이라고 부른다. 문장 임베딩 방식은 이제 널리 사용되지만 최근까지도 텍스트의 의미를 담아 임베딩 벡터로 표현하는 과제는 어려운 문제였다.  
  
임베딩(embedding)이란 '데이터의 의미를 압축한 숫자 배열(벡터)'을 말한다. 기본적으로 컴퓨터는 숫자 형식의 데이터만 연산할 수 있기 때문에 컴퓨터로 
처리하기 위해서는 텍스트, 이미지, 음성 등 모든 데이터를 숫자 형식으로 바꿔야 한다. 이때 가능하면 데이터의 의미를 담아 숫자로 변환할 수 있다면 
좋을 텐데 비교적 최근까지도 어려운 과제였다.  
  
# **문장 임베딩 방식의 장점**  
만약 데이터의 의미를 숫자로 표현할 수 있다면 데이터가 서로 유사한지, 관련이 있는지와 같이 중요한 정보를 활용할 수 있다. 예를 들어 '학교', '공부', 
'운동'이라는 세 단어가 있을 때 아래 예제를 사용해 임베딩으로 변환하면 단어 간의 코사인 유사도를 통해 '학교'가 '공부'와 0.595만큼 유사하고 '학교'와 
'운동'은 0.325만큼 유사하다는 정보를 활용할 수 있다.  
  
chapter10.ipynb 파일에서 문장 임베딩을 활용한 단어 간 유사도 계산 참조  
  
문장 임베딩 방식을 사용하면 서로 다른 텍스트를 마치 사람이 이해하는 것처럼 서로 유사한지, 서로 관련이 있는지 판단할 수 있다는 장점이 있다.  
  
# **원핫 인코딩**  
앞서 활용한 예시를 그대로 활용해 '학교', '공부', '운동'이라는 세 가지 단어가 있다고 하자. 세 데이터를 숫자로 변환한다고 하면 다음과 같이 간단히 
1, 2, 3을 아이디로 붙일 수 있다.  
  
학교 - 1, 공부 - 2, 운동 - 3  
  
숫자로 변환은 했지만 이 방법을 사용할 경우 공부가 학교의 두 배(2 = 1 * 2)라거나 운동이 학교의 세 배(3 = 1 * 3)라는 오해가 생길 수 있다. 이런 
문제를 피하기 위해 다음과 같이 표현하는 것을 원핫 인코딩(one-hot encoding)이라고 한다.  
  
학교 - [1, 0, 0], 공부 - [0, 1, 0], 운동 - [0, 0, 1]  
  
이 방식을 사용하면 '식사'라는 새로운 데이터를 추가해도 아래처럼 독립적으로 추가할 수 있고 단어와 단어 사이에 아무런 관계도 나타내지 않는다.  
  
학교 - [1, 0, 0, 0], 공부 - [0, 1, 0, 0], 운동 - [0, 0, 1, 0], 식사 - [0, 0, 0, 1]  
  
원핫 인코딩은 범주형 데이터 사이에 의도하지 않은 관계가 담기는 걸 방지한다는 장점이 있지만 충분히 관련이 있는 단어 사이의 관계도 표현할 수 없다는 
치명적인 단점이 있다. '학교'와 '공부'사이에는 '무언가를 배운다'는 공통점이 있지만 원핫 인코딩에서는 그 의미를 전혀 살릴 수 없다. 아래 예제의 코드를 
실행하면 원핫 인코딩으로 표현했을 때 학교와 공부의 유사도는 0이고 학교와 운동의 유사도도 0이다.  
  
chapter10.ipynb 파일에서 원핫 인코딩의 한계 참조  
  
