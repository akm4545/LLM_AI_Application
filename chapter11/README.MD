# **자신의 데이터에 맞춘 임베딩 모델 만들기: RAG 개선하기**  
교차 인코더는 비교하려는 두 문장을 입력으로 받아 직접 비교학 떄문에 유사도를 더 정확히 계산할 수 있지만 느리고 확장성이 떨어졌다. 바이 인코더는 
입력 문장에 대해 독립적인 문장 임베딩을 생성하고 임베딩끼리 코사인 유사도 같은 가벼운 거리 계산 방식으로 유사도를 계산하기 때문에 큰 데이터셋에서도 
빠르게 가장 비슷한 문장을 찾을 수 있었다. 이런 장점을 통해 바이 인코더는 문장 임베딩을 생성하는 대표적인 방식으로 사용되며 검색 증강 생성(RAG)
의 필수 구성요소로 활용된다.  
  
이번 장에서는 먼저 느리지만 정확한 교차 인코더와 빠르지만 덜 정확한 바이 인코더를 결합해 RAG의 검색 성능을 높이는 방법을 알아본다.  
  
교차 인코더는 느리기 때문에 대규모 데이터의 유사도를 계산하는 데 사용하지 않고 바이 인코더를 통해 선별된 소수의 데이터를 대상으로 더 정확한 
유사도 계산을 위해 사용한다. 교차 인코더를 활용해 필터링된 문장 사이의 유사도를 계산하고 순위를 변경하는 것을 순위 재정렬(re-rank)이라고 한다. 
  
실습에 필요한 라이브러리 설치  

!pip install sentence-transformers==2.7.0 datasets==2.19.0 huggingface_hub==0.23.0 faiss-cpu==1.8.0 -qqq  
  
# **검색 성능을 높이기 위한 두 가지 방법**  
문장의 유사도를 계산할 때 바이 인코더와 교차 인코더를 사용할 수 있다. 교차 인코더는 비교하려는 두 문장을 직접 입력으로 받아 비교하기 때문에 
유사도를 더 정확하게 계산할 수 있지만 유사도를 계산하려는 조합의 수만큼 모두 BERT와 같은 트랜스포머 인코더 연산을 수행하기 때문에 확장성이 
떨어진다. 바이 인코더를 사용하면 독립적인 문장 임베딩 사이의 유사도를 가벼운 벡터 연산을 통해 계산하기 때문에 빠른 검색이 가능하다. 하지만 
바이 인코더는 교차 인코더만큼 정확하게 유사도를 계산하기 어렵다.  
  
![img.png](image/img.png)  
  
위 그림과 같이 바이 인코더와 교차 인코더를 결합해 사용할 수 있다. 먼저 바이 인코더를 사용해 대규모의 문서에서 검색 쿼리와 유사한 소수의 문서(
예: 상위 100개)를 선별한다. 의미 검색을 통해 선별한 소수의 문서는 유사도를 더 정확히 계산할 수 있는 교차 인코더를 사용해 유사한 순서대로 재정렬한다. 
교차 인코더는 계산량이 많지만 소수의 선별된 문서를 대상으로 계산하기 때문에 정확하면서도 빠르게 계산할 수 있다.  
  
위와 같이 검색 과정을 변경했을 때 검색 성능을 높이기 위한 방법은 두 가지로 나눌 수 있다. 먼저 바이 인코더를 추가 학습해 검색 성능을 높일 수 있다.  
10장에서는 허깅페이스 모델 허브에서 사전 학습된 문장 임베딩을 불러와 그대로 사용했다. 따라서 문장 임베딩 모델은 우리가 사용하려는 데이터셋에 
최적화된 상태가 아니었다. 문장 임베딩 모델도 다른 딥러닝 모델과 마찬가지고 학습 데이터와 유사한 입력 데이터에 대해 더 잘 작동한다. 따라서 
문장 임베딩 모델을 사용하려는 데이터셋으로 추가 학습해 검색 성능을 높일 수 있다.  
  
다음으로 교차 인코더를 추가해 검색 성능을 높일 수 있다. 10장에서는 바이 인코더만을 사용해 검색을 구현했기 때문에 관련성이 떨어지는 결과도 포함됐었다. 
하지만 검색 쿼리 문장과 검색 대상 문장을 함께 입력으로 받는 교차 인코더를 추가하면 바이 인코더를 사용해 좁힌 소수의 후보군에서 더 관련성이 높은 
문서를 상위에 올릴 수 있다. 검색 증강 생성에서는 검색된 모든 문서를 프롬프트에 추가하지 않고 상위 몇 개의 입력만 프롬프트에 추가한다. 따라서 
검색 쿼리와 관련이 높은 문서가 상위 검색 결과에 포함되어야 검색 증강 생성이 효과적으로 작동한다.  
  
# **언어 모델을 임베딩 모델로 만들기**  
![img.png](image/img2.png)  
Sentence_transformers 라이브러리를 사용하면 문장 임베딩 모델을 쉽게 활용할 수 있다. 문장 임베딩 모델은 위 그림과 같이 크게 2개의 층으로 
나뉜다. 첫 번째 층은 대량의 텍스트 데이터로 사전 학습한 BERT나 RoBERTa 같은 언어 모델이다. 두 번째 층은 풀링 층으로 입력 문장의 길이에 따라 
달라질 수 있는 출력 차원을 고정된 차원(예: 768차원)으로 맞추는 역할을 한다. 풀링 층은 언어 모델 출력의 첫 번째 토큰을 사용하는 클래스 모드, 
언어 모델의 출력을 평균 내 사용하는 평균 모드, 언어 모델의 출력 중 가장 큰 값들을 모아 사용하는 최대 모드가 있는데 일반적으로 평균 모드를 많이 
사용한다.  
  
문장 임베딩 모델을 만들기 위해서는 위 그림과 같이 사전 학습된 언어 모델을 불러오고 그 위에 풀링 층을 추가하고 문장의 의미를 잘 담을 수 있도록 
학습해야 한다.  
  
# **대조 학습**  
문장 임베딩 모델을 학습시킬 때는 일반적으로 대조 학습(contrastive learning)을 사용한다. 대조 학습이란 관련이 있거나 유사한 데이터는 더 
가까워지도록 만들고 관련이 없거나 유사하지 않은 데이터는 더 멀어지도록 하는 학습 방식을 말한다. 대조 학습을 통해 임베딩 모델을 학습시킬 때 
다양한 데이터를 사용할 수 있다.  
  
![img.png](image/img3.png)  
  
예를 들어 위 그림에서 임베딩 모델에 2개의 문장(문장 A와 B)을 각각 입력하고 서로 유사한 데이터인 경우는 가깝게, 서로 유사하지 않은 경우는 멀게 
만들 수 있다. 또는 문장 A와 문장 B가 서로 이어지는 문장이라면 서로 가깝게, 아니라면 서로 멀게 만들 수도 있다. 마지막으로 두 문장이 서로 질문과 
답변 관계인 경우 가깝도록, 아닌 경우 멀도록 학습시킬 수도 있다.  
  
일반적으로는 서로 유사한 데이터를 수집한 데이터셋이나 서로 이어지는 문장을 수집해 임베딩 모델의 학습에 사용한다.  
  
# **실습: 학습 준비하기**  
언어 모델을 임베딩 모델로 만들기 전에 허깅페이스 모델 허브의 언어 모델을 그대로 불러와 사용해도 문장의 의미를 반영한 문장 임베딩을 잘 만들 수 
있는지 확인해보자. 아래 예제의 코드에서는 Sentence-Transformers 라이브러리인 models 모듈을 활용해 klue/roberta-base 모델을 불러오고 
평균 풀링 층을 만들었다. 마지막으로 SentenceTransformer 클래스로 두 모듈을 결합해 문장 임베딩 모델을 만들었다. 언어 모델을 불러와 추가로 
학습시키지 않았기 때문에 지금의 문장 임베딩 모델은 언어 모델의 출력을 단순히 평균 내 고정된 차원의 벡터로 만들 뿐이다.  
  
chapter11.ipynb 파일에서 사전 학습된 언어 모델을 불러와 문장 임베딩 모델 만들기 참조  
  
다음으로 문장 임베딩 모델이 의미를 담아 임베딩을 잘 생성하는지 확인하는 데 사용할 데이터셋을 불러온다. 이번 실습에서는 KLUE의 STS(Sentence Textual
 Similarity)데이터셋을 사용한다.(https://huggingface.co/datasets/klue/viewer/sts) KLUE는 한국어 벤치마크 데이터셋이고 그중에서 STS 데이터셋은 
2개의 문장이 서로 얼마나 유사한지 점수를 매긴 데이터셋이다. 아래 예제의 코드로 데이터셋을 내려받고 확인해 보자. datasets 라이브러리의 load_dataset 
함수를 사용해 학습 데이터셋(klue_sts_train)과 평가 데이터셋(klue_sts_test)을 내려받는다. 데이터의 형태를 확인하기 위해 첫 번째 학습 데이터를 
확인하면 sentence1과 sentence2 컬럼에 문장이 있고 labels 컬럼에 두 문장이 얼마나 유사한지를 나타내는 다양한 형식의 레이블이 있다. 이번 실습에서는 
소수점 한 자리까지 나타낸 label 점수를 사용한다.  
  
chapter11.ipynb 파일에서 실습 데이터셋 다운로드 및 확인 참조  
  
이번 실습에서는 다음 세 가지 데이터 전처리를 수행한다.  
  
- 학습 데이터의 일부를 검증을 위한 데이터셋으로 분리한다.  
- 유사도 점수를 0 ~ 1 사이로 정규화한다.  
- torch.utils.data.DataLoader를 사용해 배치 데이터로 만든다.  
  
먼저 아래 코드로 학습 데이터 중 10%를 학습이 잘 진행되는지 확인할 때 사용할 검증 데이터로 분리한다. 학습 데이터와 검증 데이터를 동일하게 분리할 
수 있도록 seed=42로 설정했다.  
  
chapter11.ipynb 파일에서 학습 데이터에서 검증 데이터셋 분리하기 참조  
  
다음으로 아래 예제에서 유사도 점수를 0 ~ 1 사이로 정규화하고 Sentence-Transformers에서 데이터를 관리하는 형식인 InputExample 클래스를 사용해 
데이터를 준비한다. prepare_sts_examples 함수는 데이터셋을 입력으로 받아 데이터셋을 순회하면서 InputExample 클래스에 텍스트 쌍을 리스트 
형태로 입력하고 원본 데이터셋에는 0 ~ 5점 척도로 되어 있는 label 점수를 5로 나눠 0 ~ 1 범위로 정규화한다. 앞서 준비한 3개의 데이터셋(학습, 
검증, 평가)을 prepare_sts_examples 함수를 사용해 전처리한다.  
  
chapter11.ipynb 파일에서 label 정규화하기 참조  
  
