# **자신의 데이터에 맞춘 임베딩 모델 만들기: RAG 개선하기**  
교차 인코더는 비교하려는 두 문장을 입력으로 받아 직접 비교학 떄문에 유사도를 더 정확히 계산할 수 있지만 느리고 확장성이 떨어졌다. 바이 인코더는 
입력 문장에 대해 독립적인 문장 임베딩을 생성하고 임베딩끼리 코사인 유사도 같은 가벼운 거리 계산 방식으로 유사도를 계산하기 때문에 큰 데이터셋에서도 
빠르게 가장 비슷한 문장을 찾을 수 있었다. 이런 장점을 통해 바이 인코더는 문장 임베딩을 생성하는 대표적인 방식으로 사용되며 검색 증강 생성(RAG)
의 필수 구성요소로 활용된다.  
  
이번 장에서는 먼저 느리지만 정확한 교차 인코더와 빠르지만 덜 정확한 바이 인코더를 결합해 RAG의 검색 성능을 높이는 방법을 알아본다.  
  
교차 인코더는 느리기 때문에 대규모 데이터의 유사도를 계산하는 데 사용하지 않고 바이 인코더를 통해 선별된 소수의 데이터를 대상으로 더 정확한 
유사도 계산을 위해 사용한다. 교차 인코더를 활용해 필터링된 문장 사이의 유사도를 계산하고 순위를 변경하는 것을 순위 재정렬(re-rank)이라고 한다. 
  
실습에 필요한 라이브러리 설치  

!pip install sentence-transformers==2.7.0 datasets==2.19.0 huggingface_hub==0.23.0 faiss-cpu==1.8.0 -qqq  
  
# **검색 성능을 높이기 위한 두 가지 방법**  
문장의 유사도를 계산할 때 바이 인코더와 교차 인코더를 사용할 수 있다. 교차 인코더는 비교하려는 두 문장을 직접 입력으로 받아 비교하기 때문에 
유사도를 더 정확하게 계산할 수 있지만 유사도를 계산하려는 조합의 수만큼 모두 BERT와 같은 트랜스포머 인코더 연산을 수행하기 때문에 확장성이 
떨어진다. 바이 인코더를 사용하면 독립적인 문장 임베딩 사이의 유사도를 가벼운 벡터 연산을 통해 계산하기 때문에 빠른 검색이 가능하다. 하지만 
바이 인코더는 교차 인코더만큼 정확하게 유사도를 계산하기 어렵다.  
  
![img.png](image/img.png)  
  
위 그림과 같이 바이 인코더와 교차 인코더를 결합해 사용할 수 있다. 먼저 바이 인코더를 사용해 대규모의 문서에서 검색 쿼리와 유사한 소수의 문서(
예: 상위 100개)를 선별한다. 의미 검색을 통해 선별한 소수의 문서는 유사도를 더 정확히 계산할 수 있는 교차 인코더를 사용해 유사한 순서대로 재정렬한다. 
교차 인코더는 계산량이 많지만 소수의 선별된 문서를 대상으로 계산하기 때문에 정확하면서도 빠르게 계산할 수 있다.  
  
위와 같이 검색 과정을 변경했을 때 검색 성능을 높이기 위한 방법은 두 가지로 나눌 수 있다. 먼저 바이 인코더를 추가 학습해 검색 성능을 높일 수 있다.  
10장에서는 허깅페이스 모델 허브에서 사전 학습된 문장 임베딩을 불러와 그대로 사용했다. 따라서 문장 임베딩 모델은 우리가 사용하려는 데이터셋에 
최적화된 상태가 아니었다. 문장 임베딩 모델도 다른 딥러닝 모델과 마찬가지고 학습 데이터와 유사한 입력 데이터에 대해 더 잘 작동한다. 따라서 
문장 임베딩 모델을 사용하려는 데이터셋으로 추가 학습해 검색 성능을 높일 수 있다.  
  
다음으로 교차 인코더를 추가해 검색 성능을 높일 수 있다. 10장에서는 바이 인코더만을 사용해 검색을 구현했기 때문에 관련성이 떨어지는 결과도 포함됐었다. 
하지만 검색 쿼리 문장과 검색 대상 문장을 함께 입력으로 받는 교차 인코더를 추가하면 바이 인코더를 사용해 좁힌 소수의 후보군에서 더 관련성이 높은 
문서를 상위에 올릴 수 있다. 검색 증강 생성에서는 검색된 모든 문서를 프롬프트에 추가하지 않고 상위 몇 개의 입력만 프롬프트에 추가한다. 따라서 
검색 쿼리와 관련이 높은 문서가 상위 검색 결과에 포함되어야 검색 증강 생성이 효과적으로 작동한다.  
  
# **언어 모델을 임베딩 모델로 만들기**  
![img.png](image/img2.png)  
Sentence_transformers 라이브러리를 사용하면 문장 임베딩 모델을 쉽게 활용할 수 있다. 문장 임베딩 모델은 위 그림과 같이 크게 2개의 층으로 
나뉜다. 첫 번째 층은 대량의 텍스트 데이터로 사전 학습한 BERT나 RoBERTa 같은 언어 모델이다. 두 번째 층은 풀링 층으로 입력 문장의 길이에 따라 
달라질 수 있는 출력 차원을 고정된 차원(예: 768차원)으로 맞추는 역할을 한다. 풀링 층은 언어 모델 출력의 첫 번째 토큰을 사용하는 클래스 모드, 
언어 모델의 출력을 평균 내 사용하는 평균 모드, 언어 모델의 출력 중 가장 큰 값들을 모아 사용하는 최대 모드가 있는데 일반적으로 평균 모드를 많이 
사용한다.  
  
문장 임베딩 모델을 만들기 위해서는 위 그림과 같이 사전 학습된 언어 모델을 불러오고 그 위에 풀링 층을 추가하고 문장의 의미를 잘 담을 수 있도록 
학습해야 한다.  
  
