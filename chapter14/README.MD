# **멀티 모달 LLM**  
LLM은 이름에서 알 수 있듯이 언어(텍스트)를 다루는 모델이다. 세상에는 텍스트 외에도 이미지, 음성, 영상 등 다양한 데이터 유형이 있다. AI 모델이 
더 많은 종류의 일을 처리하기 위해서는 더 많은 데이터 유형을 활요할 수 있어야 한다. 이 문제를 풀기 위해 멀티 모달(multi modal) 연구가 활발히 
진행되고 있다. OpenAI가 발표한 GPT-4V(GPT-4 Vision)나 GPT-4o(GPT-4 Omni)모델은 텍스트에 대해 이미지나 음성을 추가로 처리할 수 있는 기능을 
제공한다. OpenAI의 GPT-4o와 구글의 제미나이 모델을 필두로 멀티 모달 기능이 발표되면서 멀티 모달 모델에 대한 관심이 더 뜨거워지고 있다.  
  
![img.png](image/img.png)  
  
위 그림에서는 GPT-4V 모델에 바다 사진을 주고 "사막에서 무슨 일이 벌어지고 있어?"라고 질문했을 때 모델은 이미지를 인식해 질문에 속지 않고 사막이 
없다는 점을 명확히 지적한 것을 확인할 수 있다.  
  
오픈소스 진영에서도 LLaVA(Large Language and Visual Assistant)나 Fuyu-8B 같은 멀티 모달 모델을 발표하고 있다. 멀티 모달 모델을 LLM과 
유사하게 LMM(Large Multi-modal Model)이라는 용어로 부르기도 한다.  
  
아래 명령을 실행해 코드 실행에 필요한 라이브러리를 설치한다.  
  
!pip install transformers==4.40.1 -qqq  
  
# **멀티 모달 LLM이란**  
멀티 모달 LLM이란 텍스트뿐만 아니라 이미지, 비디오, 오디오, 3D 등 다양한 형식의 데이터를 이해하고 생성할 수 있는 LLM을 말한다. 2024년에는 텍스트와 
이미지를 처리하는 멀티 모달 LLM이 가장 활발히 연구되고 있는 기술이고 비디오, 오디오, 3D 이미지를 다루는 멀티 모달 LLM은 아직 초기 단계에 해당한다. 
또한 아직까지는 멀티 모달 생성보다는 멀티 모달 이해 성능을 높이기 위한 기술 개발에 집중되어 있다. 설명의 편의를 위해 앞으로는 이미지와 텍스트를 처리하는 
멀티 모달 LLM을 가정하고 설명한다.  
  
멀티 모달 LLM은 LLM의 뛰어난 언어 이해 능력과 추론 능력을 중심으로 다양한 형식의 데이터를 이해하고 생성하는 능력을 추가하는 방식으로 구현된다.  
    
# **멀티 모달 LLM의 구성요소**  
![img.png](image/img2.png)  
  
일반적으로 멀티 모달 LLM은 위 그림과 같이 다섯 가지 구성요소로 이뤄진다. 멀티 모달 LLM은 이름에서도 알 수 있듯이 LLM이 가장 핵심적인 역할을 한다. 
LLM은 뛰어난 이해 능력과 추론 능력을 갖고 있기 때문에 이미지 형식의 데이터를 모달리티 인코더(modality encoder)와 입력 프로젝터(input projector)를 
통해 텍스트로 변환해 LLM에 입력한다.  
  
LLM의 출력은 기본적으로 텍스트인데 출력 프로젝터(output projector)를 통해 이미지 형태의 데이터 출력이 필요한지 판단하고 모달리티 생성기(modality 
generator)를 통해 특정 데이터 형식의 출력을 생성한다.  
  
위 그림에서 입력 프로젝터와 출력 프로젝터는 아래에 불꽃 아이콘이 있고 나머지는 모달리티 인코더, LLM 백본, 모달리티 생성기에는 얼음 아이콘이 있는데, 
멀티 모달 LLM의 학습 과정에서 파라미터의 업데이트 여부를 나타낸다. 얼음 아이콘이 있는 3개의 구성요소는 학습에 많은 데이터와 연산량이 필요하기 때문에 
멀티 모달 LLM 학습 과정에서 파라미터를 업데이트하지 않고 사전 학습된 모델을 그대로 사용한다. 불꽃 아이콘이 있는 입력 프로젝터와 출력 프로젝터는 
모달리티 인코더와 LLM 백본, 모달리티 생성기를 연결하는 구성요소로 멀티 모달 LLM 학습 과정에서 파라미터를 업데이트함으로써 멀티 모달 이해와 생성 
성능을 높인다.  
  
모달리티 인코더란 이미지, 비디오, 오디오 같이 텍스트 이외의 데이터 형식을 처리하기 위해 학습된 사전 학습 모델을 말한다. 인코더를 통과한 입력 데이터는 
특징 벡터로 변환되고 특징 벡터는 이후에 다룰 입력 프로젝터를 통해 텍스트로 변환된다. 즉 모달리티 인코더는 다양한 형식의 입력 데이터를 텍스트로 변환하기 
위한 준비 단계라고 할 수 있다.  
  
![img.png](image/img3.png)  
  
이미지를 처리하는 모달리티 인코더로 한정하면 위 그림과 같은 비전 트랜스포머(vision transformer)가 가장 많이 활용된다. 비전 트랜스포머는 텍스트를 
처리하기 위해 개발된 트랜스포머 아키텍처를 이미지에 적용한 모델로 위 그림과 같이 이미지를 패치(patch) 단위로 자른 후 마치 텍스트에서 단어를 처리하는 
것과 같이 일렬로 나열해 입력해 처리한다. 텍스트 토큰의 경우 토크나이저를 통해 숫자 아이디와 토큰 임베딩으로 변환할 수 있었는데 이미지 패치의 경우 
사전(vocabulary)을 구축하기 어렵기 때문에 선형 변환을 통해 이미지 임베딩 벡터로 변환함으로써 토큰 임베딩과 유사하게 만든다.  
  
이미지를 처리하는 모달리티 인코더로는 OpenAI가 개발한 CLIP(Contrastive Language-Image Pre-training)모델이 많이 사용된다. CLIP 모델은 인터넷상에서 
수집한 이미지와 캡션 데이터를 활용해 이미지와 텍스트를 같은 벡터 공간에 임베딩하도록 만들어진 모델로 내부적으로 이미지 처리 모델과 텍스트 처리 
모델이 함께 사용된다.  
  
모달리티 인코더가 이미지 데이터를 처리해서 이미지 임베딩으로 변환했다면 입력 프로젝터는 이미지 임베딩을 LLM 백본이 이해할 수 있는 텍스트로 변환한다. 
모달리티 인코더와 입력 프로젝터를 통해 모든 입력 데이터가 텍스트로 변환됐기 때문에 이제는 LLM이 전체를 입력으로 받아 추론 능력과 컨텍스트 학습 
능력 등 LLM의 처리 능력을 활용해 텍스트를 생성하고 반환한다.  
  
아직까지는 멀티 모달 생성을 수행하는 모델이 많지 않지만 구글의 제미나이 같은 멀티 모달 LLM은 텍스트는 물론 이미지를 생성할 수도 있다. LLM의 생성 
결과를 바탕으로 이미지를 생성하기 위해서는 크게 두 가지 단계가 필요하다. 먼저 이미지 생성이 필요한지 판단하는 단계와 만약 필요하다면 어떤 이미지를 
생성할지 정하는 단계다. 이 두 단계는 출력 프로젝터를 통해 수행되며 이미지 생성이 필요한 경우 이미지 생성 모델에 적절한 프롬프트를 생성하는 것이 
프로젝터의 역할이다. 출력 프로젝터를 통해 생성된 이미지 생성 프롬프트는 모달리티 생성기에 전달되고 최종적으로 이미지를 생성한다.  
  

  
