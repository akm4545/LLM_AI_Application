# **멀티 모달 LLM**  
LLM은 이름에서 알 수 있듯이 언어(텍스트)를 다루는 모델이다. 세상에는 텍스트 외에도 이미지, 음성, 영상 등 다양한 데이터 유형이 있다. AI 모델이 
더 많은 종류의 일을 처리하기 위해서는 더 많은 데이터 유형을 활요할 수 있어야 한다. 이 문제를 풀기 위해 멀티 모달(multi modal) 연구가 활발히 
진행되고 있다. OpenAI가 발표한 GPT-4V(GPT-4 Vision)나 GPT-4o(GPT-4 Omni)모델은 텍스트에 대해 이미지나 음성을 추가로 처리할 수 있는 기능을 
제공한다. OpenAI의 GPT-4o와 구글의 제미나이 모델을 필두로 멀티 모달 기능이 발표되면서 멀티 모달 모델에 대한 관심이 더 뜨거워지고 있다.  
  
![img.png](image/img.png)  
  
위 그림에서는 GPT-4V 모델에 바다 사진을 주고 "사막에서 무슨 일이 벌어지고 있어?"라고 질문했을 때 모델은 이미지를 인식해 질문에 속지 않고 사막이 
없다는 점을 명확히 지적한 것을 확인할 수 있다.  
  
오픈소스 진영에서도 LLaVA(Large Language and Visual Assistant)나 Fuyu-8B 같은 멀티 모달 모델을 발표하고 있다. 멀티 모달 모델을 LLM과 
유사하게 LMM(Large Multi-modal Model)이라는 용어로 부르기도 한다.  
  
아래 명령을 실행해 코드 실행에 필요한 라이브러리를 설치한다.  
  
!pip install transformers==4.40.1 -qqq  
  
