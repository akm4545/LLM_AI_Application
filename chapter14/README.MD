# **멀티 모달 LLM**  
LLM은 이름에서 알 수 있듯이 언어(텍스트)를 다루는 모델이다. 세상에는 텍스트 외에도 이미지, 음성, 영상 등 다양한 데이터 유형이 있다. AI 모델이 
더 많은 종류의 일을 처리하기 위해서는 더 많은 데이터 유형을 활요할 수 있어야 한다. 이 문제를 풀기 위해 멀티 모달(multi modal) 연구가 활발히 
진행되고 있다. OpenAI가 발표한 GPT-4V(GPT-4 Vision)나 GPT-4o(GPT-4 Omni)모델은 텍스트에 대해 이미지나 음성을 추가로 처리할 수 있는 기능을 
제공한다. OpenAI의 GPT-4o와 구글의 제미나이 모델을 필두로 멀티 모달 기능이 발표되면서 멀티 모달 모델에 대한 관심이 더 뜨거워지고 있다.  
  
![img.png](image/img.png)  
  
위 그림에서는 GPT-4V 모델에 바다 사진을 주고 "사막에서 무슨 일이 벌어지고 있어?"라고 질문했을 때 모델은 이미지를 인식해 질문에 속지 않고 사막이 
없다는 점을 명확히 지적한 것을 확인할 수 있다.  
  
오픈소스 진영에서도 LLaVA(Large Language and Visual Assistant)나 Fuyu-8B 같은 멀티 모달 모델을 발표하고 있다. 멀티 모달 모델을 LLM과 
유사하게 LMM(Large Multi-modal Model)이라는 용어로 부르기도 한다.  
  
아래 명령을 실행해 코드 실행에 필요한 라이브러리를 설치한다.  
  
!pip install transformers==4.40.1 -qqq  
  
# **멀티 모달 LLM이란**  
멀티 모달 LLM이란 텍스트뿐만 아니라 이미지, 비디오, 오디오, 3D 등 다양한 형식의 데이터를 이해하고 생성할 수 있는 LLM을 말한다. 2024년에는 텍스트와 
이미지를 처리하는 멀티 모달 LLM이 가장 활발히 연구되고 있는 기술이고 비디오, 오디오, 3D 이미지를 다루는 멀티 모달 LLM은 아직 초기 단계에 해당한다. 
또한 아직까지는 멀티 모달 생성보다는 멀티 모달 이해 성능을 높이기 위한 기술 개발에 집중되어 있다. 설명의 편의를 위해 앞으로는 이미지와 텍스트를 처리하는 
멀티 모달 LLM을 가정하고 설명한다.  
  
멀티 모달 LLM은 LLM의 뛰어난 언어 이해 능력과 추론 능력을 중심으로 다양한 형식의 데이터를 이해하고 생성하는 능력을 추가하는 방식으로 구현된다.  
  
