# **LLM 애플리케이션 개발하기**  
4장에서 8장에 걸쳐 LLM이 무엇인지 살펴보고 LLM을 학습시키고 배포하는 방법을 알아봤다.  
  
![img.png](image/img.png)  
  
하지만 LLM을 활용한 애플리케이션을 개발하려면 모델 이외에도 위 그림과 같은 다양한 구성요소가 필요하다. 그림에서 LLM은 전체 시스템의 일부분에 
불과하고 임베딩 모델, 벡터 데이터베이스 등 새로운 요소가 추가적으로 필요하다.  
  
먼저 위 그림에서 A, B, C의 요소는 LLM에 답변에 필요한 정보를 제공하는 기능을 수행한다. 챗GPT가 출시된 이후 사람들은 챗GPT가 거짓말을 하고 
말을 지어낸다는 사실을 발견했다. 이런 현상을 환각(hallucination)이라고 부른다. 그리고 LLM이 답변할 때 필요한 정보를 프롬프트에 함께 전달하는 
검색 증강 생성(Retrieval Augmented Generation, RAG)을 사용하면 환각 현상을 크게 줄일 수 있었다. 검색 증강 생성은 이름 그대로 필요한 정보를 
'검색'하고 프롬프트를 '보강(증강)'해서 '생성(추론)'하는 기술이다. 그림에서 A는 검색하고 싶은 데이터를 데이터 소스에서 가져와 임베딩 모델을 통해 
임베딩 벡터로 만들고 벡터 데이터베이스에 저장하는 과정이다. C는 검색할 데이터를 저장한 벡터 데이터베이스에서 요청과 관련된 데이터를 검색하고 
검색한 결과를 프롬프트에 반영하는 과정이다. B는 검색한 문서를 사용자의 프롬프트에 반영하는 과정이다.  
  
앞서 7장과 8장에서는 LLM 추론을 효율적으로 만드는 다양한기술을 살펴봤다. 다양한 효율화 기술이 개발될 정도로 LLM 추론은 계산량이 많고 비용이 많이 
발생한다. 따라서 가능하면 LLM 추론을 줄여야 한다. LLM 추론을 줄이기 위해 이전에 같거나 비슷한 요청이 있었다면 그 결과를 활용하는 LLM 캐시를 도입할 
수 있다. 그림에서 D는 LLM 추론을 진행하기 전에 이전에 동일하거나 유사한 요청이 있었는지 확인하기 위해 캐시에 요청하는 과정이다. 만약 비슷한 요청이 
없었다면 E에서 LLM 추론을 수행한다.  
  
LLM을 활용한 애플리케이션을 개발할 때 LLM이 답변하지 않아야 할 요청에 답변하지 않고 LLM의 생성 결과에 부적절한 내용이 포함되지 않도록 해야 한다. 
예를 들어 정치적인 질문에 답변하지 않도록 만들 수 있다면 서비스가 여러 오해를 받거나 서비스의 응답으로 불편함을 느끼는 사용자를 줄일 수 있다. 
이를 위해 F는 벡터 데이터베이스에서 검색한 결과를 확인하고 G는 LLM이 생성한 결과에 문제가 없는지 검증한다.  
  
서비스에 들어온 사용자의 요청과 LLM의 응답을 기록해야 한다. 기록해 두지 않으면 사용자의 문의에 대응하기 어렵고 서비스가 잘 작동하고 있는지 확인할 수 
없다. 특히 생성형 AI 서비스의 경우 입력이 동일하더라도 추론한 생성 결과가 다를 수 있기 때문에 꼭 기록해야 한다. 그림에서 H는 사용자의 요청과 LLM 
시스템의 생성 결과를 기록하는 모니터링 과정이다.  
  
다음 명령을 실행해 실습에 사용할 라이브러리를 설치한다.  
  
!pip install datasets llama-index==0.10.34 langchain-openai==0.1.6  
"nemoguardrails[openai]==0.8.0" openai==1.25.1 chromadb==0.5.0 wandb==0.16.6  
llama-index-callbacks-wandb==0.1.2 -qqq  
  
# **검색 증강 생성(RAG)**  
LLM을 학습시킬 때는 많은 시행착오가 필요하다. 그렇기 때문에 최신 정보나 조직의 데이터를 제대로 활용할 수 있도록 모델을 학습시키는 데 상당한 시간과 
비용이 든다. 또한 LLM의 답변은 근거나 출처가 불명확하고 부정확한 정보를 지어내는 환각 현상도 존재한다.  
  
이런 문제를 해결하기 위해 RAG라는 기법이 활용된다. 검색 증강 생성이란 LLM에게 단순히 질문이나 요청만 전달하고 생성하는 것이 아니라 답변에 필요한 
충분한 정보와 맥락을 제공하고 답변하도록 하는 방법을 말한다. 이때 답변에 필요한 정보를 검색(retrieval)을 통해 선택하기 때문에 '검색을 통해 보충한 
생성'이라는 의미로 붙은 이름이다.  
  
![img.png](image/img2.png)  
  
위 그림은 LLM 애플리케이션 아키텍처에서 검색 증강 생성과 관련된 부분이다. 검색 증강 생성은 '검색'을 통해 필요한 정보를 추가한다고 했는데 워크플로 A는 
검색할 데이터를 벡터 데이터베이스에 저장하는 과정이다. 워크플로 B와 C는 사용자 인터페이스를 통해 들어온 사용자의 요청에 관련된 정보를 벡터 데이터베이스에서 
검색한 후 사용자의 요청과 결합해 프롬프트를 완성하는 과정이다. LLM 오케스트레이션 도구는 사용자 인터페이스, 임베딩 모델, 벡터 데이터베이스 등 LLM 
애플리케이션을 위한 다양한 구성요소를 연결하는 프레임워크로 대표적으로 라마인덱스(Llamaindex), 랭체인(Langchain), 캐노피(Canopy) 등이 있다. 
이번 장에서는 라마인덱스를 중심으로 설명과 실습을 진행한다.  
  
# **데이터 저장**  
![img.png](image/img3.png)  
  
워크플로 A는 위 그림과 같이 데이터 소스, 임베딩 모델, 벡터 데이터베이스로 구성된다. 데이터 소스는 텍스트, 이미지와 같은 비정형 데이터가 저장된 데이터 
저장소를 의미한다. 이번 장에서는 텍스트로 한정해서 설명하지만 이미지나 음성도 동일한 원리가 적용된다. 데이터 소스의 텍스트를 임베딩 모델을 사용해 
임베딩 벡터로 변환한다. 변환한 임베딩 벡터는 벡터 사이의 거리를 기준으로 검색하는 특수한 데이터베이스인 벡터 데이터베이스에 저장한다.  
  
![img.png](image/img4.png)  
  
임베딩 모델은 위 그림과 같이 텍스트나 이미지 같은 비정형 데이터를 입력했을 때 그 의미를 담은 임베딩 벡터로 변환하는 모델을 말한다. 텍스트 임베딩 
모델에는 대표적인 상업용 모델로 OpenAI의 text-embedding-ada-002가 있고 오픈소스로는 Sentence-Transformers 라이브러리를 활용해 임베딩 모델을 
구현할 수 있다. 이번 장에서는 간편하게 사용할 수 있는 OpenAI의 text-embedding-ada-002로 텍스트 임베딩 모델을 활용한다.  
  
벡터 데이터베이스는 임베딩 벡터의 저장소이고 입력한 벡터와 유사한 벡터를 찾는 기능을 제공한다. 대표적인 벡터 데이터베이스로는 크로마(Chroma),
밀버스(Milvus) 같은 오픈소스와 파인콘(Pinecone), 위비에이트(Weaviate) 같은 상업 서비스가 있고 최근에는 PostgreSQL 같은 관계형 데이터베이스에서도 
벡터 검색 기능을 도입하고 강화하고 있다. 이번 장에서는 벡터 데이터베이스를 쉽게 활용할 수 있도록 도와주는 라마인덱스 라이브러리의 기본 벡터 
데이터베이스를 활용한다.  
  
![img.png](image/img5.png)  
  
워크플로 A와 C에서 텍스트 데이터를 임베딩 벡터로 만들어 벡터 데이터베이스에 저장하고 검색하는 과정을 그림으로 나타내면 위 그림과 같다. 먼저 문서를 
임베딩 모델을 통해 임베딩 벡터로 변환하고 벡터 데이터베이스에 저장한다. 벡터 데이터베이스는 내부에 각각의 벡터를 저장한다. 특정 문장(검색 쿼리)
으로 검색을 수행하는 경우 임베딩 모델을 통해 검색 쿼리도 벡터로 변환해 벡터 데이터베이스에서 위치를 찾고 쿼리 임베딩과 가장 가까운 벡터를 찾는다. 
이때 일반적으로 유클리드 거리(Euclidean distance)나 코사인 유사도(Cosine similarity)를 활용해 거리를 계산한다.  
  
# **프롬프트에 검색 결과 통합**  
![img.png](image/img6.png)  
  
LLM은 결과를 생성할 떄 프롬프트만 입력으로 받는다. 따라서 우리가 앞서 저장한 텍스트를 LLM에 전달하기 위해서는 위 그림과 같이 사용자의 요청과 
관련이 큰 문서를 벡터 데이터베이스에서 찾고(C) 검색 결과를 프롬프트에 통합(B)해야 한다. 그림에서는 사용자의 요청을 임베딩 모델을 통해 임베딩 벡터로 
변환하고 벡터 데이터베이스에서 검색 임베딩 벡터와 가까운 벡터를 찾아 검색 결과를 반환받는다(C). 임베딩 모델에서 벡터 데이터베이스로 
임베딩 벡터를 보내는 부분을 임베딩 저장 범례 A로 표현했지만 검색 과정에서도 임베딩 벡터를 보내기 때문에 위 그림에서는 검색 범례 C로 표현했다. 
검색 결과는 프롬프트 모듈에서 사용자의 요청과 하나로 통합된다.  
  
![img.png](image/img7.png)    
  
검색 증강 생성의 전체 흐름을 이해하기 위해 먼저 위 그림과 같이 검색 증강 생성을 사용하지 않고 사용자의 질문을 그대로 전달하는 예시를 살펴보자. 
위 그림에서 사용자는 "북태평양 기단과 오호츠크해 기단이 만나 국내에 머무르는 기간은?"이라는 질문을 하고 어시스턴트(GPT-3.5)는 "북태평양 기단과 
오호츠크해 기단이 만나 국내에 머무르는 기간은 겨울철인 11월부터 다음 해 3월까지입니다. 이 기간 동안 기단의 영향으로 한반도에는 추운 날씨와 강한 바람이 
불게 됩니다."라고 응답했다.  
  
기단에 대한 지식이 없다고 가정했을 때 LLM의 답변은 굉장히 믿음직스러워 보인다. 위의 예시 질문은 KLUE MRC 데이터셋에서 가져온 질문인데 KLUE MRC 
데이터셋은 기사와 해당 기사에 대한 질문과 답변을 모은 데이터셋이다. 실제 데이터셋의 정답은 '한 달' 또는 '한 달가량'이다. 위 그림에서 어시스턴트는 
'11월부터 3월'이라고 답변했는데 실제 정답인 '한 달'과 상당한 차이가 있다. 데이터셋은 허깅페이스 데이터셋 허브의 KLUE MRC 데이터셋 링크(https://huggingface.
co/datasets/klue/viewer/mrc?row=0)에서 확인할 수 있다.  
  
![img.png](image/img8.png)  
  
데이터셋에 포함된 기사 본문을 함께 gpt-3.5-turbo에 전달하면 위 그림과 같이 기사 본문에 있는 내용을 반영해 '32일'이라고 잘 답변한다. 처음에 질문만 
던져 받은 답변과 비교하면 시기(겨울 X -> 여름 O)와 기간(4개월 X -> 1개월 O)모두 달라졌다. 이번 예시를 통해 사용 모델인 GPT-3.5조차 관련 정보를 
충분히 제공하지 않으면 언제든 환각 현상을 보일 수 있다는 사실을 확인할 수 있다.  
  
이번 예시에서는 관련된 텍스트를 데이터셋에서 직접 찾아 프롬프트에 입력해 줬다. 하지만 매번 질문에 관련된 정보를 수동으로 찾아 입력해 줄 수는 없기 
때문에 프로그래밍 방식으로 관련된 정보를 찾아 프롬프트에 넣을 수 있어야 한다.  
  
![img.png](image/img9.png)  
  
텍스트를 임베딩 벡터로 변환해 벡터 데이터베이스에 저장하고 검색하는 과정을 살펴봤는데 우리의 예시에 대응시켜 새롭게 나타내면 위 그림과 같다. 예시에서 
질문과 관련된 맥락(context) 정보인 '올여름 장마가...'를 임베딩 모델을 통해 문서 임베딩으로 변환하고 벡터 데이터베이스에 저장하고 사용자의 질문(question)인 
'북태평양 기단과...'를 쿼리 임베딩으로 변환해 검색했을 때 두 임베딩이 가까워 검색 결과로 반환된다면 우리는 자동으로 관련된 정보를 찾아 프롬프트에 
추가하는 검색 증강 생성 기능을 구현할 수 있다.  
  
