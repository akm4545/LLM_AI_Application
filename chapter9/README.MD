# **LLM 애플리케이션 개발하기**  
4장에서 8장에 걸쳐 LLM이 무엇인지 살펴보고 LLM을 학습시키고 배포하는 방법을 알아봤다.  
  
![img.png](image/img.png)  
  
하지만 LLM을 활용한 애플리케이션을 개발하려면 모델 이외에도 위 그림과 같은 다양한 구성요소가 필요하다. 그림에서 LLM은 전체 시스템의 일부분에 
불과하고 임베딩 모델, 벡터 데이터베이스 등 새로운 요소가 추가적으로 필요하다.  
  
먼저 위 그림에서 A, B, C의 요소는 LLM에 답변에 필요한 정보를 제공하는 기능을 수행한다. 챗GPT가 출시된 이후 사람들은 챗GPT가 거짓말을 하고 
말을 지어낸다는 사실을 발견했다. 이런 현상을 환각(hallucination)이라고 부른다. 그리고 LLM이 답변할 때 필요한 정보를 프롬프트에 함께 전달하는 
검색 증강 생성(Retrieval Augmented Generation, RAG)을 사용하면 환각 현상을 크게 줄일 수 있었다. 검색 증강 생성은 이름 그대로 필요한 정보를 
'검색'하고 프롬프트를 '보강(증강)'해서 '생성(추론)'하는 기술이다. 그림에서 A는 검색하고 싶은 데이터를 데이터 소스에서 가져와 임베딩 모델을 통해 
임베딩 벡터로 만들고 벡터 데이터베이스에 저장하는 과정이다. C는 검색할 데이터를 저장한 벡터 데이터베이스에서 요청과 관련된 데이터를 검색하고 
검색한 결과를 프롬프트에 반영하는 과정이다. B는 검색한 문서를 사용자의 프롬프트에 반영하는 과정이다.  
  
앞서 7장과 8장에서는 LLM 추론을 효율적으로 만드는 다양한기술을 살펴봤다. 다양한 효율화 기술이 개발될 정도로 LLM 추론은 계산량이 많고 비용이 많이 
발생한다. 따라서 가능하면 LLM 추론을 줄여야 한다. LLM 추론을 줄이기 위해 이전에 같거나 비슷한 요청이 있었다면 그 결과를 활용하는 LLM 캐시를 도입할 
수 있다. 그림에서 D는 LLM 추론을 진행하기 전에 이전에 동일하거나 유사한 요청이 있었는지 확인하기 위해 캐시에 요청하는 과정이다. 만약 비슷한 요청이 
없었다면 E에서 LLM 추론을 수행한다.  
  
LLM을 활용한 애플리케이션을 개발할 때 LLM이 답변하지 않아야 할 요청에 답변하지 않고 LLM의 생성 결과에 부적절한 내용이 포함되지 않도록 해야 한다. 
예를 들어 정치적인 질문에 답변하지 않도록 만들 수 있다면 서비스가 여러 오해를 받거나 서비스의 응답으로 불편함을 느끼는 사용자를 줄일 수 있다. 
이를 위해 F는 벡터 데이터베이스에서 검색한 결과를 확인하고 G는 LLM이 생성한 결과에 문제가 없는지 검증한다.  
  
서비스에 들어온 사용자의 요청과 LLM의 응답을 기록해야 한다. 기록해 두지 않으면 사용자의 문의에 대응하기 어렵고 서비스가 잘 작동하고 있는지 확인할 수 
없다. 특히 생성형 AI 서비스의 경우 입력이 동일하더라도 추론한 생성 결과가 다를 수 있기 때문에 꼭 기록해야 한다. 그림에서 H는 사용자의 요청과 LLM 
시스템의 생성 결과를 기록하는 모니터링 과정이다.  
  
다음 명령을 실행해 실습에 사용할 라이브러리를 설치한다.  
  
!pip install datasets llama-index==0.10.34 langchain-openai==0.1.6  
"nemoguardrails[openai]==0.8.0" openai==1.25.1 chromadb==0.5.0 wandb==0.16.6  
llama-index-callbacks-wandb==0.1.2 -qqq  
  
# **검색 증강 생성(RAG)**  
LLM을 학습시킬 때는 많은 시행착오가 필요하다. 그렇기 때문에 최신 정보나 조직의 데이터를 제대로 활용할 수 있도록 모델을 학습시키는 데 상당한 시간과 
비용이 든다. 또한 LLM의 답변은 근거나 출처가 불명확하고 부정확한 정보를 지어내는 환각 현상도 존재한다.  
  
이런 문제를 해결하기 위해 RAG라는 기법이 활용된다. 검색 증강 생성이란 LLM에게 단순히 질문이나 요청만 전달하고 생성하는 것이 아니라 답변에 필요한 
충분한 정보와 맥락을 제공하고 답변하도록 하는 방법을 말한다. 이때 답변에 필요한 정보를 검색(retrieval)을 통해 선택하기 때문에 '검색을 통해 보충한 
생성'이라는 의미로 붙은 이름이다.  
  
![img.png](image/img2.png)  
  
위 그림은 LLM 애플리케이션 아키텍처에서 검색 증강 생성과 관련된 부분이다. 검색 증강 생성은 '검색'을 통해 필요한 정보를 추가한다고 했는데 워크플로 A는 
검색할 데이터를 벡터 데이터베이스에 저장하는 과정이다. 워크플로 B와 C는 사용자 인터페이스를 통해 들어온 사용자의 요청에 관련된 정보를 벡터 데이터베이스에서 
검색한 후 사용자의 요청과 결합해 프롬프트를 완성하는 과정이다. LLM 오케스트레이션 도구는 사용자 인터페이스, 임베딩 모델, 벡터 데이터베이스 등 LLM 
애플리케이션을 위한 다양한 구성요소를 연결하는 프레임워크로 대표적으로 라마인덱스(Llamaindex), 랭체인(Langchain), 캐노피(Canopy) 등이 있다. 
이번 장에서는 라마인덱스를 중심으로 설명과 실습을 진행한다.  
  
# **데이터 저장**  
![img.png](image/img3.png)  
  
워크플로 A는 위 그림과 같이 데이터 소스, 임베딩 모델, 벡터 데이터베이스로 구성된다. 데이터 소스는 텍스트, 이미지와 같은 비정형 데이터가 저장된 데이터 
저장소를 의미한다. 이번 장에서는 텍스트로 한정해서 설명하지만 이미지나 음성도 동일한 원리가 적용된다. 데이터 소스의 텍스트를 임베딩 모델을 사용해 
임베딩 벡터로 변환한다. 변환한 임베딩 벡터는 벡터 사이의 거리를 기준으로 검색하는 특수한 데이터베이스인 벡터 데이터베이스에 저장한다.  
  
![img.png](image/img4.png)  
  
임베딩 모델은 위 그림과 같이 텍스트나 이미지 같은 비정형 데이터를 입력했을 때 그 의미를 담은 임베딩 벡터로 변환하는 모델을 말한다. 텍스트 임베딩 
모델에는 대표적인 상업용 모델로 OpenAI의 text-embedding-ada-002가 있고 오픈소스로는 Sentence-Transformers 라이브러리를 활용해 임베딩 모델을 
구현할 수 있다. 이번 장에서는 간편하게 사용할 수 있는 OpenAI의 text-embedding-ada-002로 텍스트 임베딩 모델을 활용한다.  
  
벡터 데이터베이스는 임베딩 벡터의 저장소이고 입력한 벡터와 유사한 벡터를 찾는 기능을 제공한다. 대표적인 벡터 데이터베이스로는 크로마(Chroma),
밀버스(Milvus) 같은 오픈소스와 파인콘(Pinecone), 위비에이트(Weaviate) 같은 상업 서비스가 있고 최근에는 PostgreSQL 같은 관계형 데이터베이스에서도 
벡터 검색 기능을 도입하고 강화하고 있다. 이번 장에서는 벡터 데이터베이스를 쉽게 활용할 수 있도록 도와주는 라마인덱스 라이브러리의 기본 벡터 
데이터베이스를 활용한다.  
  
![img.png](image/img5.png)  
  
워크플로 A와 C에서 텍스트 데이터를 임베딩 벡터로 만들어 벡터 데이터베이스에 저장하고 검색하는 과정을 그림으로 나타내면 위 그림과 같다. 먼저 문서를 
임베딩 모델을 통해 임베딩 벡터로 변환하고 벡터 데이터베이스에 저장한다. 벡터 데이터베이스는 내부에 각각의 벡터를 저장한다. 특정 문장(검색 쿼리)
으로 검색을 수행하는 경우 임베딩 모델을 통해 검색 쿼리도 벡터로 변환해 벡터 데이터베이스에서 위치를 찾고 쿼리 임베딩과 가장 가까운 벡터를 찾는다. 
이때 일반적으로 유클리드 거리(Euclidean distance)나 코사인 유사도(Cosine similarity)를 활용해 거리를 계산한다.  
  
# **프롬프트에 검색 결과 통합**  
![img.png](image/img6.png)  
  
LLM은 결과를 생성할 떄 프롬프트만 입력으로 받는다. 따라서 우리가 앞서 저장한 텍스트를 LLM에 전달하기 위해서는 위 그림과 같이 사용자의 요청과 
관련이 큰 문서를 벡터 데이터베이스에서 찾고(C) 검색 결과를 프롬프트에 통합(B)해야 한다.