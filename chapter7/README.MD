# **모델 가볍게 만들기**  
서비스를 위해 LLM을 배포하는 경우 가장 많은 비용은 GPU 사용에서 발생한다. 그렇기 때문에 GPU를 가능하면 적게 사용해서 비용을 낮춰야 비용 효율적인 
서빙을 할 수 있다. LLM은 모델의 크기가 기존 딥러닝 모델에 비해 훨씬 크기 때문에 효율적인 서빙이 더 중요하고 관련 연구도 활발히 진행되고 있다. 
GPU를 효율적으로 활용하는 방식은 모델의 성능을 약간 희생하더라도 비용을 크게 낮추는 방법과 모델의 성능을 그대로 유지하면서 연산 과정의 비효율을 
줄이는 방법으로 구분할 수 있다.  
  
LLM의 추론에서는 동일한 연산을 반복적으로 수행하면서 한 토큰씩 생성한다. 이때 발생하는 동일한 연산을 최대한 줄이기 위해 계산 결과를 저장하는 KV 캐시
(KV cache)를 사용한다. KV 캐시는 중복 계산을 줄여 추론 속도를 높이는 데 도움을 주지만 계산 결과를 저장해야 하기 때문에 많은 GPU 메모리를 사용한다. 
GPU를 효율적으로 활용하기 위해서는 한 번에 더 많은 데이터를 처리해야 하는데 KV 캐시와 모델 파라미터를 저장하는 데 많은 GPU 메모리를 사용하면 더 많은 
데이터를 처리하지 못한다.  
  
다음 명령을 실행해 라이브러리 설치
!pip install transformers==4.40.1 accelerate==0.30.0 bitsansbytes==0.43.1 auto-gptq==0.7.1 autoawq==0.2.5 optimum==1.19.1 -qqq  
  
# **언어 모델 추론 이해하기**  
# **언어 모델이 언어를 생성하는 방법**  
  
![img.png](image/img.png)  
  
언어 모델은 위 그림과 같이 입력한 텍스트 다음에 올 토큰의 확률을 계산하고 그중에서 가장 확률이 높은 토큰을 입력 텍스트에 추가하면서 한 토큰씩 
생성한다. 그림에서 '검은 고양이가 밥을'이라는 텍스트를 입력했을 때 다음 토큰으로 '그리고'가 올 확률이 0.001, '마시고'가 올 확률은 0.04, '먹고'가 
올 확률이 0.9라고 예측했는데 이럴 때 가장 확률이 높은 '먹고'를 다음 토큰으로 결정하고 입력한 텍스트에 추가해 '검은 고양이가 밥을 먹고'라는 텍스트를 
생성한다. 언어 모델이 텍스트 생성을 마치는 이유는 두 가지로 나눌 수 있다. 먼저 다음 토큰으로 생성 종료를 의미하는 특수 토큰(예: EOS, End Of Sentence 토큰)
을 생성하는 경우 생성을 종료한다. 다음으로 사용자가 최대 길이로 설정한 길이에 도달하면 더 이상 생성하지 않고 종료한다. 두 가지 경우에 해당하기 
전까지는 위 그림의 순환 화살표와 같이 새로운 토큰을 추가한 텍스트를 다시 모델에 입력을 넣는 과정을 반복한다.  
  
![img2.png](image/img2.png)  
  
앞서 살펴본 대로 언어 모델이 텍스트를 생성할 때는 한 번에 한 토큰씩만 생성할 수 있다. 위 그림처럼 다음 토큰과 그다음 토큰을 함께 예측할 수는 없다. 
이처럼 언어 모델은 입력 텍스트를 기반으로 바로 다음 토큰만 예측하는 자기 회귀적(auto-regressive) 특성을 갖는다. 하지만 새롭게 생성하는 부분이 아니라 
언어 모델에 입력하는 '검은 고양이가 밥을'같은 프롬프트는 이미 작성된 텍스트이기 때문에 한 번에 하나씩 토큰을 처리할 필요 없이 동시에 병렬적으로 
처리할 수 있다. 따라서 프롬프트가 길다고 하더라도 다음 토큰 1개를 생성하는 시간과 비슷한 시간이 거린다. 이런 이유로 추론 과정을 프롬프트를 처리하는 
단계인 사전 계산 단계(prefill phase)와 이후에 한 토큰씩 생성하는 디코딩 단계(decoding phase)로 구분한다.  
  
![img.png](image/img3.png)  
  
텍스트를 종료할 때까지 순환하며 반복 생성하는 과정을 풀어서 나타내면 위 그림과 같다. '검은 고양이가 밥을'이라는 프롬프트를 입력하고 '검은 고양이가 밥을 
먹고 물을 마신다!'라는 문장을 생성할 때까지 네 번의 생성 과정을 반복한다. 위 그림을 보면 동일한 텍스트가 반복적으로 모델에 입력되는 것을 볼 수 있다. 
예를 들어 '검은', '고양이가', '밥을'은 네 번의 생성 과정에 모두 동일하게 들어가고, '물은'은 두 번째부터 네 번째까지 세 번의 생성 과정에 들어간다. 
이렇게 동일한 토큰이 반복해서 입력으로 들어가면 동일한 연산을 반복적으로 수행하기 때문에 비효율적이다. 위 그림에서는 네 번의 생성만 수행했지만 
수백에서 수천 토큰을 생성한다면 동일한 연산을 수백 ~ 수천 번 수행하게 된다.  
  
셀프 어텐션 연산은 입력 텍스트에서 어떤 토큰이 서로 관련되는지 계산해서 그 결과에 따라 토큰 임베딩을 새롭게 조정한다. 이때 관련도를 계산하기 위해 
토큰 임베딩을 쿼리, 키, 값 벡터로 변환하는 선형 변환을 수행했다. 생성 속도를 높이기 위해 동일한 입력 토큰에 대해서는 키, 값, 벡터로의 변환을 
반복해서 수행하지 않고 계산 결과를 저장하고 있다가 다시 사용하는 방법을 사용한다.  
  
# **중복 연산을 줄이는 KV 캐시**  
KV 캐시는 셀프 어텐션 연산 과정에서 동일한 입력 토큰에 대해 중복 계산이 발생하는 비효율을 줄이기 위해 먼저 계산했던 키와 값 결과를 메모리에 저장해 
활용하는 방법을 말한다. 키와 값 계산 결과를 저장하기 떄문에 KV(Key-Value) 캐시라는 이름을 붙인 것이다.  
  
![img.png](image/img4.png)  
  
KV 캐시를 사용했을 때와 사용하지 않았을 떄의 생성 과정을 비교한 그림이 위 그림이다. 그림에서 '검은 고양이가 밥을'을 입력했을 때 다음 토큰으로 '먹고'가 
등장할 확률이 가장 높아 '검은 고양이가 밥을 먹고'를 생성하고 이제 다음 토큰을 예측하는 중이라고 하자. KV 캐시를 사용하지 않는 경우 이전에 수행했던 
'검은', '고양이가', '밥을'을 키와 값 벡터로 변환하는 동일한 연산을 반복해야 한다. 하지만 KV 캐시를 사용한다면 '검은', '고양이가', '밥을' 부분은 KV캐시에서 
계산 결과를 가져와 사용하고 새로운 토큰인 '먹고' 부분만 새롭게 연산한다.  
  
![img.png](image/img5.png)  
  
더 빠른 텍스트 생성을 위해 KV 캐시를 사용하면 추론을 수행할 때 키와 값에 대한 계산 결과를 메모리에 저장하기 때문에 GPU 메모리는 위 그림과 같이 
모델이 차지하는 부분과 KV 캐시가 차지하는 부분으로 크게 나뉜다. 그림에서 13B 모델을 fp16 데이터 형식으로 불러와 모델 파라미터를 저장하는 데 
26의 메모리를 사용하고 순전파 연산에 필요한 메모리(그림의 기타)를 제외한 나머지는 KV 캐시로 사용한다.  
  
간단한 형태의 KV 캐시의 메모리 사용량은 다음 계산식을 통해 구할 수 있다.  
  
KV 캐시 메모리 = 2바이트 * 2(키와 값) * (레이어 수) * (토큰 임베딩 차원) * (최대 시퀀스 길이) * (배치 크기)  
  
계산식을 풀어서 설명해 보면 먼저 처음의 2는 fp16형식을 사용하기 때문에 2를 곱해 주는 것이다. 다음으로 KV 캐시는 키 캐시와 값 2개를 저장하므로 
2를 곱해 준다. 또한 셀프 어텐션 연산 결과는 어텐션 레이어의 수만큼 생기기 떄문에 레이어 수를 곱해 주고 토큰 임베딩을 표현하는 차원의 수만큼의 수를 
저장하기 떄문에 토큰 임베딩 차원을 곱한다. 마지막으로 최대로 생성하려는 시퀀스 길이만큼의 메모리를 미리 확보하기 위해 최대 시퀀스 길이를 곱하고 배치 
크기가 커질수록 저장하는 데이터가 많아지기 때문에 배치 크기를 곱한다. 파라미터가 130억 개인 라마-2 13B 모델에서 각각의 값을 확인하면 다음과 같다. 
허깅페이스 모델 허브의 meta-llama/Llama-2-13b-hf 모델 설정(https://huggingface.co/meta-llama/Llama-2-13b-hf/blob/main/config.json)에서 
확인할 수 있다.  
  
- 레이어 수:40  
- 토큰 임베딩 차원: 5120  
- 최대 시퀀스 길이: 4096  
  
앞의 계산식에 라마-2 13B 모델의 값을 넣어 계산하면 KV 캐시를 저장하는 데 사용하는 메모리는 배치 크기 1당 3.125GB이다. 위 그림에서 40GB의 GPU 메모리 
중 모델을 저장하는 데 26GB를 사용했기 때문에 KV 캐시는 최대 14GB의 메모리를 사용할 수 있는데 배치 크기를 키울 때마다 추가로 3.125GB가 필요하기 때문에 
최대 배치 크기는 4 정도다.  
  
# **GPU 구조와 최적의 배치 크기**  
서빙이 효율적인지 판단하는 큰 기준으로는 1. 비용, 2. 처리량(throughput), 3. 지연 시간(latency)이 있다. 처리량은 시간당 처리한 요청 수(query/s)를 
의미하고 지연 시간은 하나의 토큰을 생성하는 데 걸리는 시간(token/s)을 말한다. 적은 비용으로 더 많은 요청을 처리하면서 생성한 다음 토큰을 빠르게 전달할 
수 있다면 효율적인 서빙이라고 할 수 있다. 여기서 비용이 GPU의 종류와 수에만 영향을 받는다고 가정하면 효율적인 서빙을 위해서는 같은 GPU로 처리량을 높이고 
지연 시간을 낮춰야 한다.  
  
![img.png](image/img6.png)  
  
GPU는 위 그림과 같은 구조로 되어 있다. 하나의 GPU는 여러 스트리밍 멀티프로세서(Streaming Multiprocessors, SM)으로 구성되고 각각의 SM에는 
연산을 수행하는 부분과 계산할 값을 저장하는 SRAM(Static Random Access Memory)이 있다. SRAM은 L1 캐시 또는 공유 메모리(shared memory)라고 
부르기도 한다. 연산을 수행하는 부분과 가까운 SRAM은 큰 메모리를 갖기 어렵기 때문에 큰 고대역폭 메모리(High Bandwidth Memory, HBM)에 큰 데이터를 
저장한다. 흔히 "GPU의 메모리가 16GB이다, 40GB이다"라고 할 때의 메모리는 HBM을 말한다.  
  
엔비디아의 A100 PCle GPU를 예시로 들면 1개의 A100 GPU에는 108개의 SM이 있고 각각의 SRAM은 192KB 메모리여서 모두 합치면 약 20MB의 데이터를 
저장할 수 있다. A100의 float16 형식에 대한 연산 속도는 312TF(테라플롭스 - TeraFLOPS, Tera Floating Point Operations per second)로 
1초에 312 * 10¹²번 연산이 가능하다. HBM의 용량은 40GB이고 HBM에서 데이터를 전달하는 대역폭(bandwidth)은 1,555GB/s이다. 자세한 사항은 A100 
GPU의 스펙 링크(https://www.nvidia.com/content/dam/en-zz/Solutions/Data-Center/a100/pdf/a100-80gb-datasheet-update-nvidia-us-1521051-r2-web.pdf)
에서 확인할 수 있다.  
  
![img.png](image/img7.png)  
  
추론을 수행할 떄는 위 그림과 같이 배치 크기만큼의 토큰을 한 번에 생성한다. 그림에서 입력 배치의 각 문장(그림의 S)은 길이가 서로 다른데 추론을 
수행하면 각각 프롬프트 토큰(그림의 옅은 회색 토큰)의 뒤로 새롭게 생성한 토큰(그림의 짙은 회색 토큰)이 더해진다. KV 캐시를 사용하면 옅은 회색 부분은 
KV 캐시에서 가져오고 짙은 회색 토큰 부분만 실제 계산한다. 모델 파라미터가 차지하는 메모리를 P라고 할 때 계산량은 대략 2 * P * (배치 크기) 바이트다.  
  
그런데 모델의 추론 과정에서 실제 행렬 곱셈 연산을 수행하는 데만 시간이 걸리는 것은 아니다. 앞서 GPU의 구조를 살펴보면서 SRAM은 크기가 작기 떄문에 
모델을 고대역폭 메모리에 저장한다고 설명했는데 연산을 수행하기 위해서는 고대역폭 메모리에 있는 모델 파라미터를 SRAM으로 이동시켜야 한다. 즉 P 만큼의 
메모리를 이동시키는데 시간이 걸린다.  
  
![img.png](image/img8.png)  
  
연산에 걸리는 시간과 모델 파라미터를 이동시키는 데 걸리는 시간을 하나의 그래프에 나타내면 위 그림과 같다. 배치 크기가 커지면 연산에 필요한 시간은 
증가하지만 모델 파라미터의 이동에 걸리는 시간은 변함없다. 모델 이동 과정과 연산 수행 과정은 함께 진행되기 때문에 두 가지 시간이 같을 때가 최적의 
배치 크기가 된다. 만약 서로 다른 시간이 걸린다면 모델 파라미터만 이동시키거나 연산만 하면서 다른 한쪽이 멈추기 때문에 비효율이 발생한다.  
  
![img.png](image/img9.png)  
  
그래프를 다시 최적의 배치 크기 관점에서 정리하면 위 그림과 같다. 최적의 배치 크기(B*)보다 배치 크기가 작으면 모델 파라미터를 이동시키느라 연산이 
멈추는 비효율이 생기는데 이런 경우를 메모리 바운드(memory bound)라고 부른다. 반대로 배치 크기가 최적 크기보다 더 커지면 연산에 오랜 시간이 걸리기 
때문에 지연 시간이 길어지는데 이런 경우를 연산 바운드(compute bound)라고 부른다.  
  
A100 GPU에 그래프의 두 계산식을 활용해 최적의 배치 크기를 구하면 약 100이 나온다. 계산식은 다음과 같다.  
  
- 2 * P * 배치 크기 / 하드웨어 연산 속도 = P / 메모리 대역폭  
- 배치 크기 = 하드웨어 연산 속도 / (2 * 메모리 대역폭) = (312 * 10¹²) / (2 * 1555 * 10⁹) = 102.73  
  
하지만 앞서 라마-2 13B 모델을 사용할 경우 GPU 메모리가 40GB인 GPU에서 최대 배치 크기가 5였다. 최적의 배치 크기에 비해 훨씬 낮은 배치 크기로 
모델을 활용하는 것이다. GPU를 더 효율적으로 활용하기 위해서는 최대 배치 크기가 최적의 배치 크기에 가까워질 수 있는 방법을 찾아야 한다. GPU 메모리에 
올라가는 주요 데이터가 모델 파라미터와 KV 캐시이기 떄문에 배치 크기를 키우기 위한 방안은 크게 모델의 용량을 줄이는 방법과 KV 캐시의 용량을 줄이는 
방법으로 나눌 수 있다.  
  
# **KV 캐시 메모리 줄이기**  
  
![img.png](image/img10.png)  
  
트랜스포머 모델이 셀프 어텐션 연산을 수행할 때는 한 번의 어텐션 연산만 수행하는 것이 아니라 위 그림과 같이 여러 헤드에 대해 어텐션 연산을 수행하는 
멀티 헤드 어텐션(multi-head attention, MHA)을 사용했다. 한 번에 여러 헤드(그림의 h)에 대한 연산을 수행하기 때문에 쿼리와 키 사이에 다양한 
측면의 관련성을 반영할 수 있고 성능을 높일 수 있었다. 하지만 많은 수의 키와 값 벡터를 저장하기 때문에 KV 캐시에 더 많은 메모리를 사용하고 KV 캐시에서 
더 많은 데이터를 불러와 계산하기 때문에 그만큼 속도가 느려진다.  
  
이런 단점을 극복하기 위해 2019년 구글의 노암 샤지르가 모든 쿼리 벡터가 하나의 키와 값 벡터를 공유하는 멀티 쿼리 어텐션(multi-query attention, MQA) 
방식을 개발했다.  
  
![img.png](image/img11.png)  
  
멀티 쿼리 어텐션 방식은 위 그림의 c와 같이 여러 헤드의 쿼리 벡터가 하나의 키와 값 벡터를 사용한다. 따라서 그림 a의 멀티 헤드 어텐션이 8개의 
키와 값 벡터를 저장했다면 멀티 쿼리 어텐션은 1개의 키와 값 벡터만 저장하기 때문에 KV 캐시를 저장하는 데 훨씬 적은 메모리를 사용한다. 하지만 키와 
값을 1개만 사용하면서 멀티 헤드 어텐션에 비해 성능이 떨어지는 문제가 있었다. 2023년 구글은 이런 문제를 해결하기 위해 멀티 헤드 어텐션보다는 
키와 값의 수를 줄이지만 멀티 쿼리 어텐션보다는 많은 키와 값을 사용하는 그룹 쿼리 어텐션(grouped-query attention, GQA)을 개발했다. 그림 b의 
그룹 쿼리 어텐션은 2개의 쿼리 벡터당 1개의 키와 값 벡터를 사용해 결과적으로 4개의 키와 값을 사용한다. 멀티 헤드 어텐션과 멀티 쿼리 어텐션을 절충한 
방법으로 볼 수 있다.  
  
멀티 쿼리 어텐션이나 그룹 쿼리 어텐션과 같이 키와 값의 수를 줄이면서 얻을 수 있는 이드은 크게 추론 속도 향상과 KV 캐시 메모리의 감소가 있다. 
구글 연구진은 6개의 텍스트 요약 벤치마크(예: CNN, arXiv 등)와 1개의 질문 답변 벤치마크(TriviaQA)를 통해 평가하고 그 평균으로 세 가지 어텐션 
속도와 성능을 비교했다.  
  
![img.png](image/img12.png)  
  
위 그림에서 멀티 헤드 어텐션을 사용했을 때 가장 성능이 높지만 그룹 쿼리 어텐션을 사용했을 때와 거의 성능이 비슷했다. 속도는 멀티 쿼리 어텐션이 가장 
빨랐지만 그룹 쿼리 어텐션도 비슷한 수준이었다. 즉 그룹 쿼리 어텐션은 속도는 멀티 쿼리 어텐션만큼 빠르면서 성능은 멀티 헤드 어텐션과 비슷했다.  
  
멀티 쿼리 어텐션의 경우 멀티 헤드 어텐션과 비교했을 때 성능 저하가 뚜렷하기 때문에 키와 값을 줄인 이후에 기존의 학습 데이터로 추가 학습(uptraining)을 
수행했다.  
  
![img.png](image/img13.png)  
  
위 그림은 추가 학습에 사용하는 학습 데이터의 비율(알파)에 따라 성능이 어떻게 달라지는지를 나타낸 그래프다. 멀티 쿼리 어텐션의 경우 추가 학습하지 
않았을 때 멀티 헤드 어텐션과 비교했을 때 뚜렷한 성능 하락이 있고 학습을 기존 학습 데이터의 10%까지 사용해도 여전히 성능 차이가 있음을 확인할 수 
있다. 하지만 그룹 쿼리 어텐션을 사용한 경우 추가 학습을 하지 않더라도 멀티 헤드 어텐션과 성능 차이가 크지 않고 기존 학습 데이터의 약 5%만 
사용해 추가 학습을 수행해도 성능 차이가 거의 없음을 확인할 수 있다.  
  
지금까지의 내용을 정리하면 그룹 쿼리 어텐션은 사용하는 키와 값 벡터의 수를 줄임으로써 성능 하락이 거의 없이도 모델의 추론 속도를 향상하고 KV 캐시의 
메모리 사용량을 줄일 수 있었다. 그룹 쿼리 어텐션은 메타에서 2023년 발표한 라마-2에 사용되기도 하며 더 효율적인 모델 개발을 위해 활발히 활용되고 
있다.  
  
