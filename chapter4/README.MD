# **말 잘 듣는 모델 만들기**  
LLM은 다음 단어를 예측하는 방식으로 대량의 텍스트를 학습해서 뛰어난 텍스트 생성 능력을 보여줬다. 2020년 GPT-3는 단순히 다음 단어를 예측하는 
방식으로 학습했기 때문에 사용자의 요청에 적절히 응답하기보다는 사용자의 말에 이어질 법한 텍스트를 생성한다는 한계를 지니고 있었다. 오늘날 챗GPT를 
사용할 때 내 말에 이어질 텍스트를 생성해 달라고 요청하는 경우는 드물 것이다.  
  
OpenAI는 두 단계를 거쳐 GPT-3를 챗GPT로 변화시켰다. 먼저 네이버 지식인과 같이 요청(또는 질문)과 답변 형식으로 된 지시 데이터셋(instruction dataset)을 
통해 GPT-3가 사용자의 요청에 응답할 수 있도록 학습시켰다. 다음으로 사용자가 더 좋아하고 사용자에게 더 도움이 되는 답변을 생성할 수 있도록 추가 학습을 
시켰다. 이를 사용자의 선호(preference)를 학습한다고 말한다. 선호를 학습한 LLM은 예를 들어 차별적인 표현을 사용하지 않고 사용자가 위험해 질 수 있는 
정보(예: 약물 제조 방법)에 대한 답변을 피하는 등 더 정제된 답변을 생성하게 된다.  
  
사람들이 더 선호하는 답변을 생성할 수 있도록 모델을 조정하는 방법은 크게 강화 학습(reinforcement learning)을 사용하는 방법과 사용하지 않는 
방법으로 나눌 수 있다. OpenAI가 챗GPT를 개발할 때 강화 학습 방법 중 하나인 근접 정책 최적화(Proximal Policy Optimization, PPO)를 사용해 
선호 학습에 강화 학습이 필요하다고 알려졌다. 하지만 PPO는 하이퍼파라미터에 민감하고 학습이 불안정해 많은 연구자와 개발자에게 좌절을 안겼다. 이후 
강화 학습을 사용하지 않고 선호를 학습시키는 다양한 기술이 개발되고 활용되고 있다.  
  
