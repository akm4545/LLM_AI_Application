# **말 잘 듣는 모델 만들기**  
LLM은 다음 단어를 예측하는 방식으로 대량의 텍스트를 학습해서 뛰어난 텍스트 생성 능력을 보여줬다. 2020년 GPT-3는 단순히 다음 단어를 예측하는 
방식으로 학습했기 때문에 사용자의 요청에 적절히 응답하기보다는 사용자의 말에 이어질 법한 텍스트를 생성한다는 한계를 지니고 있었다. 오늘날 챗GPT를 
사용할 때 내 말에 이어질 텍스트를 생성해 달라고 요청하는 경우는 드물 것이다.  
  
OpenAI는 두 단계를 거쳐 GPT-3를 챗GPT로 변화시켰다. 먼저 네이버 지식인과 같이 요청(또는 질문)과 답변 형식으로 된 지시 데이터셋(instruction dataset)을 
통해 GPT-3가 사용자의 요청에 응답할 수 있도록 학습시켰다. 다음으로 사용자가 더 좋아하고 사용자에게 더 도움이 되는 답변을 생성할 수 있도록 추가 학습을 
시켰다. 이를 사용자의 선호(preference)를 학습한다고 말한다. 선호를 학습한 LLM은 예를 들어 차별적인 표현을 사용하지 않고 사용자가 위험해 질 수 있는 
정보(예: 약물 제조 방법)에 대한 답변을 피하는 등 더 정제된 답변을 생성하게 된다.  
  
사람들이 더 선호하는 답변을 생성할 수 있도록 모델을 조정하는 방법은 크게 강화 학습(reinforcement learning)을 사용하는 방법과 사용하지 않는 
방법으로 나눌 수 있다. OpenAI가 챗GPT를 개발할 때 강화 학습 방법 중 하나인 근접 정책 최적화(Proximal Policy Optimization, PPO)를 사용해 
선호 학습에 강화 학습이 필요하다고 알려졌다. 하지만 PPO는 하이퍼파라미터에 민감하고 학습이 불안정해 많은 연구자와 개발자에게 좌절을 안겼다. 이후 
강화 학습을 사용하지 않고 선호를 학습시키는 다양한 기술이 개발되고 활용되고 있다.  
  
# **코딩 테스트 통과하기: 사전 학습과 지도 미세 조정**  
# **코딩 개념 익히기: LLM의 사전 학습**  
비유 -> 사람이 익히는 과정: 그에 대응되는 LLM의 학습 과정  
비전공자 A가 개발자를 지원하며 파이썬을 배운다고 가정한다.  A는 파이썬 문법을 공부하면서 기본 문법을 익히고 클론 코딩, 책을 찾아볼 것이다.  
  
![img.png](img2.png)  
  
A가 다양한 자료를 보며 프로그래밍을 처음 공부하는 과정은 위 그림과 같이 LLM의 사전 학습과 유사하다. LLM은 보통 인터넷상에 있는 다양한 텍스트 데이터를 
수집한 대용량의 텍스트로 사전 학습한다. LLM은 딥러닝 기반의 언어 모델이며 다음 단어를 예측하는 언어 모델링을 통해 텍스트를 이해하는 방버을 학습한다.  
  
2023년 메타에서 공개한 라마-2(Llama-2) 모델은 약 10TB 분량의 텍스트를 사전 학습에 사용했는데 사전 학습 데이터의 경우 코드, 블로그, 가사, 광고 등 
다양한 글이 섞여 있기 떄문에 사전 학습 데이터에서 다음 단어를 예측하는 방법으로 학습하는 경우 LLM이 특정한 형태로 응답하거나 사용자의 요청에 따라 
응답하길 기대하기는 어렵다. 사전 학습 동안 LLM이 언어에 대한 전체적인 이해도가 높아지고 바로 다음에 올 단어를 점점 더 잘 예측하게 된다.  
  
![img.png](img2.png)  
  
언어 모델이란 위 그림과 같이 다음에 올 단어의 확률을 예측하는 모델이다. "최고의 프로그래밍 언어는?" 이라는 문장을 입력했을 때 사전에 있는 단어가 다음에 
나타날 확률을 각각 계산한다. 영어는 언어이긴 하지만 프로그래밍 언어는 아니기 떄문에 중간 정도의 확률로 예측한다.  
  
![img.png](img3.png)  
  
다음 단어를 예측하는 언어 모델을 학습시킬 때는 위 그림과 같이 학습 데이터의 일부를 입력으로 넣고 바로 다음에 나오는 정답 토큰을 맞추도록 학습한다. 
여기서 맞추도록 학습한다는 것은 예를 들어 "최고의 프로그래밍 언어는?"이라는 입력에 대해 다음으로 오는 정답 토큰이 '파이썬'이라면 그림 오른쪽과 같이 
언어 모델이 다음 단어로 '파이썬'을 예측할 확률이 높아지도록 학습시키는 것을 말한다. 이런 과정을 수많은 학습 데이터에 대해 수행하면서 어떤 단어가 
다음에 올지 학습하게 된다.  
  
# **연습문제 풀어보기: 지도 미세 조정**  
프로그래밍 언어의 문법이나 자료구조와 알고리즘 같은 기본 개념을 익혔다면 다음으로 필요한 건 코딩 테스트 연습 문제를 풀어보는 것이다. 예제 문제를 
충분히 풀어보고 풀이 과정을 익혀야 한다. 코딩 테스트 서비스마다 정답을 작성하는 방식이 다른데 서비스에 맞게 코드를 작성할 수 있는 연습도 필요하다.  
  
LLM도 사용자의 요청에 적절히 응답하기 위해서는 사람이 코딩 테스트를 준비할 때와 비슷하게 요청의 형식을 적절히 해석하고 응답의 형태를 적절히 작성하며 
요청과 응답이 잘 연결되도록 추가로 학습한다. 이를 지도 미세 조정(supervised fine-tuning)이라고 한다. 지도 미세 조정에서 '지도'란 학습 데이터에 
정답이 포함되어 있다는 의미다. 지도 미세 조정을 통해 LLM은 사용자의 요청에 맞춰 응답하도록 학습하는데 이를 정렬(alignment)이라고 한다. 사람의 
요청과 LLM의 응답이 정렬되도록 한다는 의미다.  
  
지도 미세 조정에 사용하는 데이터셋을 지시 데이터셋(instruction dataset)이라고 부른다. 사용자의 지시에 맞춰 응답한 데이터셋이라는 의미다. 이와 같은 
지시 데이터셋에 비해 사전 학습 데이터셋은 형식이 너무나도 다양하고 사용자의 요청에 응답하는 형식의 데이터는 적다. 특히 양질의 데이터는 훨씬 더 적을 수밖에 
없다. 딥러닝 모델은 기본적으로 학습 데이터에 있는 행동을 배우기 때문에 학습 데이터에 요청에 응답하는 데이터가 적다면 그 행동은 잘 배우지 못한다.  
  
이런 문제를 보완하기 위해 사용자의 요구사항과 그에 대한 응답(정답)을 구조화한 데이터를 구축하고 언어 모델의 학습에 활용한다. 지시 데이터셋은 사용자의 요청을 
형식에 맞춰 작성하고 그에 대해 적절한 형식의 응답을 하는 형태다.  
  
![img.png](img4.png)  
  
OpenAI는 2022년에 공개한 챗GPT를 개발하면서 위 그림과 같이 데이터 레이블러(labeler)를 고용해 13000개가 넘는 지시 데이터셋을 구축해 모델을 학습시켰다.  

{  
    "instruction": "Create a classification task by clustering the given list of items.",  
    "input": "Apples, oranges, bananas, strawberries, pineapples",  
    "output": "Class 1: Apples, Oranges\nClass 2: Bananas, Strawberries\nClass 3: Pineapples",  
    "text": "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n\n### Instruction:\nCreate a classification task by clustering the given list of items.\n\n### Input:\nApples, oranges, bananas, strawberries, pineapples\n\n### Response:\nClass 1: Apples, Oranges\nClass 2: Bananas, Strawberries\nClass 3: Pineapples",  
}  
  
스탠퍼드대학교에서 오픈 소스 라마 모델을 추가 학습한 알파카(Alpaca)를 공개할 때 사용한 알파카 데이터셋은 위와 같다. 스탠퍼드대학교는 알파카 데이터셋을 
구축하기 위해 OpenAI의 text-davinci-003 모델을 사용해 175개의 예시 태스크로부터 52000개의 데이터셋을 구축했다.  
  
지시사항(instruction)은 사용자의 요구사항을 표현한 문장이다. 입력(input)에는 답변을 하는 데 필요한 데이터가 들어간다. 출력(output)은 지시사항과 
입력을 바탕으로 한 정답 응답이다. 마지막으로 텍스트(text)는 지시사항, 입력, 출력을 정해진 포맷으로 하나로 묶은 데이터다.  
  
위 데이터는 사용자의 요청과 응답 데이터가 포함돼 있지만 아직 특정한 형식을 갖추지는 않았는데 아래 데이터와 같은 템플릿에 위 데이터를 넣어 형식을 맞춘다. 
아래 데이터를 포면 첫 두 줄에서 "작업을 설명한 지시사항과 맥락 정보인 입력을 바탕으로 요청에 대한 적절한 응답을 작성하라"라는 안내를 하고 지시사항, 입력, 
응답을 각각 넣어준다. 이때 LLM이 데이터의 형식을 인식할 수 있도록 ###으로 텍스트를 구분하고 있다.  
  
f"""  
Below is an instruction that describes a task, paired with an input that provides  
further context.  
Write a response that appropriately completes the request.\n\n  
```### Instruction:\n{instruction}\n\n```   
```### Input: \n{input}\n\n```  
```### Response: \n{output}```  
"""  
  
지도 미세 조정이라는 새로운 용어가 붙었지만 사전 학습 때와 동일하게 다음 단어를 예측하는 인과적 언어 모델링을 사용해 학습한다. 즉 지도 미세 조정이라는 
이름은 LLM이 학습하는 방식이 달라서가 아니라 학습하는 데이터셋에 차이가 있기 때문에 지어진 것이다.  
  
지시 데이터셋을 어떻게 구성하는 게 좋은지 아직 정답은 없다. 메타, 마이크로소프트등 여러 회사에서 어떻게 지시 데이터셋을 구성했을 때 LLM의 성능을 
높일 수 있는지 연구를 진행했다.  
  
# **좋은 지시 데이터셋이 갖춰야 할 조건**  
지시 데이터셋을 준비한다고 했을 때 데이터의 양, 품질, 질문의 형식, 답변의 형식 등 다양한 측면에서 고민해 봐야 한다.  
  
먼저 얼마나 많은 지시 데이터셋이 필요한가? 라는 질문에 대해 메타는 2023년 [Less Is More for Alignment(정렬을 위해서는 적은 것이 더 낫다)]라는 
논문을 발표하며 파라미터가 650억 개인 라마 모델을 정렬하는 데 선별한 1000개 정도의 지시 데이터셋으로도 가능했다고 밝혔다. 이렇게 정렬한 모델의 
이름은 리마(LiMA)다. LLM의 사전 학습을 위해 대용량의 텍스트 데이터를 사용하고 라마-2 모델의 경우 약 10TB에 달하는 학습 데이터를 사용했다. 일반적으로는 
수백 GB에서 수 TB의 데이터로 사전 학습을 수행한다. 그런데 지도 미세 조정에서 불과 1000개 정도의 데이터로도 모델이 사용자의 요청에 응답하도록 만들 수 
있었다는 것이다. 그뿐 아니라 약 300개의 요청에 대한 응답 결과를 사람이 직접 확인하면서 평가했을 때 리마 모델은 앞서 52000개의 지시 데이터셋으로 학습한 
알파카 모델보다도 응답의 품질이 뛰어났고 구글의 바드(Bard), OpenAI의 GPT-4와 비교했을 때도 4~50%의 답변은 리마가 더 뛰어나거나 비슷한 수준이었다.  
  
또한 지시 데이터셋에서 지시사항이 다양한 형태로 되어 있고 응답 데이터의 품질이 높을수록 정렬한 모델의 답변 품질이 높아진다고 밝혔다. 메타가 지시 
데이터셋을 구성할 때 사요한 데이터셋은 크게 위키하우(wikiHow) 데이터와 스택익스체인지(Stack Exchange)데이터였다. 두 데이터 모두 한 사용자가 질문을 
하고 다른 사용자들이 그에 대해 답해 주는 마치 네이버 지식인과 같은 형태다. 위키하우는 답변의 퀄리티가 높지만 요청 또는 질문이 모두 '하는 방법' 
형식이었고 스택익스체인지는 질문의 형식은 다양하지만 답변의 퀄리티가 낮았다. 메타 연구진은 스택익스체인지 데이터 중 답변의 퀄리티가 높은 데이터만 선별해 
질문의 형식도 다양하면서 답변의 퀄리티가 높은 지시 데이터셋을 구축해 높은 성능의 라마 모델을 만들 수 있었다.  
  
메타에서는 이런 결과를 바탕으로 피상적인 정렬 가설(superficial alignment hypothesis)을 주장했다. 피상적인 정렬 가설이란 모델의 지식이나 능력은 
사전 학습 단계에서 대부분 학습하고 정렬 데이터를 통해서는 답변의 형식이나 모델의 능력과 지식을 어떻게 나열할지 정도만 추가로 배우기 때문에 적은 정렬 
데이터로도 사용자가 원하는 형태의 답변을 생성할 수 있다는 가설이다. 리마 모델의 결과만으로 피상적인 정렬 가설이 맞다고 판단하긴 어렵지만 지도 미세 조정을 통해 
LLM을 정렬하고자 할 때 기초 모델(foundation model)을 잘 선택한다면 작은 지시 데이터셋으로도 정렬이 가능하다는 힌트는 얻을 수 있다.  
  
지시 데이터셋의 품질도 LLM의 성능에 영향을 미친다. 2023년 마이크로소프트는 [Textbooks Are All You Need(텍스트북이면 충분하다)]라는 연구 결과를 
발표하며 지시 데이터셋의 품질을 높이면 더 작은 데이터셋과 더 작은 모델로도 높은 성능을 달성할 수 있다고 주장했다. 마이크로소프트가 2023년 6월 
공개한 파이썬 코드 생성 모델인 파이(Phi)는 파라미터가 13억 개인 작은 모델임에도 GPT-3.5나 2023년 마이크로소프트와 베이징대학교가 공개한 코드 
생성 모델인 위자드코더(WizardCoder)와 견줄 수 있는 성능을 보였다. 위자드코더가 160억 개의 파라미터를 갖고 GPT-3.5가 1750억 개의 파라미터를 갖는 
걸 감안할 때 파이 모델의 성능은 꽤 놀랄 만 했다.  
  
마이크로소프트에서 파이 모델을 훈련하기 위한 학습 데이터를 구축할 때 공개된 코드 데이터셋을 그대로 사용하지 않고 선별해서 사용했는데 그 이유는 다음과 같다.  
- 외부 모듈이나 파일을 사용하기 때문에 하나의 코드 파일 자체에서 의미를 이해하기 어려운 경우가 많았다.  
- 대부분의 파일은 의미 있는 연산보다는 보일러플레이트 코드나 설정 파일이었다.  
- 알고리즘 로직을 담고 있는 코드도 복잡하거나 제대로 문서화되지 않은 함수들 사이에 있어 의미를 파악하기 어려웠다.  
- 특정 주제나 사례에 관련된 코드가 많아 특정 개념이나 스킬에 불균형한 데이터셋 분포를 보였다.  
  
이런 이유로 코드 데이터셋에서 모델의 학습에 도움이 되는 교육적 가치가 높은 데이터를 선별했다.  
  
![img.png](img5.png)  
  
위 그림의 a 코드부는 교육적 가치가 높은 코드로 코드의 이름에 기능에 대한 의미가 충분히 담겨 있고 어떤 기능을 수행하는지 독스트링(Docstring)으로 
잘 정리돼 있다. 반면 b의 코드부는 코드의 대부분이 Default라는 클래스의 속성을 설정하는 내용으로 구성돼 있다.  
  
이 외에도 사람이 새로운 개념을 배울 때 예제 문제를 공부하는 것처럼 GPT-3.5를 활용해 코드 예제 데이터셋을 생성해 학습 데이터에 추가했다.  
  
![img.png](img6.png)  
  
에제 데이터셋은 위 그림과 같이 함수의 독스트링 내용을 바탕으로 코드를 구현하는 형식이다. 이 코드부의 독스트링에는 단어(word)안에 있는 문자(letter)를 
맞추는 게임을 할 때 아직 정답으로 예측한 적 없는 문자 리스트를 반환한다는 함수의 목적과 입력 인자의 형식과 의미, 반환하는 데이터의 타입과 정보가 
담겨 있다(1). (2)에는 독스트링에서 설명한 함수의 동작을 수행하는 코드가 작성돼 있다. 이런 형태의 데이터를 통해 파이 모델은 함수의 이름과 설명을 
바탕으로 함수를 구현하는 방법을 학습하게 된다.  
  
![img.png](img7.png)  
  
교육적 가치가 높은 데이터를 필터링하고 코드 예제 데이터셋을 추가했을 때 모델 성능의 변화는 위 그림과 같다. 모델의 평가에는 LLM의 코드 생성 정확도를 
평가하는 벤치마크인 HumanEval 데이터셋을 사용했다. 공개된 코드 데이터셋인 스택+(The Stack+)를 그대로 사용했을 때에 비해 교육적 가치가 높은 데이터를 
선별한 코드 텍스트북(code textbook)을 학습 데이터로 사용했을 때 정확도가 17에서 29로 크게 상승했다. 또한 GPT-3.5로 생성한 고품질의 예제 데이터셋으로 
추가 학습시키자 정확도는 29에서 51로 또 한 번 크게 상승했다. 학습 데이터의 품질이 높다면 더 많은 양의 데이터로 학습할 때보다도 더 높은 성능을 
달성할 수 있다는 사실을 확인할 수 있다.  
  
메타와 마이크로소프트의 연구를 통해 좋은 지시 데이터셋이 갖춰야 하는 조건을 정리하면 다음과 같다.  
- 지시 데이터셋을 작은 규모로 구축하더라도 모델이 지시사항의 형식을 인식하고 답변하도록 만들 수 있다.  
- 지시사항이 다양한 형태이고 답변의 품질이 높을수록 모델의 답변 품질도 높아진다.  
- 학습 데이터의 품질을 높이기 위해 모델의 목적에 맞춰 학습 데이터의 교육적 가치를 판단하고 교육적 가치가 낮은 데이터를 필터링하는 방법을 사용할 수 있다.  
- 교재의 예제 데이터와 같은 고품질의 데이터를 학습 데이터에 추가하면 성능을 크게 높일 수 있다.  
  
# **채점 모델로 코드 가독성 높이기**  
# **선호 데이터셋을 사용한 채점 모델 만들기**  
코드 가독성을 평가하는 모델을 만들기 위해서는 모델을 학습시킬 학습 데이터를 구축해야 한다.  
  
![img.png](img8.png)  
  
가독성 채점 모델을 학습시키기 위해 위 그림과 같은 가독성 비교 데이터셋을 구축했다. 그림에서 코드 A와 코드 B를 비교해 더 가독성이 높은 코드를 선택하고 
선택한 코드를 선호 데이터(chosen data), 선택하지 않은 코드를 비선호 데이터(rejected data)라고 한다. 이런 형태로 두 데이터 중 사람이 더 선호하는 
데이터를 선택한 데이터셋을 선호 데이터셋이라고 한다. 선호 데이터와 비선호 데이터는 정해진 것은 아니고 비교하는 대상에 따라 달라질 수 있는데 만약 코드 
B가 새로운 코드 C 보다 코드 가독성이 높다면 그 관계에서는 코드 B가 선호 데이터가 된다.  
  
![img.png](img9.png)  
  
왜 코드 가독성 채점 모델을 만들 때 위 그림과 같이 코드와 그 코드에 대한 가독성 점수를 직접 수집해 사용하지 않고 두 코드를 비교한 데이터셋을 사용할까? 
코드에서 직접 점수를 매긴 데이터셋은 구축하기가 어렵기 떄문이다. 코드를 보고 그 가독성을 바로 평가하는 작업은 좀처럼 쉽지 않다.  
  
여러 코드를 바탕으로 어떤 코드가 더 가독성이 높은지를 선택해 선호 데이터셋을 구축하고 나면 채점 모델이 선호 데이터에 비선호 데이터보다 높은 점수를 
주도록 채점 모델을 학습시킨다. 이런 방식으로 코드 가독성을 평가하는 채점 모델을 만들 수 있다.  
  
![img.png](img10.png)  
  
OpenAI도 챗GPT를 개발하는 과정에서 같은 학습 방식을 사용했다. 지도 미세 조정을 마친 LLM은 사용자의 요청에 맞춰 응답하기 때문에 사용자에게 결과적으로 
해가 될 수 있는 정보도 제공하고 차별적인 답변도 생성하는 등 문제가 있었는데 이런 부분을 줄이기 위해 위 그림과 같이 생성된 답변의 점수를 평가하는 
리워드 모델(reward model)을 만들었다. 순서대로 살펴보면 먼저 지도 미세 조정을 마친 LLM에 지시사항을 입력해 여러 응답(A, B, C)을 생성한다. 
그리고 레이블러가 응답을 비교해 더 좋다고 판단하는 순서를 정해 선호 데이터셋을 구축한다. 위 그림에서는 레이블러가 A > C > B 순서로 좋다고 판단했는데 
이렇게 구축한 선호 데이터셋을 사용해 리워드 모델이 응답 A의 점수가 응답 C의 점수보다 높도록, 응답 A의 점수가 응답 B의 점수보다 높도록 학습한다.  
  
# **강화 학습: 높은 코드 가독성 점수를 향해**  
OpenAI는 2022년 [Training language models to follow instructions with human feedback(사람의 피드백을 사용해 언어 모델이 지시를 따르도록 학습하기)] 
이라는 논문에서 강화 학습을 사용해 LLM이 리워드 모델로부터 더 높은 점수를 받도록 학습시킨 과정을 공개했다. 강화 학습을 사용했기 때문에 이 학습 방법을 
사람의 피드백을 활용한 강화 학습(Reinforcement Learning from Human Feedback, RLHF)이라고 부른다.  
  
![img.png](img11.png)  
  
강화 학습에서는 위 그림과 같이 에이전트(agent)가 환경(environment)에서 행동(action)을 한다. 행동에 따라 환경의 상태(state)가 바뀌고 행동에 대한 
보상(reward)이 생기는데 에이전트는 이 변화된 상태를 인식하고 보상을 받는다. 에이전트는 가능하면 더 많은 보상을 받을 수 있도록 행동을 수정하면서 
학습한다. 이때 에이전트가 연속적으로 수행하는 행동의 모음을 에피소드(episode)라고 한다.  
  
![img.png](img12.png)  
  
코딩 테스트 문제를 푸는 예시를 강화 학습의 관점에서 나타낸 것이 위 그림이다. 사용자는 채점 시스템의 점수를 받으며 코드의 변수명을 바꾸는 등 
여러 행동을 한다. 이때 사용자는 에이전트이고 채점 시스템은 환경, 채점 시스템이 매긴 점수는 보상, 행동을 통해 변화된 코드가 변화된 상태라고 할 
수 있다. 사용자가 높은 점수를 받기 위해 한 여러 행동을 하나로 묶으면 에피소드가 된다.  
  
![img.png](img13.png)  
  
언어 모델이 RLHF를 통해 학습하는 과정을 나타낸 그림은 위와 같다. 언어 모델은 다음 단어를 예측하는 방식으로 토큰을 하나씩 생성하는데 강화 학습 
관점에서는 토큰 생성을 하나의 행동으로 볼 수 있다. 언어 모델이 텍스트를 모두 생성하면 리워드 모델이 생성한 텍스트를 평가하고 점수를 매긴다. 
언어 모델은 행동을 취할 때마다 보상을 받지 않고 전체 생성 결과에 대해 리워드 모델의 점수를 받는다.  
  
이와 같은 방식으로 언어 모델은 생성한 문장의 점수가 높아지는 방향으로 학습한다. 그런데 이때 보상을 높게 받는 데에만 집중하는 보상 해킹(reward hacking)
이 발생할 수 있다. 예를 들어 코드 가독성 점수를 높게 받는 방법으로 깔끔한 코드를 작성하는 게 아니라 아예 코드를 작성하지 않거나 print("hello world") 같은
간단한 코드만 작성해서 코드 가독성 점수를 높게 받으려고 할 수 있다. OpenAI는 보상 해킹을 피하기 위해 PPO라는 강화 학습 방법을 사용했다.  
  
