# **GPU 효율적인 학습**  
딥러닝 모델이 입력 데이터를 처리해 결과를 내놓을 때까지 많은 행렬 곱셈 연산을 처리한다. 행렬 곱셈은 숫자를 곱하고 더하는 단순한 연산을 반복 
계산하는 형태인데 GPU(Graphic Processing Unit)는 이러한 단순한 곱셈을 동시에 여러 개 처리하는 데 특화된 처리 장치다. 따라서 딥러닝 모델의 
연산을 빠르게 처리하기 위해 GPU를 많이 활용한다. GPU는 한정된 메모리를 갖고 있는데 최근 LLM의 등장과 함께 모델의 크기가 커지면서 하나의 GPU에 
모델을 올리지 못하거나 모델의 학습과 추론을 위해 더 많은 GPU가 필요해졌다. 하지만 GPU는 가격이 비싸기 때문에 일부 기업을 제외하고눈 풍부하게 사용하기 
어렵다. 이런 상활을 개선하고 더 많은 사람이 발전된 AI 기술을 사용할 수 있도록 GPU를 효율적으로 활용할 수 있는 방법을 찾기 위한 기술 발전이 
빠르게 이뤄지고 있다.  
  
구글 코랩 설치  
!pip install transformers==4.40.1 datasets==2.19.0 accelerate==0.30.0 peft==0.10.0  
bitsandbytes==0.43.1 -qqq  
  
# **GPU에 올라가는 데이터 살펴보기**  
딥러닝 모델을 학습시키고 추론하기 위해 GPU를 사용할 때 가장 자주 만나는 에러 중 하나는 OOM(Out of Memory)에러다. OOM 에러란 한정된 GPU 
메모리에 데이터가 가득 차 더 이상 새로운 데이터를 추가하지 못해 발생하는 에러다. GPU 메모리에는 기본적으로 딥러닝 모델 자체가 올라간다. 딥러닝 
모델은 수많은 행렬 곱셈을 위한 파리미터의 집합이다. 각각의 파라미터는 소수 또는 정수 형식의 숫자이다.  
  
# **딥러닝 모델의 데이터 타입**  
컴퓨터에서는 일반적으로 소수 연산을 위해 32비트 부동소수점(float32)을 사용한다. 만약 더 세밀한 계산이 필요하다면 64비트 부동소수점(float64)을 
사용한다. 부동소수점을 나타내는 비트의 수가 커질수록 표현할 수 있는 수의 범위나 세밀한 정도가 달라진다. 딥러닝 모델은 입력한 데이터를 최종 결과로 
산출할 때까지 많은 행렬 곱셈에 사용하는 파라미터로 구성된다. 예를 들어 파라미터가 70억 개인 LLM에는 행렬 연산에 사용되는 70억 개의 수가 저장돼 
있다. 따라서 LLM 모델의 용량은 모델을 몇 비트의 데이터 형식으로 표현하는지에 따라 달라진다.  
  
과거에는 딥러닝 모델을 32비트(4바이트) 부동소수점 형식을 사용해 저장했다. 하지만 성능을 높이기 위해 점점 더 파라미터가 많은 모델을 사용하기 시작했고 
모델의 용량이 너무 커 하나의 GPU에 올리지 못하거나 계산이 너무 오래 걸리는 문제가 발생했다. 이런 문제를 해결하기 위해 성능은 유지하면서 점점 더 
적은 비트의 데이터 타입을 사용하는 방향으로 딥러닝 분야가 발전했다. 최근에는 주로 16비트로 수를 표현하는 fp16 또는 bf16(brain float 16)을 
주로 사용한다.  
  
![img.png](img2.png)  
  
위 그림에서 fp32와 fp16, bf16을 비교하고 있다. fp32는 32비트를 사용해 수를 표현하는데 그림에서 S는 부호(sign), E는 지수(exponent), M은 가수(mantissa)를 
의미한다. 지수는 수를 표현할 수 있는 범위의 크기를 결정하고 가수는 표현할 수 있는 수의 촘촘함을 결정한다. fp32는 지수로 8비트, 가수로 23비트를 사용하는데 
fp32를 사용하면 넓은 범위를 촘촘하게 표현할 수 있다. 반면 fp16 또는 bf16 형식은 사용하는 비트를 절반으로 줄였기 때문에 fp32에 비해 표현할 수 있는 
범위나 세밀함이 제한된다. fp16은 지수로 5비트, 가수로 10비트를 사용하기 때문에 fp32에 비해 표현할 수 있는 범위도 좁고 세밀함도 떨어진다. fp16이 
표현할 수 있는 수의 범위가 좁아 딥러닝 연산 과정에서 수를 제대로 표현하지 못하는 문제가 발생했다. bf16은 이런 문제를 줄이기 위해 지수에 fp32와 같이 8비트를 
사용하고 가수에 7비트만 사용하도록 개발됐다.  
  
딥러닝 모델은 학습과 추론 같은 모델 연산 과정에서 GPU 메모리에 올라가기 떄문에 모델의 용량이 얼마인지가 GPU 메모리 사용량을 체크할 때 중요하다. 
딥러닝 모델의 용량은 파라미터의 수에 파라미터당 비트(또는 바이트) 수를 곱하면 된다. 예를 들어 파라미터가 10억 개인 모델이 fp16 형식으로 저장돼 있다면 총 20억 
바이트가 된다. 20억 바이트를 기가바이트 단위로 표현하려면 1024로 세 번 나눠줘야 하는데 10억이 1024를 세 번 곱한 값과 가깝기 떄문에 간단히 계산하면 모델의 
용량은 2GB라고 할 수 있다. 최근 LLM은 모델이 커지면서 7B 모델과 같이 10억을 의미하는 B(billion)를 단위로 사용하는데 B를 지우고 모델의 데이터 타입 
바이트 수를 곱하면 모델의 용량이 된다. 예를 들어 파라미터가 70억 개인 7B 모델이 16비트(2바이트) 데이터 형식으로 저장된다면 모델의 용량은 7*2 = 14GB가 된다.  
  
# **양자화로 모델 용량 줄이기**  
모델 파라미터의 데이터 타입이 더 많은 비트를 사용할수록 모델의 용량이 커지기 때문에 더 적은 비트로 모델을 표현하는 양자화(quantization)기술이 개발됐다. 
예를 들어 fp32로 저장하던 모델을 fp16 형식으로 저장하면 모델의 용량은 절반이 된다. 하지만 fp32가 fp16에 비해 더 넓은 범위의 수를 더 세밀하게 
표현할 수 있기 때문에 fp16으로 변환하면 fp32의 수가 담고 있던 정보가 소실될 수 있는데 이런 이유로 양자화를 수행하면 딥러닝 모델의 성능이 저하된다. 
따라서 양자화 기술에서는 더 적은 비트를 사용하면서도 원본 데이터의 정보를 최대한 소실 없이 유지하는 것이 핵심 과제라고 할 수 있다.  
  
원본 데이터의 정보를 최대한 유지하면서 더 적은 용량의 데이터 형식으로 변환하려면 변환하려는 데이터 형식의 수를 최대한 낭비하지 않고 사용해야 한다.  
  
![img.png](img2.png)  
  
예를 들어 위 그림과 같이 fp32의 데이터를 int8로 변환한다고 할 때 fp32는 하나의 수를 표현하기 위해 32비트나 사용하지만 int8은 8비트만 사용하기 때문에 
사용할 수 있는 수가 훨씬 적다. int8의 경우 256개의 수(-128~127)만 표현할 수 있기 때문에 256개 수 가운데 사용되지 않는 수를 줄여야 한다. 
위 그림에서는 두 데이터 형식의 최댓값을 각각 대응시키고 최솟값을 각각 대응시키는 간단한 양자화 방식을 사용했다. 그림에서 fp32범위의 양쪽 끝에는 
데이터가 없는데 서로 최대와 최소를 대응시키는 경우 int8의 양쪽 끝 수는 사용하는 데이터가 없이 존재해 낭비되는 문제가 발생한다.  
  
![img.png](img3.png)  
  
낭비를 줄이기 위해 위 그림과 같이 데이터 형식 자체의 최대/최소를 대응시키는 것이 아니라 존재하는 데이터의 최댓값 범위로 양자화하는 방법도 있다. 
그림에서 absmax는 절대 최댓값을 말하는데 양수와 음수를 통틀어 가장 크기가 큰 값을 찾는다. 예를 들어 데이터에서 가장 큰 수가 15이고 가장 작은 
수가 -17이라면 absmax는 17이 된다. 이와 같은 양자화 방식을 사용할 경우 int8 값을 낭비하던 문제를 해결할 수 있다.  
  
![img.png](img4.png)  
  
절대 최댓값을 활용한 방법도 문제를 완전히 해결하지는 못한다. 이 방법은 대부분의 경우 잘 동작하지만 이상치(outlier)가 있는 경우 취약하다. 예를 들어 
위 그림과 같이 데이터에 이상치가 있는 경우 절대 최댓값이 이상치에 의해 정해지기 때문에 대부분의 데이터가 중앙에 모여 있고 int8의 범위 양쪽 수를 사용하지 
못하는 낭비가 발생하게 된다.  
  
이상치의 영향을 줄이고 int8 형식의 낭비를 줄이기 위해 전체 데이터에 동일한 변환을 수행하는 것이 아니라 K개의 데이터를 묶은 블록(block) 단위로 
양자화를 수행하는 방법이 있다. 위 그림에서 전체 데이터에 대해 절대 최댓값을 구하고 양자화를 수행하는 것이 아니라 3개씩 데이터를 묶어 그 안에서 
절대 최댓값을 구하고 변환을 수행한다면 이상치와 함께 블록으로 묶인 3개의 데이터에만 이상치의 영향이 미친다. 중앙에 모여 있는 대부분의 데이터는 
int8 형식을 낭비하지 않고 양자화를 수행하게 된다.  
  
또 다른 방법으로 퀀타일(quantile) 방식이 있다. 절대 최댓값(absmax)만 보는 것이 아니라 입력 데이터를 크기순으로 등수를 매겨 int8 값에 동일한 
개수의 fp32 값이 대응되도록 배치하는 방식이다.  
  
![img.png](img5.png)  
  
위 그림에서는 각 int8 값에 fp32 값 1개가 대응되도록 변환하고 있다. 이 방식은 int8 값의 낭비는 없지만 매번 모든 입력 데이터의 등수를 확인하고 
배치해야 하기 떄문에 계산량도 많고 그 순서를 기억해야 하기 때문에 별도로 메모리를 사용해야 한다는 단점이 있다.  
  
양자화는 모델을 효율적으로 사용하는 다양한 기술에서 활용된다.  
  

