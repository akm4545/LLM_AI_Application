# **GPU 효율적인 학습**  
딥러닝 모델이 입력 데이터를 처리해 결과를 내놓을 때까지 많은 행렬 곱셈 연산을 처리한다. 행렬 곱셈은 숫자를 곱하고 더하는 단순한 연산을 반복 
계산하는 형태인데 GPU(Graphic Processing Unit)는 이러한 단순한 곱셈을 동시에 여러 개 처리하는 데 특화된 처리 장치다. 따라서 딥러닝 모델의 
연산을 빠르게 처리하기 위해 GPU를 많이 활용한다. GPU는 한정된 메모리를 갖고 있는데 최근 LLM의 등장과 함께 모델의 크기가 커지면서 하나의 GPU에 
모델을 올리지 못하거나 모델의 학습과 추론을 위해 더 많은 GPU가 필요해졌다. 하지만 GPU는 가격이 비싸기 때문에 일부 기업을 제외하고눈 풍부하게 사용하기 
어렵다. 이런 상활을 개선하고 더 많은 사람이 발전된 AI 기술을 사용할 수 있도록 GPU를 효율적으로 활용할 수 있는 방법을 찾기 위한 기술 발전이 
빠르게 이뤄지고 있다.  
  
구글 코랩 설치  
!pip install transformers==4.40.1 datasets==2.19.0 accelerate==0.30.0 peft==0.10.0  
bitsandbytes==0.43.1 -qqq  
  
# **GPU에 올라가는 뎅터 살펴보기**  
딥러닝 모델을 학습시키고 추론하기 위해 GPU를 사용할 때 가장 자주 만나는 에러 중 하나는 OOM(Out of Memory)에러다. OOM 에러란 한정된 GPU 
메모리에 데이터가 가득 차 더 이상 새로운 데이터를 추가하지 못해 발생하는 에러다. GPU 메모리에는 기본적으로 딥러닝 모델 자체가 올라간다. 딥러닝 
모델은 수많은 행렬 곱셈을 위한 파리미터의 집합이다. 각각의 파라미터는 소수 또는 정수 형식의 숫자이다.  
  
